,Abstract,Author,Date,Jornal,Keywords,Title
0,"This paper presents an unsupervised segment-based method for robust voice activity detection (rVAD). The method consists of two passes of denoising followed by a voice activity detection (VAD) stage. In the first pass, high-energy segments in a speech signal are detected by using a posteriori signal-to-noise ratio (SNR) weighted energy difference and if no pitch is detected within a segment, the segment is considered as a high-energy noise segment and set to zero. In the second pass, the speech signal is denoised by a speech enhancement method, for which several methods are explored. Next, neighbouring frames with pitch are grouped together to form pitch segments, and based on speech statistics, the pitch segments are further extended from both ends in order to include both voiced and unvoiced sounds and likely non-speech parts as well. In the end, a posteriori SNR weighted energy difference is applied to the extended pitch segments of the denoised speech signal for detecting voice activity. We evaluate the VAD performance of the proposed method using two databases, RATS and Aurora-2, which contain a large variety of noise conditions. The rVAD method is further evaluated, in terms of speaker verification performance, on the RedDots 2016 challenge database and its noise-corrupted versions. Experiment results show that rVAD is compared favourably with a number of existing methods. In addition, we present a modified version of rVAD where computationally intensive pitch extraction is replaced by computationally efficient spectral flatness calculation. The modified version significantly reduces the computational complexity at the cost of moderately inferior VAD performance, which is an advantage when processing a large amount of data and running on low resource devices. The source code of rVAD is made publicly available.","['Zheng-Hua Tan', 'Achintya kr. Sarkar', 'Najim Dehak']",January 2020,Computer Speech & Language,"['a posteriori SNR', 'Energy', 'Pitch detection', 'Spectral flatness', 'Speech enhancement', 'Voice activity detection', 'Speaker verification']",rVAD: An unsupervised segment-based robust voice activity detection method
1,"Recently several end-to-end speaker verification systems based on deep neural networks (DNNs) have been proposed. These systems have been proven to be competitive for text-dependent tasks as well as for text-independent tasks with short utterances. However, for text-independent tasks with longer utterances, end-to-end systems are still outperformed by standard i-vector + PLDA systems. In this work, we present an end-to-end speaker verification system that is initialized to mimic an i-vector + PLDA baseline. The system is then further trained in an end-to-end manner but regularized so that it does not deviate too far from the initial system. In this way we mitigate overfitting which normally limits the performance of end-to-end systems. The proposed system outperforms the i-vector + PLDA baseline on both long and short duration utterances.","['Johan Rohdin', 'Anna Silnova', 'Mireia Diez', 'Oldřich Plchot', 'Pavel Matějka', 'Lukáš Burget', 'Ondřej Glembek']",January 2020,Computer Speech & Language,"['Speaker verification', 'DNN', 'End-to-end', 'Text-independent', 'i-vector', 'PLDA']",End-to-end DNN based text-independent speaker recognition for long and short utterances
2,"In this work, we simulate a scenario, where a publicly available ASV system is used to enhance mimicry attacks against another closed source ASV system. In specific, ASV technology is used to perform a similarity search between the voices of recruited attackers (6) and potential target speakers (7,365) from VoxCeleb corpora to find the closest targets for each of the attackers. In addition, we consider ‘median’, ‘furthest’, and ’common’ targets to serve as a reference points.Our goal is to gain insights how well similarity rankings transfer from the attacker’s ASV system to the attacked ASV system, whether the attackers are able to improve their attacks by mimicking, and how the properties of the voices of attackers change due to mimicking. We address these questions through ASV experiments, listening tests, and prosodic and formant analyses. For the ASV experiments, we use i-vector technology in the attacker side, and x-vectors in the attacked side. For the listening tests, we recruit listeners through crowdsourcing.The results of the ASV experiments indicate that the speaker similarity scores transfer well from one ASV system to another. Both the ASV experiments and the listening tests reveal that the mimicry attempts do not, in general, help in bringing attacker’s scores closer to the target’s. A detailed analysis shows that mimicking does not improve attacks, when the natural voices of attackers and targets are similar to each other. The analysis of prosody and formants suggests that the attackers were able to considerably change their speaking rates when mimicking, but the changes in F0 and formants were modest. Overall, the results suggest that untrained impersonators do not pose a high threat towards ASV systems, but the use of ASV systems to attack other ASV systems is a potential threat.","['Ville Vestman', 'Tomi Kinnunen', 'Rosa González Hautamäki', 'Md Sahidullah']",January 2020,Computer Speech & Language,"['Speaker verification', 'Mimicry', 'Crowdsourcing', 'Spoofing', 'Automatic target speaker selection', 'Perceptual speaker similarity', 'Prosody']",Voice Mimicry Attacks Assisted by Automatic Speaker Verification
3,"In this paper, we describe the process that we used to create a new corpus of children’s emotional speech. We used a Wizard of Oz (WoZ) setting to induce different emotional reactions in children during speech-based interactions with two robots. We recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age. The recordings were manually segmented and transcribed. The segments were then labeled with two types of emotional-related paralinguistic information: emotion and attitude. The corpus contained 2093 min of audio recordings (34.88 h) divided into 19,793 speech segments. The Interactive Emotional Children’s Speech Corpus (IESC-Child) can be a valuable resource for researchers studying affective reactions in speech communication during child-computer interactions in Spanish and for creating models to recognize acoustic paralinguistic information. IESC-Child is available to the research community upon request.","['Humberto Pérez-Espinosa', 'Juan Martínez-Miranda', 'Ismael Espinosa-Curiel', 'Josefina Rodríguez-Jacobo', 'Luis Villaseñor-Pineda', 'Himer Avila-George']",January 2020,Computer Speech & Language,"['Interactive systems', 'Emotional analysis', 'Paralinguistic information']",IESC-Child: An Interactive Emotional Children’s Speech Corpus
4,"Acoustic characteristics and articulatory movements are known to vary with speaking rates. This study investigates the role of speaking rate on acoustic-to-articulatory inversion (AAI) performance using deep neural networks (DNNs). Since fast speaking rate causes fast articulatory motion as well as changes in spectro-temporal characteristics of the speech signal, the articulatory-acoustic map in a fast speaking rate could be different from that in a slow speaking rate. We examine how these differences alter the accuracy with which different articulatory positions could be recovered from the acoustics. AAI experiments are performed in both matched and mismatched train-test conditions using data of five subjects, in three different rates – normal, fast and slow (fast and slow rates are at least 1.3 times faster and slower than the normal rate). Experiments in matched cases reveal that, the errors in estimating vertical motion of sensors on the tongue articulators from acoustics with fast speaking rate, is significantly higher than those with slow speaking rate. Experiments in mis-matched conditions reveal that there is consistent drop in AAI performance compared to the matched condition. Further experiments performed by training AAI with acoustic-articulatory data pooled from different speaking rates reveal that a single DNN based AAI model is capable of learning multiple rate-specific mapping.","['Aravind Illa', 'Prasanta Kumar Ghosh']",January 2020,Computer Speech & Language,"['Acoustic-to-articulatory inversion', 'Speaking rate', 'Electromagnetic articulograph']",The impact of speaking rate on acoustic-to-articulatory inversion
5,"Synesthesia focuses on the cross-modality connections between different perceptual modalities (visual, auditory, olfactory, gustatory, and haptic). It presents the aesthetics and creativity of our language and cognitive systems. A model that automatically interprets synesthetic metaphor is proposed in this paper. The interpretation model can learn the semantic knowledge of a feature, and simulate the cross-modality semantic similarity. The semantic knowledge has three parts: feature, the perceptual modality of the feature, and its sentimental orientation. They are used for supervising feature generation and expansion. By multi-step synonyms expansion, our model explores the cross-modality associations between the source and target domains which belong to different modalities. The appropriate feature is selected by a heuristic way to describe the latent meaning of the target domain in the current context. The experimental result shows that our model can fully consider the semantic knowledge and embody the cross-modality relations. It achieves an accuracy of 84%.","['Chang Su', 'Xiaomei Wang', 'Zita Wang', 'Yijiang Chen']",November 2019,Computer Speech & Language,"['Synesthetic metaphor', 'Semantic knowledge', 'Cross-modality similarity']",A model of synesthetic metaphor interpretation based on cross-modality similarity
6,,"['Adrian-Horia Dediu', 'Carlos Martín-Vide']",November 2019,Computer Speech & Language,[],"Third International Conference on Statistical Language and Speech Processing, SLSP 2015 Preface"
7,"The pioneering research of G. K. Zipf on the relationship between word frequency and other word features led to the formulation of various linguistic laws. The most popular is Zipf’s law for word frequencies. Here we focus on two laws that have been studied less intensively: the meaning-frequency law, i.e. the tendency of more frequent words to be more polysemous, and the law of abbreviation, i.e. the tendency of more frequent words to be shorter. In a previous work, we tested the robustness of these Zipfian laws for English, roughly measuring word length in number of characters and distinguishing adult from child speech. In the present article, we extend our study to other languages (Dutch and Spanish) and introduce two additional measures of length: syllabic length and phonemic length. Our correlation analysis indicates that both the meaning-frequency law and the law of abbreviation hold overall in all the analyzed languages.","['Bernardino Casas', 'Antoni Hernández-Fernández', 'Neus Català', 'Ramon Ferrer-i-Cancho', 'Jaume Baixeries']",November 2019,Computer Speech & Language,"['Zipf’s laws', 'Polysemy', 'Brevity', 'Word frequency']",Polysemy and brevity versus frequency in language
8,"Kernel-based and Deep Learning methods are two of the most popular approaches in Computational Natural Language Learning. Although these models are rather different and characterized by distinct strong and weak aspects, they both had impressive impact on the accuracy of complex Natural Language Processing tasks. An advantage of kernel-based methods is their capability of exploiting structured information induced from examples. For instance, Sequence or Tree kernels operate over structures reflecting linguistic evidence, such as syntactic information encoded in syntactic parse trees. Deep Learning approaches are very effective as they can learn non-linear decision functions: however, general models require input instances to be explicitly modeled via vectors or tensors, and operating on structured data is made possible only by using ad-hoc architectures.In this work, we discuss a novel architecture that efficiently combines kernel methods and neural networks, in the attempt at squeezing the best from the two paradigms. The so-called Kernel-based Deep Architecture (KDA) adopts a Nyström-based projection function to approximate any valid kernel function and convert any structure they operate on (for instance, linguistic structures, such as trees) into dense linear embeddings. These can be used as input of a Deep Feed-forward Neural Network that exploits such embeddings to learn non-linear classification functions. KDA is a mathematically justified integration of expressive kernel functions and deep neural architectures, with several advantages: it (i) directly operates over complex non-tensor structures, e.g., trees, without ad hoc manual feature engineering or architectural design, (ii) achieves a drastic reduction of the computational cost w.r.t. pure kernel methods, and (iii) exploits the non-linearity of Deep Architectures to produce accurate models. We experimented the KDA in three rather different semantic inference tasks: Semantic Parsing, Question Classification, and Community Question Answering. Results show that the KDA achieves state-of-the-art accuracy, with a computational cost that is much lower than the one necessary to train and test a pure kernel-based method, such as the SVM algorithm.","['D. Croce', 'S. Filice', 'R. Basili']",November 2019,Computer Speech & Language,"['Kernel-based learning', 'Neural methods', 'Semantic spaces', 'Nystrom embeddings']",Making sense of kernel spaces in neural learning
9,"Recently, several information retrieval (IR) models have been proposed in order to boost the retrieval performance using term dependencies. However, in the context of the Arabic language, most IR researchers have focused on the problem of stemming, which is highly challenging in this language. In this paper, we propose to explore whether term dependencies can help improve Arabic IR systems, and what are the best methods to use. To do so, we consider both explicit term dependencies based on multi-word terms (MWTs) that are extracted using syntactic patterns and statistical filters, as well as implicit ones based on the notion of cross-terms or term proximities. Our experiments, performed on standard TREC Arabic IR collections, show the importance of taking into account term dependencies for Arabic IR. To the best of our knowledge, this is the first study that provides complete extensions, and their comparison, of most standard IR models to deal with term dependencies in the Arabic language.","['Abdelkader El Mahdaouy', 'Eric Gaussier', 'Saïd Ouatik El Alaoui']",November 2019,Computer Speech & Language,"['Arabic information retrieval', 'Multi-word terms', 'Term proximity', 'Term dependence IR models']",Should one use term proximity or multi-word terms for Arabic information retrieval?☆
10,"Neural machine translation systems require large amounts of training data and resources. Even with this, the quality of the translations may be insufficient for some users or domains. In such cases, the output of the system must be revised by a human agent. This can be done in a post-editing stage or following an interactive machine translation protocol.We explore the incremental update of neural machine translation systems during the post-editing or interactive translation processes. Such modifications aim to incorporate the new knowledge, from the edited sentences, into the translation system. Updates to the model are performed on-the-fly, as sentences are corrected, via online learning techniques. In addition, we implement a novel interactive, adaptive system, able to react to single-character interactions. This system greatly reduces the human effort required for obtaining high-quality translations.In order to stress our proposals, we conduct exhaustive experiments varying the amount and type of data available for training. Results show that online learning effectively achieves the objective of reducing the human effort required during the post-editing or the interactive machine translation stages. Moreover, these adaptive systems also perform well in scenarios with scarce resources. We show that a neural machine translation system can be rapidly adapted to a specific domain, exclusively by means of online learning techniques.","['Álvaro Peris', 'Francisco Casacuberta']",November 2019,Computer Speech & Language,"['Neural machine translation', 'Interactive machine translation', 'Machine translation post-editing', 'Online learning', 'Domain adaptation', 'Deep learning']",Online learning for effort reduction in interactive neural machine translation
11,"Alignment is a key step before learning a mapping function between a source and a target speaker’s spectral features in various state-of-the-art parallel data Voice Conversion (VC) techniques. After alignment, some corresponding pairs are still inconsistent with the rest of the data and are considered outliers. These outliers shift the parameters of the mapping function from their true value and hence, negatively affect the learning of mapping function during the training phase of the VC task. To the best of the authors’ knowledge, the effect of outliers (and hence, their removal) on quality of the converted voice has not been much explored in the VC literature. Recent research has shown the effectiveness of the outlier removal as a pre-processing step in the VC. In this paper, we extend this study with a detailed theory and analysis. The proposed method uses a score distance that is estimated using Robust Principal Component Analysis (ROBPCA) to detect the outliers. In particular, the outliers are determined using a fixed cut-off on the score distances, based on the degrees of freedom in a chi-squared distribution, which is speaker-pair independent. The fixed cut-off is due to the assumption that the score distances follow the normal (i.e., Gaussian) distribution. However, this is a weak statistical assumption even in the cases where quite many samples are available. Hence, in this paper, we propose to explore speaker-pair dependent cut-offs to detect the outliers. In addition, we have presented our results on two state-of-the-art databases, namely, CMU-ARCTIC and Voice Conversion Challenge (VCC) 2016 by developing various state-of-the-art methods in the VC. In particular, we have presented the effectiveness of the outlier removal on Gaussian Mixture Model (GMM), Artificial Neural Network (ANN), and Deep Neural Network (DNN)-based VC techniques. Furthermore, we have presented subjective and objective evaluations using a 95% confidence interval for the statistical significance of the tests. We obtained an average 0.56% relative reduction in Mel Cepstral Distortion (MCD) with the proposed outlier removal approach as a pre-processing step. In particular, with the proposed speaker-pair dependent cut-off, we have observed relative improvement of 12.24% and 30.51% in the speech quality, and 39.7% and 4.27% absolute improvement in the speaker similarity for the CMU-ARCTIC and the VCC 2016, respectively.","['Nirmesh J. Shah', 'Hemant A. Patil']",November 2019,Computer Speech & Language,"['Outlier removal', 'Outlier detection', 'Parallel data voice conversion', 'Robust Principal Component Analysis']",A novel approach to remove outliers for parallel voice conversion
12,"Speaker recognition is a task of remarkable relevance, with applications in diversified domains. Recently, mainly due to the facilities in audio-visual content acquisition, the capacity of analyzing growing datasets independent of labeled data has become a crucial advantage. This paper presents a speaker recognition approach based on recent unsupervised learning methods, which do not require any labeled data or user intervention. The approach is organized in terms of a framework which exploits a rank-based formulation. The similarity information defined by speaker modeling techniques is encoded in ranked lists, which are used as input by the unsupervised learning algorithms. Vector quantization, Gaussian mixture models and i-vectors are employed as modeling techniques, while the algorithms RL-Sim and ReckNN are used for unsupervised learning tasks. The framework was experimentally evaluated on query-by-example speaker retrieval and speaker identification tasks, both on clean and noisy speech recordings. An experimental evaluation was conducted on three public datasets, different languages, and recordings conditions. Effectiveness gains up to +56% on retrieval measures were obtained through the use of unsupervised learning algorithms over traditional speaker recognition techniques.","['Victor de Abreu Campos', 'Daniel Carlos Guimarães Pedronette']",November 2019,Computer Speech & Language,"['Speaker recognition', 'Speaker retrieval', 'Unsupervised learning', 'Vector quantization', 'Gaussian mixture model', 'i-vector']",A framework for speaker retrieval and identification through unsupervised learning
13,"A speech spectrum is known to be changed by the variations in the length of the vocal tract of a speaker. This is because of the fact that speech formants are inversely related to the vocal tract length (VTL). The process of compensating spectral variation due to the length of the vocal tract is known as Vocal Tract Length Normalization (VTLN). VTLN is a very important speaker normalization technique for speech recognition and related tasks. In this paper, we used Gaussian Posteriorgram (GP) of VTL-warped spectral features for a Query-by-Example Spoken Term Detection (QbE-STD) task. This paper presents the use of a Gaussian Mixture Model (GMM) framework for VTLN warping factor estimation. In particular, the presented GMM framework does not require phoneme-level transcription. We observed the correlation between the VTLN warping factor estimates obtained via a supervised HMM-based approach and an unsupervised GMM-based approach. In addition, a phoneme recognition and speaker de-identification tasks were conducted using GMM-based VTLN warping factor estimates. For QbE-STD, we considered three spectral features, namely, Mel Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP), and MFCC-TMP (which uses Teager Energy Operator (TEO) to exploit implicitly magnitude and phase information in the MFCC framework). Linear frequency scaling variations for VTLN warping factor are incorporated into these three cepstral representations for the QbE-STD task. Similarly, VTL-warped Gaussian posteriorgram improved the Maximum Term Weighted Value by 0.021 (i.e., 2.1%), and 0.015 (i.e., 1.5%), for MFCC and PLP feature sets, respectively, on the evaluation set of the MediaEval SWS 2013 corpus. The better performance is primarily due to VTLN warping factor estimation using unsupervised GMM framework. Finally, the effectiveness of the proposed VTL-warped GP is presented to rescore using various detection sources, such as depth of detection valley, Self-Similarity Matrix, Pseudo Relevance Feedback and weighted mean features.","['Maulik C. Madhavi', 'Hemant A. Patil']",November 2019,Computer Speech & Language,"['Vocal Tract Length Normalization', 'Query-by-example spoken term detection', 'Spoken web search task', 'Gaussian posteriorgrams', 'Dynamic time warping']",Vocal Tract Length Normalization using a Gaussian mixture model framework for query-by-example spoken term detection
14,"With the rise of robotics and artificial intelligence, good communication between humans and machines becomes more important. However, users with language and hearing disadvantages may find synthetic speech systems to be difficult to understand. In this study, we explore the types of sentence structure and level of word complexity that affect intelligibility of speech in unfamiliar context. Using semantically unpredictable sentences, we found that sentence with more complex syntax such as relative pronouns and question words are harder to comprehend, while on the word level, it is the shorter and simpler words that contribute to misunderstandings. We found that although word frequency affects how well a word is recognised, the effect from the occurring frequency is much less than the effect of how phonetically distinctive the word is. There was also evidence of significant difference between native speakers and non-native speakers on how well they could understand the sentences. These results may help us in designing better dialogue system for machine to human interactions, especially in the healthcare arena, where often users have disadvantages in language and hearing abilities.","['C.T. Justine Hui', 'Sahil Jain', 'Catherine I. Watson']",November 2019,Computer Speech & Language,"['Dialogue design', 'Speech perception', 'Speech synthesis', 'Machine to human communications']",Effects of sentence structure and word complexity on intelligibility in machine-to-human communications
15,"The lack of multi-document based models and the inaccuracy in representing multiple long documents into a fixed size vector inspired us to solve abstractive multi-document summarization. Also, there is lack of good multi-document based human-authored datasets to train any encoder-decoder models. To overcome this, we have designed complementary models for two different tasks such as sentence clustering and neural sentence fusion. In this work, we minimize the risk of producing incorrect fact by encoding a related set of sentences as an input to the encoder. We have applied our complementary models to implement a full abstractive multi-document summarization system which simultaneously considers importance, coverage, and diversity under a desired length limit. We conduct extensive experiments for all the proposed models which bring significant improvements over the state-of-the-art methods across different evaluation metrics.","['Tanvir Ahmed Fuad', 'Mir Tafseer Nayeem', 'Asif Mahmud', 'Yllias Chali']",November 2019,Computer Speech & Language,"['68T50', 'Abstractive multi-document Summarization', 'Sentence fusion', 'Neural fusion model', 'Document clustering']",Neural sentence fusion for diversity driven abstractive multi-document summarization
16,"We introduce a real-time capable algorithm which estimates the long-term signal to noise ratio (SNR) of the speech in multi-talker babble noise. In real-time applications, long-term SNR is calculated over a sufficiently long moving frame of the noisy speech ending at the current time. The algorithm performs the real-time long-term SNR estimation by averaging “speech-likeness” values of multiple consecutive short-frames of the noisy speech which collectively form a long-frame with an adaptive length. The algorithm is calibrated to be insensitive to short-term fluctuations and transient changes in speech or noise level. However, it quickly responds to non-transient changes in long-term SNR by adjusting the duration of the long-frame on which the long-term SNR is measured. This ability is obtained by employing an event detector and adaptive frame duration. The event detector identifies non-transient changes of the long-term SNR and optimizes the duration of the long-frame accordingly. The algorithm was trained and tested for randomly generated speech samples corrupted with multi-talker babble. In addition to its ability to provide an adaptive long-term SNR estimation in a dynamic noisy situation, the evaluation results show that the algorithm outperforms the existing overall SNR estimation methods in multi-talker babble over a wide range of number of talkers and SNRs. The relatively low computational cost and the ability to update the estimated long-term SNR several times per second make this algorithm capable of operating in real-time speech processing applications.","['Roozbeh Soleymani', 'Ivan W. Selesnick', 'David M. Landsberger']",November 2019,Computer Speech & Language,"['Multi-talker babble', 'Long-term SNR', 'Adaptive SNR', 'Real-time SNR', 'Signal-to-noise ratio']",ALTIS: A new algorithm for adaptive long-term SNR estimation in multi-talker babble
17,"The repeated use of out-of-vocabulary (OOV) words in a spoken document seriously degrades a speech recognizer performance. Even though such recurrent OOV words are often important keywords in a spoken document, they are never correctly recognized. We propose a novel method for robustly detecting recurrent OOV words, which focuses on the degree of consistency among them. It first detects recurrent segments, that is recurrent phoneme sub-sequence in the output of a phoneme sequence decoder. Then, we measure the degree of consistency by using the mean and variance (distribution) of features (DOF) derived from the recurrent segments, and use our DOF for IV/OOV classification. Experiments on academic lectures illustrate that the proposed DOF-based method can robustly detect recurrent OOV words in spontaneous speech and achieves over 60% relative reduction in false alarms. It is also confirmed that detection performance improves as the OOV words are repeated more often.","['Taichi Asami', 'Ryo Masumura', 'Yushi Aono', 'Koichi Shinoda']",November 2019,Computer Speech & Language,"['Speech recognition', 'Out-of-vocabulary (OOV) word detection', 'Recurrent OOV words', 'Distribution of features']",Recurrent out-of-vocabulary word detection based on distribution of features
18,"This study proposes a new non-intrusive measure of speech quality, the neurogram speech quality measure (NSQM), based on the responses of a biologically-inspired computational model of the auditory system for listeners with normal hearing. The model simulates the responses of an auditory-nerve fiber with a characteristic frequency to a speech signal, and the population response of the model is represented by a neurogram (2D time-frequency representation). The responses of each characteristic frequency in the neurogram were decomposed into sub-bands using 1D discrete Wavelet transform. The normalized energy corresponding to each sub-band was used as an input to a support vector regression model to predict the quality score of the processed speech. The performance of the proposed non-intrusive measure was compared to the results from a range of intrusive and non-intrusive measures using three standard databases: the EXP1 and EXP3 of supplement 23 to the P series (P.Supp23) of ITU-T Recommendations and the NOIZEUS databases. The proposed NSQM achieved an overall better result over most of the existing metrics for the effects of compression codecs, additive and channel noises.","['Wissam A. Jassim', 'Muhammad S. Zilany']",November 2019,Computer Speech & Language,"['Speech quality assessment', 'PESQ', 'POLQA', 'Neurogram', 'Auditory-nerve model', 'Discrete Wavelet transform']",NSQM: A non-intrusive assessment of speech quality using normalized energies of the neurogram
19,"After reading a news article, some readers post their opinion to social networks, particularly as tweets. These opinions (responses) have an important emotional content. By analyzing users’ responses in context, it is possible to find a set of emotions expressed in these tweets. In this work we propose a method to predict the emotional reactions that Twitter users would have after reading a news article. We consider the prediction of emotions as a classification problem and we follow a supervised approach. For this purpose, we collected a corpus of Spanish news articles and their associated tweet responses. Then, a group of annotators tagged the emotions expressed in them. Twitter users can express more than one emotion in their responses, so that in this work we deal with this characteristic by using a multi-target classification strategy. The use of this strategy allows an instance (a news article) to have more than one associated class (emotions expressed by users). In addition to that, the multi-target strategy permits to predict not only the emotional reactions, but also the intensity of these emotions, considering how often each specific emotion was triggered by users. By measuring the deviation of the predicted emotional reactions with regard to the annotated ones, we obtain an emotional reactions similarity of 89%.","['Omar Juárez Gambino', 'Hiram Calvo']",November 2019,Computer Speech & Language,[],Predicting emotional reactions to news articles in social networks☆
20,"Keyphrases of a given document represent its main topic and they are used as a simple method to represent the document. Statistical and graph-based models as unsupervised approaches have been mainly studied. The statistical models have some difficulty in extracting keyphrases from a single document because most statistical ones generally require statistical information from a large corpus. On the other hand, graph-based models can extract keyphrases by only using the information from a single document; nevertheless, they have some drawbacks. The scores of the edges can be biased because a single document does not contain sufficient information to score the edges of a graph and this influences the score of the nodes. In this paper, we propose an effective combination method of a statistical model, C-value method, and a graph-based model to overcome the drawbacks of each model. A new scoring method for keyphrase candidates is developed by the graph-based model and the scores calculated by the new method are applied to the modified C-value method to estimate the final importance scores of the keyphrase candidates. Subsequently, the proposed model is evaluated using two datasets, SemEval 2010 and Inspec, and its results outperformed the state-of-the-art model among unsupervised models and the existing graph-based ranking models.","['Hongseon Yeom', 'Youngjoong Ko', 'Jungyun Seo']",November 2019,Computer Speech & Language,"['Automatic keyphrase extraction', 'Graph-basedranking algorithm', 'C-value', 'PageRank', 'Information extraction']",Unsupervised-learning-based keyphrase extraction from a single document by the effective combination of the graph-based model and the modified C-value method
21,"The rapid population aging has stimulated the development of assistive devices that provide personalized medical support to the needies suffering from various etiologies. One prominent clinical application is a computer-assisted speech training system which enables personalized speech therapy to patients impaired by communicative disorders in the patient’s home environment. Such a system relies on the robust automatic speech recognition (ASR) technology to be able to provide accurate articulation feedback. With the long-term aim of developing off-the-shelf ASR systems that can be incorporated in clinical context without prior speaker information, we compare the ASR performance of speaker-independent bottleneck and articulatory features on dysarthric speech used in conjunction with dedicated neural network-based acoustic models that have been shown to be robust against spectrotemporal deviations. We report ASR performance of these systems on two dysarthric speech datasets of different characteristics to quantify the achieved performance gains. Despite the remaining performance gap between the dysarthric and normal speech, significant improvements have been reported on both datasets using speaker-independent ASR architectures.","['Emre Yılmaz', 'Vikramjit Mitra', 'Ganesh Sivaraman', 'Horacio Franco']",November 2019,Computer Speech & Language,"['Pathological speech', 'Automatic speech recognition', 'Articulatory features', 'Time-frequency convolutional neural networks', 'Dysarthria']",Articulatory and bottleneck features for speaker-independent ASR of dysarthric speech☆
22,"Named Entity Recognition (NER) is a significant information extraction task since it is an important component of many natural language processing applications, such as Information Retrieval, Question Answering and Speech Recognition. The complexity and morphological richness of the Arabic language is the main reason why most existing Arabic NER systems rely strongly on hand-crafted feature engineering. In this paper, we propose to augment the existing LSTM neural tagging model for Arabic NER with a Convolutional Neural Network (CNN) for the extraction of relevant character-level features. By operating on the character-level, the proposed model is able to handle out-of-vocabulary words. Our results show that character CNN is able to outperform the previously used character-level Bi-directional Long Short-Term Memory Networks (BiLSTM) in many settings. Moreover, our observations indicate that CNNs tend to perform better than BiLSTM on relatively longer tokens. In addition, we conduct a comparison of four different pre-trained word vector models for Arabic NER and results show that a Skip-Gram Word2vec model, pre-trained on a subset of the Arabic Gigaword corpus, is generally sufficient to obtain acceptable Arabic NER performance.","['Muhammad Khalifa', 'Khaled Shaalan']",November 2019,Computer Speech & Language,"['Named Entity Recognition', 'Arabic', 'Recurrent Neural Network', 'LSTM', 'Convolutional Neural Network']",Character convolutions for Arabic Named Entity Recognition with Long Short-Term Memory Networks
23,"This paper proposes novel training algorithms for vocoder-free text-to-speech (TTS) synthesis based on generative adversarial networks (GANs) that compensate for short-term Fourier transform (STFT) amplitude spectra in low/multi frequency resolution. Vocoder-free TTS using STFT amplitude spectra can avoid degradation of synthetic speech quality caused by the vocoder-based parameterization used in conventional TTS. Our previous work for the vocoder-based TTS proposed a method for incorporating the GAN-based distribution compensation into acoustic model training to improve synthetic speech quality. This paper extends the algorithm to the vocoder-free TTS and propose a GAN-based training algorithm using low-frequency-resolution amplitude spectra to overcome the difficulty in modeling complicated distribution of the high-dimensional spectra. In the proposed algorithm, amplitude spectra are transformed into low-frequency-resolution amplitude spectra by applying an average pooling function along with a frequency axis; then the GAN-based distribution compensation is performed in the low-frequency-resolution domain. Because the low-frequency-resolution amplitude spectra approximately emulate filter banks, the proposed algorithm is expected to improve synthetic speech quality by reducing differences in spectral envelopes of natural and synthetic speech. Furthermore, various frequency scales that are related to human speech perception (e.g., mel and inverse mel frequency scales) can be introduced to the proposed training algorithm by applying an frequency warping function to amplitude spectra. This paper also proposes a GAN-based training algorithm using multi-frequency-resolution amplitude spectra that uses both low- and original-frequency-resolution amplitude spectra to reduce the differences in not only spectral envelopes but also fine structures. Experimental results demonstrate that (1) GANs using low-frequency-resolution amplitude spectra improve speech quality and work robustly against the settings of the frequency resolution and hyperparameters, (2) in comparison among low-, original-, and multi-frequency-resolution amplitude spectra, the use of low-frequency-resolution ones work best improve the synthetic speech quality, and (3) the use of the inverse mel frequency scale for obtaining low-frequency-resolution amplitude spectra further improves synthetic speech quality.","['Yuki Saito', 'Shinnosuke Takamichi', 'Hiroshi Saruwatari']",November 2019,Computer Speech & Language,"['Vocoder-free text-to-speech', 'Training algorithm', 'STFT amplitude spectra', 'Generative adversarial networks', 'Frequency resolution', 'Frequency warping']",Vocoder-free text-to-speech synthesis incorporating generative adversarial networks using low-/multi-frequency STFT amplitude spectra
24,"The problem of language identification from speech is tackled in this work. Residual convolutional neural networks are employed to this end, aiming at exploiting the ability of such architectures to take into account large contextual segments of input data. Moreover, learnable attention mechanisms are introduced on top of the convolutional stack for data-driven feature pooling across time, enabling the computation of fixed-dimension representations given varying-length speech segments as input. Training is performed using a combination of language identification and metric learning via triplet loss minimization, aimed at enforcing class separability within the embeddings space. Evaluation is performed across different conditions, such as multi-class classification, short-duration test utterances, and confusing languages, for the closed-set case, while open-set performance is evaluated with the introduction of unseen languages. At test time, end-to-end scoring along with cosine similarity and PLDA are employed, outperforming state-of-the-art benchmark methods, such as i-vectors by improving the average cost by 30% to 40% depending on the evaluation condition.","['João Monteiro', 'Jahangir Alam', 'Tiago H. Falk']",November 2019,Computer Speech & Language,"['Language recognition', 'Language modeling', 'Residual convolutional neural networks', 'Attentive feature pooling']",Residual convolutional neural network with attentive feature pooling for end-to-end language identification from short-duration speech
25,"With the increasing use of voice as a biometric, it has become imperative to develop countermeasures to thwart malicious spoofing attacks on speaker recognition systems. Even though there has been a significant research effort over the last few years dedicated to the development of countermeasures to detect and deflect spoofing attacks, the problem is far from being solved. While a deep learning technique has been successfully applied in anti-spoofing research, it suffers from a data scarcity issue where large amounts of labeled training data are required to build a robust model. In this paper, we investigate a domain adaptation approach of deep architectures in both a supervised setting where we use labeled data, and in an unsupervised setting where we assume unlabeled data when transferring knowledge from the source to target domain. Specifically, we employ convolutional neural networks (CNNs) as back-end classifiers for spoofed speech detection. For supervised domain adaptation, we propose joint neural network training while allowing the weights to be shared between the source and target streams, and an additional domain regularizer. In the unsupervised domain adaptation scenario, the weights are not shared in order to explicitly model the domain shift. However, the weights are related by weight regularizers to take into account the difference between the two domains. We conduct extensive cross-database (domain mismatch) experiments using ASVspoof 2015 and BTAS 2016 datasets to demonstrate the generalization capability of the proposed deep domain architectures for spoofing detection. Experimental results reveal that the proposed architectures can generalize across databases for both supervised and unsupervised adaptation scenarios.","['Ivan Himawan', 'Fernando Villavicencio', 'Sridha Sridharan', 'Clinton Fookes']",November 2019,Computer Speech & Language,"['Convolutional neural networks', 'ASVspoof 2015', 'BTAS 2016', 'Deep domain adaptation', 'Speaker verification', 'Cross-database study']",Deep domain adaptation for anti-spoofing in speaker verification systems☆
26,"In this work, we present an analysis of a DNN-based autoencoder for speech enhancement, dereverberation and denoising. The target application is a robust speaker verification (SV) system. We start our approach by carefully designing a data augmentation process to cover a wide range of acoustic conditions and to obtain rich training data for various components of our SV system. We augment several well-known databases used in SV with artificially noised and reverberated data and we use them to train a denoising autoencoder (mapping noisy and reverberated speech to its clean version) as well as an x-vector extractor which is currently considered as state-of-the-art in SV. Later, we use the autoencoder as a preprocessing step for a text-independent SV system. We compare results achieved with autoencoder enhancement, multi-condition PLDA training and their simultaneous use. We present a detailed analysis with various conditions of NIST SRE 2010, 2016, PRISM and with re-transmitted data. We conclude that the proposed preprocessing can significantly improve both i-vector and x-vector baselines and that this technique can be used to build a robust SV system for various target domains.","['Ondřej Novotný', 'Oldřich Plchot', 'Ondřej Glembek', 'Jan “Honza” Černocký', 'Lukáš Burget']",November 2019,Computer Speech & Language,"['Speaker verification', 'Signal enhancement', 'Autoencoder', 'Neural network', 'Robustness', 'Embedding']",Analysis of DNN Speech Signal Enhancement for Robust Speaker Recognition☆
27,"This paper presents the adversarial learning approaches to deal with various tasks in speaker recognition based on probabilistic discriminant analysis (PLDA) which is seen as a latent variable model for reconstruction of i-vectors. The first task aims to reduce the dimension of i-vectors based on an adversarial manifold learning where the adversarial neural networks of generator and discriminator are merged to preserve neighbor embedding of i-vectors in a low-dimensional space. The generator is trained to fool the discriminator with the generated samples in latent space. A PLDA subspace model is constructed by jointly minimizing a PLDA reconstruction error, a manifold loss for neighbor embedding and an adversarial loss caused by the generator and discriminator. The second task of adversarial learning is developed to tackle the imbalanced data problem. A PLDA based generative adversarial network is trained to generate new i-vectors to balance the size of training utterances across different speakers. An adversarial augmentation learning is proposed for robust speaker recognition. In particular, the minimax optimization is performed to estimate a generator and a discriminator where the class conditional i-vectors produced by generator could not be distinguished from real i-vectors via discriminator. A multiobjective learning is realized for a specialized neural model with the cosine similarity between real and fake i-vectors as well as the regularization for Gaussianity. Experiments are conducted to show the merit of adversarial learning in subspace construction and data augmentation for PLDA-based speaker recognition.","['Jen-Tzung Chien', 'Kang-Ting Peng']",November 2019,Computer Speech & Language,"['Probabilistic linear discriminant analysis', 'Adversarial learning', 'Manifold learning', 'Data augmentation', 'Speaker recognition']",Neural adversarial learning for speaker recognition
28,"Speech recordings are a rich source of personal, sensitive data that can be used to support a plethora of diverse applications, from health profiling to biometric recognition. It is therefore essential that speech recordings are adequately protected so that they cannot be misused. Such protection, in the form of privacy-preserving technologies, is required to ensure that: (i) the biometric profiles of a given individual (e.g., across different biometric service operators) are unlinkable; (ii) leaked, encrypted biometric information is irreversible, and that (iii) biometric references are renewable. Whereas many privacy-preserving technologies have been developed for other biometric characteristics, very few solutions have been proposed to protect privacy in the case of speech signals. Despite privacy preservation this is now being mandated by recent European and international data protection regulations. With the aim of fostering progress and collaboration between researchers in the speech, biometrics and applied cryptography communities, this survey article provides an introduction to the field, starting with a legal perspective on privacy preservation in the case of speech data. It then establishes the requirements for effective privacy preservation, reviews generic cryptography-based solutions, followed by specific techniques that are applicable to speaker characterisation (biometric applications) and speech characterisation (non-biometric applications). Glancing at non-biometrics, methods are presented to avoid function creep, preventing the exploitation of biometric information, e.g., to single out an identity in speech-assisted health care via speaker characterisation. In promoting harmonised research, the article also outlines common, empirical evaluation metrics for the assessment of privacy-preserving technologies for speech data.","['Andreas Nautsch', 'Abelino Jiménez', 'Amos Treiber', 'Jascha Kolberg', 'Catherine Jasserand', 'Els Kindt', 'Héctor Delgado', 'Massimiliano Todisco', 'Mohamed Amine Hmani', 'Aymen Mtibaa', 'Mohammed Ahmed Abdelraheem', 'Alberto Abad', 'Francisco Teixeira', 'Driss Matrouf', 'Marta Gomez-Barrero', 'Dijana Petrovska-Delacrétaz', 'Gérard Chollet', 'Nicholas Evans', 'Christoph Busch']",November 2019,Computer Speech & Language,"['Data privacy', 'Voice biometrics', 'Standardisation', 'Cryptography', 'Legislation']",Preserving privacy in speaker and speech characterisation☆
29,,[],September 2019,Computer Speech & Language,[],Editorial Board
30,"There are two fundamental difficulties that are still hindering the development of microblog summarization. The first problem is the features sparseness of microblog, which restricts the performance of sub-topics detection. The second one is the sentence selection from sub-topics that is based mainly on centrality approaches to measure sentence salience. Also, the semantic features and relations features between sentences and sub-topics were not given much attention. In order to address the two aforementioned problems, we propose a summarization method considering Paragraph Vector and semantic structure. Firstly, we construct sentence similarity matrix that involves the contextual information of microblogs to detect sub-topics by using Paragraph Vector. Secondly, we analyze the sentences by utilizing Chinese Sentential Semantic Model (CSM) to get semantic features; then the relations features are obtained based on the similarity matrix and semantic features above. Finally, the most informative sentences can be selected accurately from microblogs belonging to the same sub-topics by semantic features and relation features. The experimental results show that the ROUGE-1 value is up to 53.17% with 1.5% compression ratio. The results indicate that applying Paragraph Vector to the field of microblog summarization can effectively improve sub-topics detection. Additionally, semantic features and relation features enhance summarization result jointly. Furthermore, CSM provides a promising tool for sentence semantic analysis.","['Ruiyi Wang', 'Senlin Luo', 'Limin Pan', 'Zhouting Wu', 'Yujiao Yuan', 'Qianrou Chen']",September 2019,Computer Speech & Language,"['Chinese Sentential Semantic Model', 'Deep learning', 'Language models', 'Language parsing and understanding', 'Microblog summarization']",Microblog summarization using Paragraph Vector and semantic structure
31,"We present a method enabling the unsupervised discovery of sub-word units (SWUs) and associated pronunciation lexicons for use in automatic speech recognition (ASR) systems. This includes a novel SWU discovery approach based on self-organising HMM-GMM states that are agglomeratively tied across words as well as a novel pronunciation lexicon induction approach that iteratively reduces pronunciation variation by means of model pruning. Our approach relies only on recorded speech and associated orthographic transcriptions and does not require alphabetic graphemes. We apply our methods to corpora of recorded radio broadcasts in Ugandan English, Luganda and Acholi, of which the latter two are under-resourced. The speech is conversational and contains high levels of background noise, and therefore presents a challenge to automatic lexicon induction. We demonstrate that our proposed method is able to discover lexicons that perform as well as baseline expert systems for Acholi, and close to this level for the other two languages when used to train DNN-HMM ASR systems. This demonstrates the potential of the method to enable and accelerate ASR for under-resourced languages for which a phone inventory and pronunciation lexicon are not available by eliminating the dependence on human expertise this usually requires.","['Wiehan Agenbag', 'Thomas Niesler']",September 2019,Computer Speech & Language,"['Unsupervised SWU discovery', 'Automatic lexicon induction', 'ASR', 'Under-resourced languages']",Automatic sub-word unit discovery and pronunciation lexicon induction for ASR with application to under-resourced languages
32,"In the current work, an efficient approach has been implemented to model the fractional delay in the elongated cylindrical segments of the vocal tract in waveguide modeling. The vocal tract has been divided into uniform cylindrical segments of the variable lengths. In this case, the time taken by the sound wave to propagate through a cylindrical segment in an axial direction may not be an integer multiple of each other. This means that the delay in an axial direction is necessarily a fractional delay for each fractional elongated segment. In the previous work, to accommodate the fractional delay for each elongated cylindrical segment, two extra cylindrical segments of same lengths were added to maintain the even number of segments. In the proposed work, we add only a single extra segment for each fractional elongated segment which reduces memory and computational cost as well. To keep the even number of segments, we assume that the extra and original segments constitute a single long segment. Lagrange interpolation is used for the approximation of the fractional delay. The proposed model has been devised for the elongation of any arbitrary cylindrical segment by a suitable scaling factor. These results are validated with an accurate benchmark model. This model has a single algorithm and there is no need to make sections of the segments for the elongation of the intermediate segments.","['Tahir Mushtaq Qureshi', 'Khalid Saifullah Syed']",September 2019,Computer Speech & Language,"['Waveguide', 'Vocal tract', 'Fractional delay', 'Elongation of segment']",Improved vocal tract model for the elongation of segment lengths in a real time
33,"The presence of background noise or nonlinear distortions encountered in real-world situations often reduces the intelligibility of speech signals. Several objective measurements and prediction procedures have been developed to assess speech intelligibility in noise. Most of the existing measures are, however, suitable for only a subset of specified forms of distortion. This study developed a reliable, reference-free speech intelligibility metric that uses the properties of an acoustic signal to predict the effects of a wide range of distortions that influence speech intelligibility in quiet and noisy conditions. The bispectral speech intelligibility metric (BSIM), was developed by extracting the features from the spectrogram of speech signals using the third-order statistics, which are collectively known as the bispectrum. Speech intelligibility scores predicted by the BSIM were compared to behavioral speech intelligibility scores in quiet and noise. The performance of the BSIM was also compared with that of several widely used speech intelligibility metrics. Results showed that the BSIM can successfully predict nonlinear distortions, such as peak-clipping and center-clipping, as well as time domain distortions, such as phase-jitter and reverberation. Unlike existing metrics, such as the articulation index and speech transmission index, the BSIM successfully captured the effect of fluctuating noise on speech intelligibility and predicted the effects of the degradation of noisy speech processed by the ideal time-frequency segregation method. The BSIM presents a reliable, reference-free, and objective measure of speech intelligibility that can provide real-time predictions of the effect of signal processing and acoustics distortion on speech intelligibility in quiet and noise. In addition, the BSIM could be used to analyze algorithms that process noisy speech.","['Md Ekramul Hossain', 'Muhammad S.A. Zilany', 'Evelyn Davies-Venn']",September 2019,Computer Speech & Language,"['Speech intelligibility', 'Spectrogram', 'Higher order statistics', 'Bispectrum']",On the feasibility of using a bispectral measure as a nonintrusive predictor of speech intelligibility
34,"Being able to tell and understand stories is a key component for efficient communications. The narrative structure is the structural framework that underlies the order and manner in which this story is presented to a reader, listener, or viewer, this work presents a novel visualization methodology that can be used to extract the narrative structure of texts and present in a visual, immediate and intuitive manner. This tool efficiently summarizes the content of a text, and it also allows in depth analysis of the narrative structure. The characteristics and use of the tool are exemplified using a TED talk by Hans Rosling. Its potential then is shown using two historical speeches: President Donald Trump’s inaugural speech and I Have a Dream by Dr. Martin Luther King Jr.","['Dante Gama Dessavre', 'Jose E. Ramirez-Marquez']",September 2019,Computer Speech & Language,"['Visual text analytics', 'Exploratory analytics', 'Document summarization', 'Topic analysis', 'Interactive visualization']",Nar-A-Viz: A methodology to visually extract the narrative structure of text
35,"Keyword extraction is a building block of information retrieval. Many techniques have been developed to tackle this problem. However, most of the existing methods suffer from high computational complexity or large corpus dependency, which limits the practical applications. Indeed, given a single document, new visions and strategies are needed for keyword extraction to face the challenges. In this work, we proposed a new graph-based measure for keyword extraction, by leveraging higher-order structural features (e.g. motifs) of word co-occurrence graph. The experiments on real datasets shows superior performance of the proposed method, compared to TF-IDF and PageRank based methods.","['Yan Chen', 'Jie Wang', 'Ping Li', 'Peilun Guo']",September 2019,Computer Speech & Language,"['Word graph', 'Motifs', 'Keyword extraction']",Single document keyword extraction via quantifying higher-order structural features of word co-occurrence graph
36,"A dialog act represents the communicative function of an utterance in a conversation, and thus provides informative cues for understanding, managing, and generating dialog. While most spoken dialog systems process user input and system output at the turn level, a single turn can consist of multiple dialog acts in human conversations. Therefore, segmenting turn-level tokens into a meaningful dialog act unit is just as important as recognizing the dialog act. Towards joint segmentation and recognition of dialog acts, we propose an encoder–decoder model featuring joint coding and incorporate contextual information by means of an attentional mechanism. The proposed encoder–decoder outperforms other models in segmentation, and the application of attentions significantly reduces recognition error rates. By combining the encoder–decoder model with contextual attention, we achieve state-of-the-art performance in the joint evaluation of dialog act segmentation and recognition.","['Tianyu Zhao', 'Tatsuya Kawahara']",September 2019,Computer Speech & Language,"['Spoken dialog system', 'Spoken language understanding', 'Dialog act segmentation', 'Dialog act recognition']",Joint dialog act segmentation and recognition in human conversations using attention to dialog context
37,"RoEmoLex (Romanian Emotion Lexicon) is a resource developed for emotion detection in Romanian language, now at its third version. Initially translated automatically from an English version and containing terms annotated with eight primary emotions and two polarity tags, this lexicon has undergone a series of changes. New tags were attached to words, including derived emotions, part-of-speech, additional polarity scores and conceptual category information. Moreover, the lexicon was expanded with synonyms of the original terms and words and phrases from similar resources. This paper presents the current version of the emotion lexicon, describing improvements over the previous forms and presenting summary statistics. Through Formal Concept Analysis, a knowledge discovery technique, we examine the structure and content of the lexicon. Dependencies between words at the emotional level are discovered, conceptual hierarchies are built and comparisons with information from another Romanian lexical resource (RoWordNet) are made. The results encourage further research and the possibility to integrate RoEmoLex as a subcomponent of emotion analysis systems for Romanian text.","['Mihaiela Lupea', 'Anamaria Briciu']",September 2019,Computer Speech & Language,"['Natural language processing', 'Emotion analysis', 'RoEmoLex', 'Formal Concept Analysis']",Studying emotions in Romanian words using Formal Concept Analysis
38,,[],July 2019,Computer Speech & Language,[],Editorial Board
39,"One of the most popular approaches to machine translation consists in formulating the problem as a pattern recognition approach. Under this perspective, bilingual corpora are precious resources, as they allow for a proper estimation of the underlying models. In this framework, selecting the best possible corpus is critical, and data selection aims to find the best subset of the bilingual sentences from an available pool of sentences such that the final translation quality is improved. In this paper, we present a new data selection technique that leverages a continuous vector-space representation of sentences. Experimental results report improvements compared not only with a system trained only with in-domain data, but also compared with a system trained on all the available data. Finally, we compared our proposal with other state-of-the-art data selection techniques (Cross-entropy selection and Infrequent ngrams recovery) in two different scenarios, obtaining very promising results with our proposal: our data selection strategy is able to yield results that are at least as good as the best-performing strfategy for each scenario. The empirical results reported are coherent across different language pairs.","['Mara Chinea-Rios', 'Germán Sanchis-Trilles', 'Francisco Casacuberta']",July 2019,Computer Speech & Language,"['Statistical machine translation', 'Data selection', 'Continuous vector-space representation', 'Cross-entropy', 'Infrequent ngrams recovery']",Vector sentences representation for data selection in statisticalmachine translation
40,"Varied manifestations of social communication deficits, atypical prosody, and restricted and repetitive behaviors are often observed in individuals with autism spectrum disorder (ASD). The pervasiveness and heterogeneity in ASD have made it an increasingly important interdisciplinary research domain. The categorizations in ASD, ie. Autistic disorder, High-functioning autism, Asperger’ syndrome, has varied throughout the past versions of Diagnostic and Statistical Manual of Mental Disorders (DSM) in order to have a better description of ASD. Using computational approach in characterizing these neurodevelopmental disorders is, therefore, important for characterizing relevant behavior constructs consistently with potential wide applicability. In this work, we propose to compute signal-derived multimodal behavior descriptors of ASD subjects during dyadic interactions of Autism Diagnostic Observation Schedule (ADOS), and we further examine these behavior features’ discriminatory power in differentiating between the three groups in ASD: Autistic disorder (AD), Asperger’ syndrome (AS), and High-functioning autism (HFA). Additionally by combining the assessment of ASD subject’s executive functions, i.e., measured by Cambridge Neuropsychological Test Automated Battery (CANTAB), the classification accuracy improved further especially on AD versus AS. Finally, we found a moderate correlation between turn-taking duration in our computed behavior features and measures of the Rapid Visual Information Processing in CANTAB.","['Chin-Po Chen', 'Susan Shur-Fen Gau', 'Chi-Chun Lee']",July 2019,Computer Speech & Language,"['Behavioral signal processing', 'Autism spectrum disorder', 'Multimodal behaviors descriptors', 'Executive functions', 'Differential diagnosis']",Toward differential diagnosis of autism spectrum disorder using multimodal behavior descriptors and executive functions☆☆☆
41,"Sentiment analysis has become a phenomenon with the proliferation of social media and the popularity of opinion-rich resources such as online reviews and blogs. Even though significant advances have been achieved in this field, there are still some major challenges to be addressed – i.e. sentiment analysis in multiple languages or thematic domains. Only a few studies have focused on minor or morphologically rich languages. Moreover, it is a question of whether the results of sentiment analysis could be further improved by incorporating the surrounding context (local or chronological) of the analyzed document. This paper presents a language- and domain-independent sentiment analysis model based on character n-grams which improves the classifiers performance by utilizing surrounding context.Four experiments on various datasets were conducted in order to validate the model. The datasets included a reference corpus containing movie reviews in English, movie reviews in the Czech language, the bestselling Amazon book of 2012 Fifty Shades of Grey novel reviews dataset from three Amazon language mutations (English, German, and French), another reference corpus containing Amazon reviews in multiple languages (German, French, and Japanese), and a multi-domain dataset (movies, books, and product categories ranging from electronics and home appliances to sports gear and supplies for hobbies and pets).The experiments confirmed the approach of incorporating the surrounding context in order to be effective for datasets from various languages and domains, suggesting a strong performance of a character n-gram based model for multi-domain and language datasets as well. A simple all-in-one classifier, which uses a mixture of labeled data from multiple languages (or domains) to train a sentiment classification model, may rival more sophisticated domain/language adaptation techniques. Such an approach reflects the needs of companies – with the interconnectedness of today's world, most companies operate across multiple markets and would find it difficult to obtain a specific sentiment analysis solution for each market they serve.","['Tomáš Kincl', 'Michal Novák', 'Jiří Přibil']",July 2019,Computer Speech & Language,"['Sentiment analysis', 'Cross-domain', 'Multilingual', 'Surrounding context', 'Morphologically rich languages']",Improving sentiment analysis performance on morphologically rich languages: Language and domain independent approach
42,"Representation of words in different languages is fundamental for various cross-lingual applications. In the past researches, there was an argument in using or not using word alignment in learning bilingual word representations. This paper presents a comprehensive empirical study on the uses of parallel corpus to learn the word representations in the embedding space. Various non-alignment and alignment approaches are explored to formulate the contexts for Skip-gram modeling. In the approaches without word alignment, concatenating A and B, concatenating B and A, interleaving A with B, shuffling A and B, and using A and B separately are considered, where A and B denote parallel sentences in two languages. In the approaches with word alignment, three word alignment tools, including GIZA++, TsinghuaAligner, and fast_align, are employed to align words in sentences A and B. The effects of alignment direction from A to B or from B to A are also discussed. To deal with the unaligned words in the word alignment approach, two alternatives, using the words aligned with their immediate neighbors and using the words in the interleaving approach, are explored. We evaluate the performance of the adopted approaches in four tasks, including bilingual dictionary induction, cross-lingual information retrieval, cross-lingual analogy reasoning, and cross-lingual word semantic relatedness. These tasks cover the issues of translation, reasoning, and information access. Experimental results show the word alignment approach with conditional interleaving achieves the best performance in most of the tasks.","['An-Zi Yen', 'Hen-Hsen Huang', 'Hsin-Hsi Chen']",July 2019,Computer Speech & Language,"['Cross-lingual applications', 'Distributed word representation', 'Word alignment']",Learning English–Chinese bilingual word representations from sentence-aligned parallel corpus☆
43,"We use the Average Word Embeddings (AWE) model for retrieving relevant CVs based on a job description. We designed experiments to demonstrate that the trained vectors, obtained from a balanced domain corpus, are better than using pre-trained word embeddings. We also present some experiments to show that different combinations of both word embeddings spaces increase the overall accuracy of the retrieval task compared to using only the pre-trained vectors. However an issue arised when both embeddings spaces are not sharing the same dimensions and terms, as shown in our case. In order to handle this situation, we suggest to use a method to reduce dimensions of pre-trained vectors (e.g. PCA), and combine them with our trained vectors. This improves the accuracy of the retrieval task for unseen CVs. Our main contribution is to create a model that detects which embeddings need to be used in order to maximize the relevant retrieval results.","['Francis C. Fernández-Reyes', 'Suraj Shinde']",July 2019,Computer Speech & Language,"['Word embeddings', 'Information retrieval models', 'Semantic vectors']",CV Retrieval System based on job description matching using hybrid word embeddings
44,"Autism Spectrum Disorder (ASD), a neurodevelopmental disability, has become one of the high incidence diseases among children. Studies indicate that early diagnosis and intervention treatments help to achieve positive longitudinal outcomes. In this paper, we focus on the speech and language abnormalities of young children with ASD and present an automated assessment framework in quantifying atypical prosody and stereotyped idiosyncratic phrases related to ASD. For detecting atypical prosody from speech, we propose both the hand-crafted feature based method as well as the end-to-end deep learning framework. First, we use the OpenSMILE toolkit to extract utterance level high dimensional acoustic features followed by a support vector machine (SVM) backend as the conventional baseline. Second, we propose several end-to-end deep neural network setups and configurations to model the atypical prosody label directly from the constant Q transform spectrogram of speech. Third, we apply cross-validation on the training data to perform segments selection and enhance the subject level classification performance. Fourth, we fuse the deep learning based methods with the conventional baseline at the score level to further enhance the overall system performance. For detecting the stereotyped idiosyncratic usage of words or phrases from speech transcripts, we adopt language model, dependency treebank and Term Frequency–Inverse Document Frequency (TF–IDF) in addition to Linguistic Inquiry and Word Count software (LIWC) methods to extract a set of text features followed by a standard SVM backend. We collect a database of spontaneous Mandarin speech recorded during the Autism Diagnostic Observation Schedule (ADOS) Module 2 and Module 3 sessions. The Module 2 part consists of 118 children while the Module 3 part includes 71 children. Experimental results on this database show that our proposed methods can effectively predict the atypical prosody and stereotyped idiosyncratic phrases codes for young children with the risk of ASD. On the two categories classification task, the unweighted accuracy of the aforementioned two tasks are 88.1% and 77.8%, respectively.","['Ming Li', 'Dengke Tang', 'Junlin Zeng', 'Tianyan Zhou', 'Huilin Zhu', 'Biyuan Chen', 'Xiaobing Zou']",July 2019,Computer Speech & Language,"['Autism spectrum disorder', 'Atypical prosody', 'Stereotyped idiosyncratic phases', 'Recurrent neural network', 'Convolutional neural network']",An automated assessment framework for atypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder
45,"In this article, we describe an empirical evaluation of compounds indexing for Turkish texts. We dive beyond the keyword indexing to propose a framework for Turkish compounds extraction and indexing. We identify twelve Turkish compounds pattern types that we classify in six categories. To extract Turkish compounds, we rely on a light natural language processing approach based on syntactic pattern recognition. We compare different compounds indexing strategies. We also investigate the effectiveness of using one compounds type and the effectiveness of combining different compound types. We conduct experiments over the Milliyet test dataset. The results of our experiments show that using compounds as index terms can improve retrieval performances. However, not all the compound types have a positive impact on the retrieval process.","['Chedi Bechikh Ali', 'Hatem Haddad', 'Yahya Slimani']",July 2019,Computer Speech & Language,"['Information retrieval', 'Indexing', 'Turkish language', 'Compound', 'Syntactic patterns']",Empirical evaluation of compounds indexing for Turkish texts
46,"Sentence representation at the semantic level is a challenging task for natural language processing and Artificial Intelligence. Despite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. The weights of the series are fitted by using Shannon’s Mutual Information (MI) among words, sentences and the corpus. In fact, the Term Frequency–Inverse Document Frequency transform (TF–IDF) is a reliable estimate of such MI. Our method offers advantages over existing ones: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and linguistic annotation resources. Results showed that our model, despite its concreteness and low computational cost, was competitive with the state of the art in well-known Semantic Textual Similarity (STS) tasks.","['Ignacio Arroyo-Fernández', 'Carlos-Francisco Méndez-Cruz', 'Gerardo Sierra', 'Juan-Manuel Torres-Moreno', 'Grigori Sidorov']",July 2019,Computer Speech & Language,"['Sentence representation', 'Sentence embedding', 'Word embedding', 'Information entropy', 'TF–IDF', 'Natural language processing']",Unsupervised sentence representations as word information series: Revisiting TF–IDF
47,"Natural language understanding (NLU) is a core technology for implementing natural interfaces and has received much attention in recent years. While learning embedding models has yielded fruitful results in several NLP subfields, most notably Word2Vec, embedding correspondence has relatively not been well explored especially in the context of NLU, a task that typically extracts structured semantic knowledge from a text. A NLU embedding model can facilitate analyzing and understanding relationships between unstructured texts and their corresponding structured semantic knowledge, essential for both researchers and practitioners of NLU. Toward this end, we propose a framework that learns to embed semantic correspondence between text and its extracted semantic knowledge, called semantic frame. One key contributed technique is semantic frame reconstruction used to derive a one-to-one mapping between embedded vectors and their corresponding semantic frames. Embedding into semantically meaningful vectors and computing their distances in vector space provides a simple, but effective way to measure semantic similarities. With the proposed framework, we demonstrate three key areas where the embedding model can be effective: visualization, distance based semantic search, similarity-based intent classification and re-ranking.",['Sangkeun Jung'],July 2019,Computer Speech & Language,"['Natural language understanding', 'Semantic frame learning', 'Deep learning', 'Distributed representation', 'Semantic vector', 'Semantic Corpus Visualization']",Semantic vector learning for natural language understanding☆
48,,[],May 2019,Computer Speech & Language,[],Editorial Board
49,"This paper describes the experimental setups and the evaluation results of the sixth Dialog System Technology Challenges (DSTC6) aiming to develop end-to-end dialogue systems. Neural network models have become a recent focus of investigation in dialogue technologies. Previous models required training data to be manually annotated with word meanings and dialogue states, but end-to-end neural network dialogue systems learn to directly output natural-language system responses without needing training data to be manually annotated. Thus, this approach allows us to scale up the size of training data and cover more dialog domains. In addition, dialogue systems require a meta-function to avoid deploying inappropriate responses generated by themselves. To challenge such issues, the DSTC6 consists of three tracks, (1). End-to-End Goal Oriented dialogue Learning to select system responses, (2). End-to-End Conversation Modeling to generate system responses using Natural Language Generation (NLG) and (3). Dialogue Breakdown Detection. Since each domain has different issues to be addressed to develop dialogue systems, we targeted restaurant retrieval dialogues to fill slot-value in Track 1, customer services on Twitter by combining goal-oriented dialogues and ChitChat in Track 2 and human-machine dialogue data for ChitChat in Track 3.DSTC6 had 141 people declaring their interests and 23 teams submitted their final results. 18 scientific papers were presented in the wrap-up workshop. We find the blending end-to-end trainable models associated to meaningful prior knowledge performs the best for the restaurant retrieval for Track 1. Indeed, Hybrid Code Network and Memory Network have been the best models for this task. In Track 2, 78.5% of the system responses automatically generated by the best system were rated better than acceptable by humans and this achieves 89% of the number of the human responses rated in the same class. In Track3, the dialogue breakdown detection technologies performed as well as human agreements, in both data-sets of English and Japanese.","['Chiori Hori', 'Julien Perez', 'Ryuichiro Higashinaka', 'Takaaki Hori', 'Y-Lan Boureau', 'Michimasa Inaba', 'Yuiko Tsunomori', 'Tetsuro Takahashi', 'Koichiro Yoshino', 'Seokhwan Kim']",May 2019,Computer Speech & Language,"['DSTC', 'End-to-end dialogue system', 'Conversation model', 'Sequence-to-sequence model', 'Natural Language Generation', 'Dialogue breakdown']",Overview of the sixth dialog system technology challenge: DSTC6☆
50,"Modifying clean speech prior to output in noisy conditions can lead to substantial intelligibility gains. Most algorithms operate by redistributing energy across the signal, leaving the timing of the underlying speech sounds intact. Other techniques do alter the timing of speech relative to the masker. Both classes of approach – spectral and temporal – lead to a reduction in energetic masking. The current study examines how their combination affects intelligibility. Arguments can be made for both synergy and redundancy, and the presence of distortions introduced by both spectral and temporal approaches might even lead to an antagonistic combination. A cohort of native Spanish listeners identified keywords in sentences in unmodified form and following spectral, temporal and spectro-temporal modification, in the presence of a fluctuating masker. Errors in the spectro-temporal condition were substantially lower than following spectral or temporal modification alone, with a three-fold reduction compared to unmodified speech. Spectro-temporal gains were observed for all phonemes. A glimpse-based model of energetic masking incorporating speech rate changes predicts intelligibility (r=.96), and a glimpsing analysis provides further insights into the distinct mechanisms through which spectral and temporal approaches lead to a release from energetic masking.","['Martin Cooke', 'Vincent Aubanel', 'María Luisa García Lecumberri']",May 2019,Computer Speech & Language,"['Speech modification', 'Intelligibility', 'Retiming', 'Glimpsing']",Combining spectral and temporal modification techniques for speech intelligibility enhancement☆
51,"The ability to track depression severity over time using passive sensing of speech would enable frequent and inexpensive monitoring, allowing rapid assessment of treatment efficacy as well as improved long term care of individuals at high risk for depression. In this paper an algorithm is proposed that estimates the articulatory coordination of speech from audio and video signals, and uses these coordination features to learn a prediction model to track depression severity with treatment. In addition, the algorithm is able to adapt its prediction model to an individual’s baseline data in order to improve tracking accuracy. The algorithm is evaluated on two data sets. The first is the Wyss Institute Biomarkers for Depression (WIBD) multi-modal data set, which includes audio and video speech recordings. The second data set was collected by Mundt et al (2007) and contains audio speech recordings only. The data sets are comprised of patients undergoing treatment for depression as well as control subjects. In its within-subject tracking of clinical Hamilton depression (HAM-D) ratings, the algorithm achieves root mean squared error (RMSE) of 5.49 with Spearman correlation of r = 0.63 on the WIBD data set, and achieves RMSE = 5.99 with r = 0.48 on the Mundt data set.","['James R. Williamson', 'Diana Young', 'Andrew A. Nierenberg', 'James Niemi', 'Brian S. Helfer', 'Thomas F. Quatieri']",May 2019,Computer Speech & Language,"['Depression', 'Speech', 'Articulation', 'Coordination', 'Audio', 'Video']",Tracking depression severity from audio and video based on speech articulatory coordination☆☆☆
52,"Sentiment lexicons including opinion words, sentiment phrases, and idioms with sentiment polarities play an important role in sentiment analysis tasks. Apart from explicit sentiment features, extracting implicit sentiment features is a challenging research issue. The sentiment expression is very domain-specific, and constructing a general sentiment lexicon that is suitable for all domains is hard or even impossible. In this paper, we propose a novel sentiment unit context propagation framework to extract Chinese microblog-specific explicit and implicit sentiment features. In the process of the selection of seed sentiment units, we select the seed sentiment units that have a large standard degree of centrality with other units, and mark these units with sentiment labels using general sentiment lexicons and manual calibrations. To realize sentiment label propagation from a small amount of labeled sentiment units to unlabeled ones, we exploit local contexts, topic features, and so`cial relationships among users in microblog social networks. After that, the sentiment scores of units are calculated using unit context sentiment propagation. Experiments on two real-world microblog data sets demonstrate that our method can generate microblog-specific sentiment lexicons effectively. Furthermore, the sentiment classification accuracies significantly outperform state-of-the-art baselines.","['Chuanjun Zhao', 'Suge Wang', 'Deyu Li']",May 2019,Computer Speech & Language,"['Implicit sentiment features', 'Social relationships', 'Context propagation', 'Microblog-specific sentiment lexicons', 'Sentiment classification']",Exploiting social and local contexts propagation for inducing Chinese microblog-specific sentiment lexicons☆
53,"Deep neural networks (DNNs) have recently been successful in many applications and have become a popular approach for speech recognition. Training a DNN model for speech recognition is computationally expensive due to the model large number of parameters. Pre-training improves DNN modeling. However, DNN learning is challenging if pre-training is inefficient. This paper introduces a new framework for pre-training that utilizes label information in lower layers (layers near input) for better recognition. The proposed pre-training method dynamically inserts discriminative information not only in the last layer but also in other layers. In this algorithm, the lower layers achieve more generative information while the higher layers achieve more discriminative information. In addition, this method uses speaker information by employing the Subspace Gaussian Mixture Model (SGMM), which improves recognition accuracy. Experimental results on TIMIT, MNIST, Switchboard, and English Broadcast News datasets show that this approach significantly outperforms current state-of-the-art methods such as the Deep Belief Network and the Deep Boltzmann Machine. Moreover, the proposed algorithm has minimal memory requirements.","['Toktam Zoughi', 'Mohammad Mehdi Homayounpour']",May 2019,Computer Speech & Language,"['Speech recognition', 'Deep neural networks', 'Deep boltzmann machine', 'Pre-training', 'Subspace gaussian mixture model']",DBMiP: A pre-training method for information propagation over deep networks☆
54,"This paper proposes and compares a range of methods to improve the naturalness of visual speech synthesis. A feedforward deep neural network (DNN) and many-to-one and many-to-many recurrent neural networks (RNNs) using long short-term memory (LSTM) are considered. Rather than using acoustically derived units of speech, such as phonemes, viseme representations are considered and we propose using dynamic visemes together with a deep learning framework. The input feature representation to the models is also investigated and we determine that including wide phoneme and viseme contexts is crucial for predicting realistic lip motions that are sufficiently smooth but not under-articulated. A detailed objective evaluation across a range of system configurations shows that a combined dynamic viseme-phoneme speech unit combined with a many-to-many encoder-decoder architecture models visual co-articulations effectively. Subjective preference tests reveal there to be no significant difference between animations produced using this system and using ground truth facial motion taken from the original video. Furthermore, the dynamic viseme system also outperforms significantly conventional phoneme-driven speech animation systems.","['Ausdang Thangthai', 'Ben Milner', 'Sarah Taylor']",May 2019,Computer Speech & Language,"['Talking head', 'Visual speech synthesis', 'Deep neural network', 'Dynamic visemes']",Synthesising visual speech using dynamic visemes and deep learning architectures☆
55,"With the improved quality of Machine Translation (MT) systems in the last decades, post-editing (the correction of MT errors) has gained importance in Computer-Assisted Translation (CAT) workflows. Depending on the number and the severity of the errors in the MT output, the effort required to post-edit varies from sentence to sentence. The existing Quality Estimation (QE) systems provide quality scores that reflect the quality of an MT output at sentence level or word level. However, they fail to explain the relationship between different types of MT errors and the required post-editing effort to correct them. We suggest a more informative approach to QE in which different types of MT errors are detected in a first step, which are then used to estimate post-editing effort in a second step. In this paper we define the upper boundary of such a system. We use different machine learning methods to estimate Post-Editing Time (PET) by using a gold-standard set of MT errors as features. We show that post-editing time can be estimated with high accuracy when all the translation errors in the MT output are known. Furthermore, we apply feature selection methods and investigate the predictive power of different MT error types on PET. Our results show that the same prediction performance can be achieved by only using a small subset of MT error types, indicating that successful two-step QE systems can be built with less effort in the future, by detecting only the error types with highest predictive power.","['Arda Tezcan', 'Véronique Hoste', 'Lieve Macken']",May 2019,Computer Speech & Language,"['Machine translation', 'Quality estimation', 'Post-editing', 'Machine learning', 'Feature selection']",Estimating post-editing time using a gold-standard set of machine translation errors☆
56,"Overlapping speech is a natural and frequently occurring phenomenon in human–human conversations with an underlying purpose. Speech overlap events may be categorized as competitive and non-competitive. While the former is an attempt to grab the floor, the latter is an attempt to assist the speaker to continue the turn. The presence and distribution of these categories are indicative of the speakers’ states during the conversation. Therefore, understanding these manifestations is crucial for conversational analysis and for modeling human–machine dialogs. The goal of this study is to design computational models to classify overlapping speech segments of dyadic conversations into competitive vs. non-competitive acts using lexical and acoustic cues, as well as their surrounding context. The designed overlap representations are evaluated in both linear – Support Vector Machines (SVM) – and non-linear – feed-forward (FFNN), convolutional (CNN) and long short-term memory (LSTM) neural network – models. We experiment with lexical and acoustic representations and their combinations from both speaker channels in feature and hidden space. We observe that lexical word-embedding features significantly increase the overall F1-measure compared to both acoustic and bag-of-ngrams lexical representations, suggesting that lexical information can be utilized as a powerful cue for overlap classification. Our comparative study shows that the best computational architecture is an FFNN along with a combination of word embeddings and acoustic features.","['Shammur Absar Chowdhury', 'Evgeny A. Stepanov', 'Morena Danieli', 'Giuseppe Riccardi']",May 2019,Computer Speech & Language,"['Overlap', 'Acoustic', 'Lexical', 'Deep learning', 'Spoken conversation']",Automatic classification of speech overlaps: Feature representation and algorithms☆☆☆
57,"Speaker adaptation techniques can be classified as intra-lingual or cross-lingual depending on whether or not the source model and the target speaker employ the same language. Most of the work in this field has been focused on the first case, while the second one has been less explored. In this paper we address the cross-lingual paradigm in the framework of a HMM-based speech synthesis system by further developing a formerly proposed approach. This method is able to clone a given speaker into a different language by combining the linguistic structure and the acoustic characteristics of two HTS models. In this work, we discuss the extension of the adaptation procedure to some other source model parameters that were kept unmodified in the initial version, and compare the performance of both versions by means of subjective and objective tests. These results are also contrasted with those obtained by a KLD-based technique proposed in the literature for a similar purpose. While no significant preference for any of the versions of our method is observed, our approach clearly outperforms the KLD-based technique.","['Carmen Magariños', 'Daniel Erro', 'Eduardo R. Banga']",May 2019,Computer Speech & Language,"['HMM-based speech synthesis', 'Cross-lingual speaker adaptation', 'Voice cloning', 'Polyglot synthesis', 'Multilingual synthesis']",Language-independent acoustic cloning of HTS voices☆
58,"This paper addresses errors in continuous Automatic Speech Recognition (ASR) in two stages: error detection and error type classification. Unlike the majority of research in this field, we propose to handle the recognition errors independently from the ASR decoder. We first establish an effective set of generic features derived exclusively from the recognizer output to compensate for the absence of ASR decoder information. Then, we apply a variant Recurrent Neural Network (V-RNN) based models for error detection and error type classification. Such model learn additional information to the recognized word classification using label dependency. As a result, experiments on Multi-Genre Broadcast Media corpus have shown that the proposed generic features setup leads to achieve competitive performances, compared to state of the art systems in both tasks. Furthermore, we have shown that V-RNN trained on the proposed feature set appear to be an effective classifier for the ASR error detection with an Accuracy of 85.43%.","['Rahhal Errattahi', 'Asmaa EL Hannani', 'Thomas Hain', 'Hassan Ouahmane']",May 2019,Computer Speech & Language,"['Automatic Speech Recognition', 'ASR error detection', 'ASR error type classification', 'Recurrent Neural Network']",System-independent ASR error detection and classification using Recurrent Neural Network
59,"End-to-end dialog systems are gaining interest due to the recent advances of deep neural networks and the availability of large human–human dialog corpora. However, in spite of being of fundamental importance to systematically improve the performance of this kind of systems, automatic evaluation of the generated dialog utterances is still an unsolved problem. Indeed, most of the proposed objective metrics shown low correlation with human evaluations. In this paper, we evaluate a two-dimensional evaluation metric that is designed to operate at sentence level, which considers the syntactic and semantic information carried along the answers generated by an end-to-end dialog system with respect to a set of references. The proposed metric, when applied to outputs generated by the systems participating in track 2 of the DSTC-6 challenge, shows a higher correlation with human evaluations (up to 12.8% relative improvement at the system level) than the best of the alternative state-of-the-art automatic metrics currently available.","[""Luis Fernando D'Haro"", 'Rafael E. Banchs', 'Chiori Hori', 'Haizhou Li']",May 2019,Computer Speech & Language,"['Automatic evaluation metrics', 'dialog systems', 'DSTC', 'adequacy and fluency']",Automatic evaluation of end-to-end dialog systems with adequacy-fluency metrics☆
60,,"['Yang Liu', 'Kun Wang', 'Chengqing Zong', 'Keh-Yih Su']",May 2019,Computer Speech & Language,[],"CorrigendumCorrigendum to ‘A unified framework and models for integrating translation memory into phrase-based statistical machine translation’ [Volume 54, March 2019, Pages 176-206]"
61,,[],March 2019,Computer Speech & Language,[],Editorial Board
62,"The dialog state tracker is one of the most important modules on task-oriented dialog systems, its accuracy strongly affects the quality of the system response. The architecture of the tracker has been changed from pipeline processing to an end-to-end approach that directly estimates a user’s intention from each current utterance and a dialog history because of the growth in the use of the neural-network-based classifier. However, tracking appropriate slot-value pairs of dialog states that are not explicitly mentioned in user utterances is still a difficult problem. In this research, we propose creating feature vectors by using inference results on an external knowledge base. This inference process predicts associative entities in the knowledge base, which contribute to the dialog state tracker for unseen entities of utterances. We extracted a part of a graph structure from an external knowledge base (Wikidata). Label propagation was used for inferring associative nodes (entities) on the graph structure to produce feature vectors. We used the vectors for the input of a fully connected neural network (FCNN) based tracker. We also introduce a convolutional neural network (CNN) tracker as a state-of-the-art tracker and ensemble models of FCNN and CNN trackers. We used a common test bed, Dialog State Tracking Challenge 4 for experiments. We confirmed the effectiveness of the associative knowledge feature vector, and one ensemble model outperformed other models.","['Yukitoshi Murase', 'Yoshino Koichiro', 'Satoshi Nakamura']",March 2019,Computer Speech & Language,"['Dialog state tracking', 'Knowledge base', 'Knowledge graph', 'Associative knowledge inference']",Associative knowledge feature vector inferred on external knowledge base for dialog state tracking☆
63,"rWe propose Quantized Dialog, a novel approach for the development of conversational systems. The methodology relies on the semantic quantization and clustering of the dialog utterances in order to reduce the dialog interaction space, making prediction of the next utterance more tractable. The effectiveness of this method is showcased using the goal-oriented dataset of the sixth Dialog System Technology Challenge (DSTC6). We compare the performance of Quantized Dialog based on an n-gram language model for next-utterance prediction against other models that employ popular deep-learning architectures, such as multi-layer neural network classifiers, memory networks, long short-term memory recurrent neural networks and convolutional neural networks. The experimental results demonstrate the promising potential of the new quantized approach in goal-oriented dialog prediction.","['R. Chulaka Gunasekara', 'David Nahamoo', 'Lazaros C. Polymenakos', 'David Echeverría Ciaurri', 'Jatin Ganhotra', 'Kshitij P. Fadnis']",March 2019,Computer Speech & Language,"['Dialog', 'Conversation', 'Embedding', 'Clustering', 'Language model']",Quantized Dialog – A general approach for conversational systems☆
64,"Dialogue breakdown is a significant problem in conversational agents. Timely breakdown detection helps the agents quickly recover from mistakes, minimizing the impact on user experience. In this paper, we focus on two problems: variations in determining a response that breakdowns a conversation i.e., subjectivity, and variations in breakdown types due to designs of conversational agents, i.e., variationality. To address the subjectivity, which decreases the agreement rate among annotators, our methods detect a dialogue breakdown by ensembling detectors trained by different sets of annotators that are grouped using a clustering algorithm. To address the variationality, our methods apply two types of detector architectures to capture global and local breakdowns. The long short-term memory detector considers the global context and the convolutional neural networks detector is sensitive to the local characteristics. The ensemble of all detectors makes a final judgment. The results of the Japanese task in the Dialogue Breakdown Detection Challenge 3 (DBDC3) confirm that our approach significantly outperforms the baseline, which uses the conventional conditional random field. Detailed error analysis reveals that our encoders based on a convolutional neural network and a long short-term memory have different characteristics. It also confirms the effects of annotator clustering.","['Junya Takayama', 'Eriko Nomoto', 'Yuki Arase']",March 2019,Computer Speech & Language,"['Dialogue breakdown detection', 'Ensemble learning', 'Clustering', 'Convolutional neural network', 'Recurrent neural network']",Dialogue breakdown detection robust to variations in annotators and dialogue systems☆
65,"We created a supertagger for the Spanish language aimed at disambiguating the HPSG lexical frames for the verbs, nouns and adjectives in a sentence. The supertagger uses a maximum entropy model and achieves an accuracy of 84.16% over the verb classes, 86.60% over the noun classes and 91.30% over the adjective classes on the test set. The tagset contains 92 verb classes, 27 noun classes and 13 adjective classes extracted from a Spanish HPSG-compatible annotated corpus that was created by automatically transforming the AnCora Spanish corpus. The tags include information about the arguments structure, their syntactic categories and semantic roles. These are important pieces of HPSG style feature structures.","['Luis Chiruzzo', 'Dina Wonsever']",March 2019,Computer Speech & Language,"['HPSG', 'Supertagging', 'Parsing', 'Spanish', 'Lexical frames', 'Word embeddings']",Building a supertagger for Spanish HPSG☆
66,"Most recent speech codecs employ code excited linear prediction (CELP) and transmit side information to improve speech quality under packet loss. Another approach to achieve high robustness to packet loss is to use the frame independent coding scheme based on the internet low bitrate codec (iLBC). The scalable wideband speech codec based on the iLBC was previously presented and outperformed G.729.1 at most bit rates according to the objective quality. This paper presents improvements to the previous work. Specifically, we employ the wavelet packet transform (WPT) instead of the modified discrete cosine transform (MDCT) to enhance the quality, and evaluate the proposed codec based on both the objective and subjective quality measures. The objective quality evaluation results show that clear improvement is achieved and that the proposed codec outperforms G.729.1 at the bit rate of 18 kbps or higher under clean channel conditions and has higher robustness to packet loss than G.729.1. The informal subjective test results also show similar trends.","['Koji Seto', 'Tokunbo Ogunfunmi']",March 2019,Computer Speech & Language,"['Wavelet packet transform (WPT)', 'Internet low bitrate codec (iLBC)', 'Packet loss', 'Scalable coding', 'Speech coding', 'Voice over Internet protocol (VoIP)']",A scalable wideband speech codec using the wavelet packet transform based on the internet low bitrate codec☆
67,"For generative conversational agents, especially service-oriented systems, it is of great importance to improve the informativeness of generated responses and avoid bland results. In this paper, we describe our attempt at generating natural and informative responses for customer service oriented dialog systems, by incorporating dialog history related information and external knowledge. Two improved sequence-to-sequence frameworks are proposed to generate responses based on extra information in addition to the current user input, one encodes the entire dialogue history, while the other integrates external knowledge extracted from a search engine. The experimental results on the DSCT6-Track2 and Ubuntu Dialog corpora demonstrate that the proposed systems are promising to generate more informative responses. However, case studies suggest that some particular features of the proposed systems and the datasets might restrict the systems to fully exploit such extra information.","['Zongsheng Wang', 'Zhuoran Wang', 'Yinong Long', 'Jianan Wang', 'Zhen Xu', 'Baoxun Wang']",March 2019,Computer Speech & Language,"['Response generation', 'Conversational agent', 'Dialog history', 'External knowledge']",Enhancing generative conversational service agents with dialog history and external knowledge
68,"This paper presents a method for modifying speech to enhance its intelligibility in noise. The features contributing to intelligibility are analyzed using the recently proposed single frequency filtering (SFF) analysis of speech signals. In the SFF method, the spectral and temporal resolutions can be controlled using a single parameter of the filter, corresponding to the location of the pole on the negative real axis with respect to the unit circle in the z-plane. The SFF magnitude (envelope) and phase at several frequencies can be used to synthesize the original speech signal. Analysis of highly intelligible speech shows that the speech signal is more intelligible when it has higher dynamic range of amplitude locally (fine structure) and/or lower dynamic range of amplitude globally (gross structure) in both the spectral and temporal domains. Some features of normal speech are modified at fine and gross temporal and spectral levels, and the modified SFF envelopes are used to synthesize speech. The proposed method gives higher objective scores of intelligibility compared to original and the reference method (spectral shaping and dynamic range compression), under different conditions of noise. In subjective evaluation, though the word accuracies are not significantly different between the proposed and reference methods, listeners seem to prefer the proposed method as it gives louder and crisper sound.","['Nivedita Chennupati', 'Sudarsana Reddy Kadiri', 'Yegnanarayana B.']",March 2019,Computer Speech & Language,"['Speech intelligibility', 'single frequency filtering (SFF)', 'spectral fine structure (SFS)', 'temporal fine structure (TFS)', 'spectral gross structure (SGS)', 'temporal gross structure (TGS)']",Spectral and temporal manipulations of SFF envelopes for enhancement of speech intelligibility in noise☆
69,"Standard approaches to named entity recognition (NER) are based on sequential labeling methods, such as conditional random fields (CRFs), which label each word in a sentence and extract entities from them that correspond to named entities. With the extensive deployment of deep learning methods for sequential labeling tasks, state-of-the-art NER performance has been achieved on long short-term memory (LSTM) architectures using only basic features. In this paper, we address Korean NER tasks and propose an extension of a bidirectional LSTM CRF by investigating character-based representation. Our extension involves deploying a hybrid representation using ConvNet and LSTM for the sequential modeling of characters, namely a character-based LSTM-ConvNet hybrid representation. Using morphemes as processing units for bidirectional LSTM, we apply a proposed hybrid representation composed of morpheme vectors. Experimental results showed that the proposed LSTM-ConvNet hybrid representation yielded improvements over each single representation on standard Korean NER tasks.","['Seung-Hoon Na', 'Hyun Kim', 'Jinwoo Min', 'Kangil Kim']",March 2019,Computer Speech & Language,"['Named entity recognition', 'Long short term memory', 'Convolutional neural networks', 'Character-based composition']",Improving LSTM CRFs using character-based compositions for Korean named entity recognition
70,"This paper presents adversarial training and decoding methods for neural conversation models that can generate natural responses given dialog contexts. In our prior work, we built several end-to-end conversation systems for the 6th Dialog System Technology Challenges (DSTC6) Twitter help-desk dialog task. These systems included novel extensions of sequence adversarial training, example-based response extraction, and Minimum Bayes-Risk based system combination. In DSTC6, our systems achieved the best performance in most objective measures such as BLEU and METEOR scores and decent performance in a subjective measure based on human rating. In this paper, we provide a complete set of our experiments for DSTC6 and further extend the training and decoding strategies more focusing on improving the subjective measure, where we combine responses of three adversarial models. Experimental results demonstrate that the extended methods improve the human rating score and outperform the best score in DSTC6.","['Takaaki Hori', 'Wen Wang', 'Yusuke Koji', 'Chiori Hori', 'Bret Harsham', 'John R. Hershey']",March 2019,Computer Speech & Language,"['Dialog system', 'Conversation model', 'Sequence-to-sequence model', 'Sentence generation']",Adversarial training and decoding strategies for end-to-end neural conversation models☆
71,"Chat-oriented dialogue systems sometimes generate inappropriate response utterances to user utterances that cause dialogue breakdown. Detecting such inappropriate utterances and suppressing them will support the continuation of the dialogue. Although a previous state-of-the-art dialogue breakdown detector leveraged dialogue-act transitions and word-based similarities between utterance pairs, these features are insufficient to evaluate the appropriateness of question-answering or relatedness between utterances that share few topic words. In this paper, we propose novel features to assess these problems, and examine their effectiveness for improving the performance of dialogue breakdown detection.",['Hiroaki Sugiyama'],March 2019,Computer Speech & Language,"['Dialogue breakdown detection', 'Chat-oriented dialogue system', 'Feature analysis']",Empirical feature analysis for dialogue breakdown detection*
72,"Code-switching is the phenomenon whereby multilingual speakers spontaneously alternate between more than one language during discourse and is widespread in multilingual societies. Current state-of-the-art automatic speech recognition (ASR) systems are optimised for monolingual speech, but performance degrades severely when presented with multiple languages. We address ASR of speech containing switches between English and four South African Bantu languages. No comparable study on code-switched speech for these languages has been conducted before and consequently no directly applicable benchmarks exist. A new and unique corpus containing 14.3 hours of spontaneous speech extracted from South African soap operas was used to perform our study. The varied nature of the code-switching in this data presents many challenges to ASR. We focus specifically on how the language model can be improved to better model bilingual language switches for English-isiZulu, English-isiXhosa, English-Setswana and English-Sesotho. Code-switching examples in the corpus transcriptions were extremely sparse, with the majority of code-switched bigrams occurring only once. Furthermore, differences in language typology between English and the Bantu languages and among the Bantu languages themselves contribute further challenges. We propose a new method using word embeddings trained on text data that is both out-of-domain and monolingual for the synthesis of artificial bilingual code-switched bigrams to augment the sparse language modelling training data. This technique has the particular advantage of not requiring any additional training data that includes code-switching. We show that the proposed approach is able to synthesise valid code-switched bigrams not seen in the training set. We also show that, by augmenting the training set with these bigrams, we are able to achieve notable reductions for all language pairs in the overall perplexity and particularly substantial reductions in the perplexity calculated across a language switch boundary (between 5 and 31%). We demonstrate that the proposed approach is able to reduce the unseen code-switched bigram types in the test sets by up to 20.5%. Finally, we show that the augmented language models achieve reductions in the word error rate for three of the four language pairs considered. The gains were larger for language pairs with disjunctive orthography than for those with conjunctive orthography. We conclude that the augmentation of language model training data with code-switched bigrams synthesised using word embeddings trained on out-of-domain monolingual text is a viable means of improving the performance of ASR for code-switched speech.","['Ewald van der Westhuizen', 'Thomas R. Niesler']",March 2019,Computer Speech & Language,"['Code-switching', 'Word embeddings', 'IsiZulu', 'IsiXhosa', 'Setswana', 'Sesotho']",Synthesised bigrams using word embeddings for code-switched ASR of four South African language pairs
73,"Since statistical machine translation (SMT) and translation memory (TM) complement each other in TM matched and unmatched regions, a unified framework for integrating TM into phrase-based SMT is proposed in this paper. Unlike previous two-stage pipeline approaches, which directly merge TM results into the input sentences and subsequently let the SMT only translates those unmatched regions, the proposed framework refers to the corresponding TM information associated with each phrase at the SMT decoding. Under this unified framework, several integrated models are proposed to incorporate different types of information extracted from TM to guide the SMT decoding. We thus let SMT implicitly and indirectly utilize global context with a local dependency model. Furthermore, the SMT phrase table is dynamically enhanced with TM phrase pairs when the TM database and the SMT training set are different.On a Chinese–English TM database, our experiments show that the proposed Model-I significantly improves over both SMT and TM when the SMT training set is also adopted as the TM database and when the fuzzy match score is over 0.4 (overall 3.5 BLEU points improvement and 2.6 TER points reduction). In addition, the proposed Model-II is significantly better than the TM and the SMT systems when the SMT training set and the TM database are different. Furthermore, the proposed Model-III outperforms both the TM and the SMT systems even when the SMT training set and the TM database are from different domains. Additionally, the proposed Model-IV further achieves significant improvements with the help of Top-N TM sentence pairs. Lastly, all our models significantly outperform those state-of-the-art approaches under all test conditions.","['Yang Liu', 'Kun Wang', 'Chengqing Zong', 'Keh-Yih Su']",March 2019,Computer Speech & Language,"['Phrase-based machine translation', 'Translation memory']",A unified framework and models for integrating translation memory into phrase-based statistical machine translation☆
74,,[],January 2019,Computer Speech & Language,[],Editorial Board
75,"In shouting, speakers use increased vocal effort to convey spoken messages over distance or above environmental noise. For automatic speaker recognition systems trained using normal speech, shouting causes a severe vocal effort mismatch between the enrollment and test hence reducing the recognition performance. In this study, two compensation methods are proposed to tackle the mismatch in a shouted versus normal speaker recognition task. These techniques are applied in the feature extraction stage of a speaker recognition system to modify the spectral envelopes of shouts to be closer to those in normal speech. The techniques modify the all-pole power spectrum of the MFCC computation chain with shouted-to-normal compensation filtering that is obtained using a GMM-based statistical mapping. In an evaluation using the state-of-the-art i-vector based recognition system, the proposed techniques provided considerable improvements in identification rates compared to the case when shouted speech spectra were not processed.","['Emma Jokinen', 'Rahim Saeidi', 'Tomi Kinnunen', 'Paavo Alku']",January 2019,Computer Speech & Language,"['Speaker recognition', 'Vocal effort mismatch', 'Shouted speech']",Vocal effort compensation for MFCC feature extraction in a shouted versus normal speaker recognition task☆
76,"In this paper, we introduce the Task Dependent Recurrent Entity Network (TDREN) to solve Dialogue System Technology Challenges 6 (DSTC 6) track 1. Traditionally, there have been methods such as collecting the intent of the user in a conversation directly using rules. We design an end-to-end structure that properly models the restaurant pre-related user preferences that appear in the dialogue and gives appropriate responses. We perform experiments on the TDREN and achieved 97.7% at precision 1. We propose a new artificial neural network structure and recurrent cell for modeling user preference information. Then, we show that task-oriented dialogue modeling experiment results using the structure and the recurrent cell.","['Chang-Uk Shin', 'Jeong-Won Cha']",January 2019,Computer Speech & Language,[],End-to-end task dependent recurrent entity network for goal-oriented dialog learning☆
77,"Although parallel corpora are essential language resources for many natural language processing tasks, they are rare or even not available for many language pairs. Instead, comparable corpora are widely available and contain parallel fragments of information that can be used in applications like statistical machine translation systems. In this research, we propose a generative latent Dirichlet allocation based model for extracting parallel fragments from comparable documents without using any initial parallel data or bilingual lexicon. The experimental results show significant improvement if the extracted fragments generated by the proposed method are used for augmenting an existing parallel corpus in an statistical machine translation system. According to the human judgment, the accuracy of the proposed method for an English-Persian task is about 59.7%. Also, the out of vocabulary error rate for the same task is reduced by 28%.","['Somayeh Bakhshaei', 'Reza Safabakhsh', 'Shahram Khadivi']",January 2019,Computer Speech & Language,"['Fragment extraction', 'Comparable corpora', 'Generative model', 'Statistical machine translation', 'Persian', 'English', 'German']",Extracting parallel fragments from comparable documents using a generative model☆
78,"A number of audio signal processing applications characterize different properties of the source underlying an audio signal by analyzing the distribution of a sequence of feature vectors obtained from the signal. The Total Variability Model has been widely used for this purpose as a mechanism for capturing the variability in the feature vector distribution across different signals within a low dimensional representation. In order to arrive at a compact representation, a number of assumptions are made within the model regarding the properties of this distribution. In this paper, we first present an analysis of a parameter estimation method for the model which offers a computationally efficient alternative to the widely used Expectation Maximization (EM) algorithm, but relies on the validity of the model assumptions, using experiments on speaker and language identification tasks. To explain some of the results obtained using this method, we present an extensive statistical analysis aimed at verifying the validity of some of the model assumptions. We show that many of these model assumptions are not valid for the observed data, and propose model generalizations to replace these assumptions. The proposed generalizations lead to a better performance while also opening up possibilities for discriminative training of the model.","['Ruchir Travadi', 'Shrikanth Narayanan']",January 2019,Computer Speech & Language,"['Total variability model', 'i-vector', 'Speaker identification', 'Language identification']",Efficient estimation and model generalization for the totalvariability model☆
79,"Neurogenerative disorders, like dementia, can affect a person’s speech, language and as a consequence, conversational interaction capabilities. A recent study, aimed at improving dementia detection accuracy, investigated the use of conversation analysis (CA) of interviews between patients and neurologists as a means to differentiate between patients with progressive neurodegenerative memory disorder (ND) and those with (non-progressive) functional memory disorders (FMD). However, doing manual CA is expensive and difficult to scale up for routine clinical use. In this paper, we present an automatic classification system using an intelligent virtual agent (IVA). In particular, using two parallel corpora of respectively neurologist- and IVA-led interactions, we show that using acoustic, lexical and CA-inspired features enable ND/FMD classification rates of 90.0% for the neurologist-patient conversations, and 90.9% for the IVA-patient conversations. Analysis of the differentiating potential of individual features show that some differences exist between the IVA and human-led conversations, for example in average turn length of patients.","['Bahman Mirheidari', 'Daniel Blackburn', 'Traci Walker', 'Markus Reuber', 'Heidi Christensen']",January 2019,Computer Speech & Language,"['Dementia detection', 'Conversation analysis', 'Speech recognition and segmentation', 'Processing of pathological speech']",Dementia detection using automatic analysis of conversations☆
80,"Goal-oriented dialog systems require a different approach from chit-chat conversational systems in that they should perform various subtasks as well as continue the conversation itself. Since these systems typically interact with an external knowledge base that changes over time, it is desirable to incorporate domain knowledge to deal with such changes, yet with minimum human effort. This paper presents an extended version of the Hybrid Code Network (HCN) developed for the Facebook AI research (FAIR) dialog dataset used in the Sixth Dialog System Technology Challenge (DSTC6). Compared to the original HCN, the system was more adaptable to changes in the knowledge base due to the modules that are extended to be learned from data. Using the proposed learning scheme with fairly elementary domain-specific rules, the proposed model achieved 100% accuracy in all test datasets.","['Jiyeon Ham', 'Soohyun Lim', 'Kyeng-Hun Lee', 'Kee-Eung Kim']",January 2019,Computer Speech & Language,"['Dialog systems technology', 'Goal-oriented dialog system', 'Extended hybrid code network', 'DSTC6']",Extensions to hybrid code networks for FAIR dialog dataset☆☆☆
81,"Situation Awareness (SA) involves the correct interpretation of a situation, allowing a system to respond to the observed environment and providing support for decision making in many systems domains. Speech therapy is an example of domain where situation awareness can provide benefits, since practitioners should monitor the patient in order to perform therapeutic actions. However, there are a few studies in the area that address reasoning about a situation to improve these tasks. For this reason, this systematic mapping study aims to identify and compare different proposals in the speech therapy domain in order to verify which aspects related to obtaining and maintaining SA are supported. Our analyzes provide useful insights on whats aspects of SA are best integrated in the speech domain - such as knowledge bases and adaptation - and other aspects that remain to be improved - like the action support and projection abilities. Also, this paper includes statistics, methodologies used by different authors and other issues involving research processes. As main contributions, this work presents an overview of the SA integration in the speech therapy domain, discussing challenges in the area and providing directions for further research.","['Maria Helena Franciscatto', 'Iara Augustin', 'João Carlos Damasceno Lima', 'Vinícius Maran']",January 2019,Computer Speech & Language,"['Situation awareness', 'Speech therapy', 'Speech sound disorders', 'Speech recognition']",Situation awareness in the speech therapy domain: A systematic mapping study☆
82,"We analyze the information content of narrative speech samples from individuals with mild cognitive impairment (MCI), in both English and Swedish, using a combination of supervised and unsupervised learning techniques. We extract information units using topic models trained on word embeddings in monolingual and multilingual spaces, and find that the multilingual approach leads to significantly better classification accuracies than training on the target language alone. In many cases, we find that augmenting the topic model training corpus with additional clinical data from a different language is more effective than training on additional monolingual data from healthy controls. Ultimately we are able to distinguish MCI speakers from healthy older adults with accuracies of up to 63% (English) and 72% (Swedish) on the basis of information content alone. We also compare our method against previous results measuring information content in Alzheimer’s disease, and report an improvement over other topic-modeling approaches. Furthermore, our results support the hypothesis that subtle differences in language can be detected in narrative speech, even at the very early stages of cognitive decline, when scores on screening tools such as the Mini-Mental State Exam are still in the “normal” range.","['Kathleen C. Fraser', 'Kristina Lundholm Fors', 'Dimitrios Kokkinakis']",January 2019,Computer Speech & Language,"['Machine learning', 'Topic modeling', 'Mild cognitive impairment', 'Dementia', 'Narrative analysis', 'Multilingual analysis']",Multilingual word embeddings for the assessment of narrative speech in mild cognitive impairment☆
83,"The effects of psychomotor retardation associated with clinical depression are linked to a reduction in variability in acoustic parameters. However, linguistic stress differences between non-depressed and clinically depressed individuals have yet to be investigated. In this paper, by examining vowel articulatory parameters, statistically significant differences in articulatory characteristics are found at a paraphonetic level. For articulatory characteristic features, tongue height and advancement in terms of ‘mid’ and ‘front’ vowel sets show similar depression classification performance trends for both the DAIC-WOZ (English) and AViD (German) databases. Considering linguistic stress feature components, for both databases, depressed speakers exhibit shorter vowel durations and less variance for ‘low’, ‘back’, and ‘rounded’ vowel positions. Results for the DAIC-WOZ and AViD datasets using a small set of linguistic stress based features derived from multiple vowel articulatory parameter sets show absolute, statistically significant, gains of 7% and 20% in two-class depression classification performance over baseline approaches. Linguistic stress feature results indicate that specific vowel set analysis provides better discrimination of clinically depressed and non-depressed speakers. Knowledge gleaned from this research allows the design of more effective automatic depression disorder classification systems.","['Brian Stasak', 'Julien Epps', 'Roland Goecke']",January 2019,Computer Speech & Language,"['Hypoarticulation', 'Paralinguistics', 'Psychomotor retardation', 'Vowel quadrilateral']",An investigation of linguistic stress and articulatory vowel characteristics for automatic depression classification☆
84,"In this article, we review the INTERSPEECH 2013 Computational Paralinguistics ChallengE (ComParE) – the first of its kind– in light of the recent developments in affective and behavioural computing. The impact of the first ComParE instalment is manifold: first, it featured various new recognition tasks including social signals such as laughter and fillers, conflict in dyadic group discussions, and atypical communication due to pervasive developmental disorders, as well as enacted emotion; second, it marked the onset of the ComParE, subsuming all tasks investigated hitherto within the realm of computational paralinguistics; finally, besides providing a unified test-bed under well-defined and strictly comparable conditions, we present the definite feature vector used for computation of the baselines, thus laying the foundation for a successful series of follow-up Challenges. Starting with a review of the preceding INTERSPEECH Challenges, we present the four Sub-Challenges of ComParE 2013. In particular, we provide details of the Challenge databases and a meta-analysis by conducting experiments of logistic regression on single features and evaluating the performances achieved by the participants.","['Björn Schuller', 'Felix Weninger', 'Yue Zhang', 'Fabien Ringeval', 'Anton Batliner', 'Stefan Steidl', 'Florian Eyben', 'Erik Marchi', 'Alessandro Vinciarelli', 'Klaus Scherer', 'Mohamed Chetouani', 'Marcello Mortillaro']",January 2019,Computer Speech & Language,"['Computational Paralinguistics', 'Social Signals', 'Conflict', 'Emotion', 'Autism', 'Survey', 'Challenge']",Affective and behavioural computing: Lessons learnt from the First Computational Paralinguistics Challenge
85,"Alzheimer’s disease (AD) is a neurodegenerative disorder that develops for years before clinical manifestation, while mild cognitive impairment is clinically considered as a prodromal stage of AD. For both types of neurodegenerative disorders, early diagnosis is crucial for the timely treatment and to decelerate progression. Unfortunately, the current diagnostic solutions are time-consuming. Here, we seek to exploit the observation that these illnesses frequently disturb the mental and linguistic functions, which might be detected from the spontaneous speech produced by the patient. First, we present an automatic speech recognition based procedure for the extraction of a special set of acoustic features. Second, we present a linguistic feature set that is extracted from the transcripts of the same speech signals. The usefulness of the two feature sets is evaluated via machine learning experiments, where our goal is not only to differentiate between the patients and the healthy control group, but also to tell apart Alzheimer’s patients from those with mild cognitive impairment. Our results show that based on only the acoustic features, we are able to separate the various groups with accuracy scores between 74–82%. We attained similar accuracy scores when using only the linguistic features. With the combination of the two types of features, the accuracy scores rise to between 80–86%, and the corresponding F1 values also fall between 78–86%. We hope that with the full automation of the processing chain, our method can serve as the basis of an automatic screening test in the future.","['Gábor Gosztolya', 'Veronika Vincze', 'László Tóth', 'Magdolna Pákáski', 'János Kálmán', 'Ildikó Hoffmann']",January 2019,Computer Speech & Language,"['Mild cognitive impairment', 'Alzheimer’s disease', 'Automatic screening', 'Automatic speech recognition', 'Natural language processing', 'Classifier combination']",Identifying Mild Cognitive Impairment and mild Alzheimer’s disease based on spontaneous speech using ASR and linguistic features☆
86,"Prosodic cues such as the word prominence play a fundamental role in human communication, e.g., to express important information. Since different speakers use a wide variety of features to express prominence, there is a large difference in performance between speaker dependently and speaker independently trained models. To cope with these variations without training a new speaker dependent model, in speech recognition speaker adaptation techniques such as feature-space Maximum Likelihood Linear Regression (fMLLR) turned out to be very useful. These methods are developed for GMM-HMM based classifiers under the assumption that the data can be well modeled via the mixture of a few Gaussian distributions. However, in many cases these assumptions are too restrictive. In particular a discriminative classifier such as an SVM often yields far superior results to a GMM. Therefore, we propose a new adaptation method, which adapts the data to the radial basis function kernel of the SVM. To avoid overfitting we apply two regularization terms. The first is based on fMLLR and the second is an L1 regularization to enforce a sparse transformation matrix. We analyze the method in the context of speaker adaptation for word prominence detection, with varying amounts of adaptation data and different weights of the regularization terms. We show that our novel method clearly outperforms fMLLR-GMM and fMLLR-SVM based adaptation.","['Andrea Schnall', 'Martin Heckmann']",January 2019,Computer Speech & Language,"['Prosody', 'Speaker adaptation', 'FMLLR', 'SVM', 'Prominence,']",Feature-space SVM adaptation for speaker adapted word prominence detection☆
87,"We develop a model to satisfy the requirements of Dialog System Technology Challenge 6 (DSTC6) Track 1: building an end-to-end dialog systems for goal-oriented applications. This task involves learning a dialog policy from transactional dialogs in a given domain. Automatic system responses are generated using given task-oriented dialog data (http://workshop.colips.org/dstc6/index.html). As this task has a similar structure to a question answering task (Weston et al., 2015), we employ the MemN2N architecture (Sukhbaatar et al., 2015), which outperforms models based on recurrent neural networks or long short-term memory (LSTM). However, two problems arise when applying this model to the DSTC6 task. First, we encounter an out-of-vocabulary problem, which we resolve by categorizing the metadata types of words that exist in the knowledge base; the metadata is similar to the named entity. Second, the original memory network model has a weak ability to reflect sufficient temporal information, because it only uses sentence-level embeddings. Therefore, we add bidirectional LSTM (Bi-LSTM) at the beginning of the model to better reflect temporal information. The experimental results demonstrate that our model reflects temporal features well. Furthermore, our model achieves state-of-the-art performance among the memory networks, and is comparable to hybrid code networks (Ham et al., 2017) and hierarchical LSTM model (Bai et al., 2017) which is not an end-to-end architecture.","['Byoungjae Kim', 'KyungTae Chung', 'Jeongpil Lee', 'Jungyun Seo', 'Myoung-Wan Koo']",January 2019,Computer Speech & Language,"['Bi-LSTM memory network', 'End-to-end goal-oriented dialog', 'Human–computer interaction']",A Bi-LSTM memory network for end-to-end goal-oriented dialog learning☆
88,"This paper presents a two-stage procedure for automatic prosodic boundary detection in Russian based on textual and acoustic data. The key idea of the method is (1) to predict all potential prosodic boundaries based on syntax and (2) among these potential boundaries, to choose those which are marked acoustically. For the first stage we developed a system which predicted a potential boundary whenever two adjacent words were not connected with each other in terms of syntax; for this we used a dependency tree parser and added several simple rules. At the second stage we run a random forest classifier to detect the actual prosodic boundaries using a small set of acoustic features. Of all the observed prosodic features pause duration worked best, and for some speakers it could be used as the only acoustic cue with no change in efficiency. For other speakers, however, other features were useful, such as tempo and amplitude resets or F0 range, and the choice of the features was speaker-dependent. In the end the procedure worked with the F1 measure of 0.91, recall of 0.90 and precision of 0.93, which is the best published result for Russian.","['Daniil Kocharov', 'Tatiana Kachkovskaia', 'Pavel Skrelin']",January 2019,Computer Speech & Language,"['Prosodic phrasing', 'Automatic boundary detection', 'Dependency parsing', 'Acoustic feature', 'Russian']",Prosodic boundary detection using syntactic and acoustic information☆
89,"In monolingual human language processing, the predictability of a word given its surrounding sentential context is crucial. With regard to receptive multilingualism, it is unclear to what extent predictability in context interplays with other linguistic factors in understanding a related but unknown language – a process called intercomprehension. We distinguish two dimensions influencing processing effort during intercomprehension: surprisal in sentential context and linguistic distance. Based on this hypothesis, we formulate expectations regarding the difficulty of designed experimental stimuli and compare them to the results from think-aloud protocols of experiments in which Czech native speakers decode Polish sentences by agreeing on an appropriate translation. On the one hand, orthographic and lexical distances are reliable predictors of linguistic similarity. On the other hand, we obtain the predictability of words in a sentence with the help of trigram language models. We find that linguistic distance (encoding similarity) and in-context surprisal (predictability in context) appear to be complementary, with neither factor outweighing the other, and that our distinguishing of these two measurable dimensions is helpful in understanding certain unexpected effects in human behaviour.","['Klára Jágrová', 'Tania Avgustinova', 'Irina Stenger', 'Andrea Fischer']",January 2019,Computer Speech & Language,"['Statistical language modelling', 'Surprisal', 'Receptive multilingualism', 'Slavic languages', 'Sentential context', 'Think-aloud protocols', 'Polish', 'Czech', 'Reading']","Language models, surprisal and fantasy in Slavic intercomprehension☆"
90,"Word feature vectors have been proven to improve many natural language processing tasks. With recent advances in unsupervised learning of these feature vectors, it became possible to train it with much more data, which also resulted in better quality of learned features. Since it learns joint probability of latent features of words, it has the advantage that we can train it without any prior knowledge about the goal task we want to solve. We aim to evaluate the universal applicability property of feature vectors, which has been already proven to hold for many standard NLP tasks like part-of-speech tagging or syntactic parsing. In our case, we want to understand the topical focus of text documents and design an efficient representation suitable for discriminating different topics. The discriminativeness can be evaluated adequately on text categorisation task. We propose a novel method to extract discriminative keywords from documents. We utilise word feature vectors to understand the relations between words better and also understand the latent topics which are discussed in the text and not mentioned directly but inferred logically. We also present a simple way to calculate document feature vectors out of extracted discriminative words. We evaluate our method on the four most popular datasets for text categorisation. We show how different discriminative metrics influence the overall results. We demonstrate the effectiveness of our approach by achieving state-of-the-art results on text categorisation task using just a small number of extracted keywords. We prove that word feature vectors can substantially improve the topical inference of documents’ meaning. We conclude that distributed representation of words can be used to build higher levels of abstraction as we demonstrate and build feature vectors of documents. Our method can help in any multi-domain environment to automatically extract discriminative keywords. It can be used to organise and search documents more efficiently.","['Marius Sajgalik', 'Michal Barla', 'Maria Bielikova']",January 2019,Computer Speech & Language,"['Text categorisation', 'Distributed representation', 'Feature vectors', 'Word vectors', 'NLP']",Searching for discriminative words in multidimensional continuous feature space☆
91,"Reviews are valuable sources of information for many important decision making tasks. Summarizing the massive amount of reviews, which are available these days on many entities and services, is critical to help users better digest the sentiment about an entity or a service and its aspects (i.e. features of the entity or the service). This article presents a novel aspect-based summarization framework that generates an abstract from multiple reviews of an entity without the need for a handcrafted feature taxonomy or any training data. We generate summaries using Natural Language Generation (NLG) by taking into account the importance of aspects, as well as the association between them. We model these information in the form of a tree, called Aspect Hierarchy Tree (AHT), in which nodes indicate the important aspects and edges indicate the relationship between them. We propose and investigate three alternative content selection and structuring models for the automatic construction of an AHT in our summarization framework: 1) Rhetorical model, which captures the aspects' importance and relationship by looking at the way people discuss and relate the aspects when expressing opinion in their reviews. 2) Conceptual model, which exploits a common-sense knowledge base (e.g. ConceptNet) to find the conceptual association between aspects. 3) Hybrid model, which exploits both the rhetorical and conceptual information. Our abstractive summarization framework has the potential to implement one of the proposed models dependingon the application or apply all three models and let a user choose the output, depending on his/her desire to use the conceptual, rhetorical or both sources of information. Quantitative and qualitative analysis on the resulting AHTs of the three content selection and structuring models for seven entities in three domains shows that the three models generate AHTs that differ in interesting ways in terms of both content (i.e. selected aspects to be included in the summary) and structure (i.e. the relation between aspects).","['Shima Gerani', 'Giuseppe Carenini', 'Raymond T. Ng']",January 2019,Computer Speech & Language,"['Sentiment', 'Summarization', 'Rhetorical Structure', 'ConceptNet']",Modeling content and structure for abstractive review summarization
92,,[],November 2018,Computer Speech & Language,[],Editorial Board
93,"Recent advances in real-time magnetic resonance imaging (RT-MRI) have made it possible to study the anatomy and dynamic motion of the vocal tract during speech production with great detail. The abundance of rich data on speech articulation provided by medical imaging techniques affords new opportunities for speech science, linguistics, clinical and technological research and application development, but also presents new challenges in audio–video data analysis and data modeling. We review techniques used in analysis of articulatory data acquired using RT-MRI, and assess the utility of different approaches for different types of data and research goals.","['Vikram Ramanarayanan', 'Sam Tilsen', 'Michael Proctor', 'Johannes Töger', 'Louis Goldstein', 'Krishna S. Nayak', 'Shrikanth Narayanan']",November 2018,Computer Speech & Language,"['Real-time magnetic resonance imaging', 'Speech production', 'Region of interest', 'Vocal tract', 'Medical image processing', 'Speech science']",Analysis of speech production real-time MRI☆
94,"In this paper we explore the automatic prediction of speaker appeal from recordings of political speech. The database used contains recordings of a single speaker in a wide range of situations (interview, election rally etc.) which has been annotated for six speaker traits: boring; charismatic; enthusiastic; inspiring; likeable; and persuasive. The aim of this study is to predict these ratings using acoustic features of the speech. We offer three key contributions in this paper. Firstly, we explore the effect of acoustic environment on the perception of speaker ability. We find significant biases in the perception of all six traits, with interview speech being consistently rated as less appealing, and election rally speech as more appealing. In our second contribution, we attempt to exploit this bias by modelling speech from each situation separately, which gives a significant improvement in classification performance. Finally, the database covers 7 years. Thus, our third contribution is an analysis of the variance in both annotations and acoustic features over time to uncover temporal trends in speaker appeal. We find significant trends which show a decline in the speaker’s prosodic activity over time, which mirror a decline in the perception of speaker appeal as measured by the database annotations.","['Ailbhe Cullen', 'Andrew Hines', 'Naomi Harte']",November 2018,Computer Speech & Language,"['Computational paralinguistics', 'Speaker trait detection', 'Speaker appeal', 'Political speech', 'Speech processing']",Perception and prediction of speaker appeal – A single speaker study☆
95,"This paper presents the design of a mixture of Gaussian Mixture Models (GMMs) for Query-by-Example Spoken Term Detection (QbE-STD). The speech data governs acoustically similar broad phonetic structures. To capture broad phonetic structure, we exploit additional information of broad phoneme classes (such as vowels, semi-vowels, nasals, fricatives, and plosives) for the training of the GMM. The mixture of GMMs is tied with GMMs of these broad phoneme classes, i.e., each GMM expresses the probability density function (pdf) of a broad phoneme category. The Expectation Maximization (EM) algorithm is used to obtain the GMM for each broad phoneme class. Thus, a mixture of GMMs represents the spoken query with the broad phonetic constraints. These constraints restrict the posterior probability within the broad class, which results into a better posteriorgram design. The novelty of our work lies in prior probability assignments (as weights of the mixture of GMMs) for better Gaussian posteriorgram design. The proposed simple yet effective posteriorgram outperform Gaussian posteriorgram because of its implicit constraints supplied by broad phonetic posteriors. The Maximum Term Weighted Value (MTWV) for SWS 2013 dataset is improved by 0.052, and 0.051 w.r.t. Gaussian posteriorgram for Mel Frequency Cepstral Coefficients (MFCC) and Perceptual Linear Prediction (PLP), respectively. We found that the proposed mixture of GMMs approach gave consistently better performance than the Gaussian posteriorgram across various evaluation factors, such as different cepstral representations, number of Gaussian components, the number of spoken examples per query, and effect of amount of labeled data used for broad phoneme posterior computation.","['Maulik C. Madhavi', 'Hemant A. Patil']",November 2018,Computer Speech & Language,"['Query-by-Example Spoken Term Detection', 'Phone posteriorgram', 'Mixture of GMMs', 'Gaussian posteriorgram']",Design of mixture of GMMs for Query-by-Example Spoken Term Detection☆
96,"This paper evaluates an automatic spelling error tagger and classifier for German texts. After explaining the existing error tags in detail, the accuracy of the tool is validated against a publicly available database containing around 1700 written texts ranging from first grade to eighth grade. The tool is then applied to a longitudinal study consisting of weekly children’s texts from second and third grades. It can be shown which error categories contribute most significantly to children’s error profiles. Additionally, it can be shown whether or not children make progress on improving in the categories under study.","['Kay Berkling', 'Rémi Lavalley']",November 2018,Computer Speech & Language,"['Orthography', 'Children’s text', 'Corpus', 'Longitudinal study']",Automatic orthographic error tagging and classification for German texts☆
97,"We detail the results of experiments towards a fine-grained stylometric analysis, the identification of distinguishing features between contemporaneous literary translations, both parallel works and also translations of non-parallel sets of works by the same author. We examine translations of plays by the Norwegian dramatist Henrik Ibsen with the initial point of focus being the Ibsen drama Ghosts, for which there exists comparable contemporaneous translations by R. Farqhuarson Sharp and William Archer. Consequently, a number of prose translations of Russian author Anton Chekhov by Marian Fell and Constance Garnett are examined in order to validate hypotheses formed from the results of the Ibsen study and investigate possible particularities in translator’s style which may vary according to genre.By carrying out an analysis of these texts using a variety of machine learning approaches such as Support Vector Machines, Simple Logistic Regression, Naïve Bayes and Decision Tree classifiers, a number of distinguishing textual features are obtained, and the relative frequency of these features in the texts are compared to their frequencies in reference corpora in order to establish which features can be attributed to stylistic choices by the translators themselves and which features may be due to influence from the source language or the topic or genre of a text. We also use the popular Delta metric from authorship attribution studies to investigate the clustering of texts based on most frequent words and a list of discriminatory terms learned in the supervised machine learning experiments.We find that common word unigrams and bigrams are the most salient features for translator fingerprinting across our two authors and four translators examined and are ultimately successful in our goal of classifying which text originated from a particular translator with accuracy measurements of over 90% on average.","['Gerard Lynch', 'Carl Vogel']",November 2018,Computer Speech & Language,"['Stylometry', 'Authorship', 'Translation']",The translator’s visibility: Detecting translatorial fingerprints in contemporaneous parallel translations☆
98,"The duration of speech segments can significantly impact the performance of text-independent speaker verification systems. In real world applications which require high accuracy on short utterances, the performance of i-vector speaker verification framework degrades significantly considering that i-vectors extracted from short utterances are less reliable (i.e., uncertainty is higher) than those extracted from long utterances. Therefore, to handle duration variability properly, a more realistic approach seems to be required. This study is an extension to our recently proposed nearest neighbor probabilistic linear discriminant analysis (NN-PLDA) which estimates the parameters of PLDA in i-vector speaker verification framework using a nonparametric form rather than maximum likelihood estimation (MLE) obtained by an EM algorithm, and has been shown to provide superior performance. In NN-PLDA, the between-speaker covariance matrix that represents global information about the speaker variability is replaced with a local estimation computed on a nearest neighbor basis for each target speaker. Compared to their parametric counterparts, the nonparametric between- and within-speaker scatter matrices can better exploit the discriminant information in training data and are more adapted to sample distributions. In this paper, we provide further analysis on the proposed nonparametrically trained PLDA as well as introduce a duration variability modeling technique in the estimation of the within-speaker scatter matrix as to compensate for the effect of limited speech data. We evaluate our approach using core–10sec and 10sec–10sec telephone trial conditions of NIST 2010 SRE as well as on the truncated test utterances in extended core condition with duration less than 10 s. We also present the results obtained by the successful incorporation of NN-PLDA on the recent NIST 2016 speaker recognition evaluation. In all experiments, considerable performance improvement is obtained with the proposed technique compared to a generatively trained PLDA model.","['Abbas Khosravani', 'Mohammad M. Homayounpour']",November 2018,Computer Speech & Language,"['Speaker recognition', 'PLDA', 'Nonparametric', 'NIST SRE', 'Short duration', 'i-Vector']",Nonparametrically trained PLDA for short duration i-vector speaker verification☆
99,"Former comparisons of human speech recognition (HSR) and automatic speech recognition (ASR) have shown that humans outperform ASR systems in nearly all speech recognition tasks. However, recent progress in ASR has led to substantial improvements of recognition accuracy, and it is therefore unclear how large the task-dependent human-machine gap still remains. This paper investigates this gap between HSR and ASR based on deep neural networks (DNNs) in different acoustic conditions, with the aim of comparing differences and identifying processing strategies that should be considered in ASR. We find that DNN-based ASR reaches human performance for single-channel, small-vocabulary tasks in the presence of speech-shaped noise and in multi-talker babble noise, which is an important difference to previous human-machine comparisons: The speech reception threshold, i.e., the signal-to-noise ratio with 50% word recognition rate is at about −7 to −8 dB both for HSR and ASR. However, in more complex spatial scenes with diffuse noise and moving talkers, the SRT gap amounts to approximately 12 dB. Based on cross comparisons that use oracle knowledge (e.g., the speakers’ true position), incorrect responses are attributed to localization errors or missing pitch information to distinguish between speakers with different gender. In terms of the SRT, localization errors and missing spectral information amount to 2.1 and 3.2 dB, respectively. The comparison hence identifies specific components in ASR that can profit from learning from auditory signal processing.","['Constantin Spille', 'Birger Kollmeier', 'Bernd T. Meyer']",November 2018,Computer Speech & Language,"['Human-machine comparison', 'Speech recognition threshold', 'Deep neural networks', 'Speech intelligibility prediction', 'Spatial scenes']",Comparing human and automatic speech recognition in simple and complex acoustic scenes☆
100,"Automatic Text Summarization is the process of creating a compressed representation of one or more related documents, keeping only the most valuable information. The extractive approach for summarization is the most studied and aims to generate a compressed version of a document by identifying, ranking, and selecting the most relevant sentences or phrases from a text. The selected sentences go verbatim into the summary. However, this strategy may yield incoherent summaries, as pronominal coreferences may appear unbound. To alleviate this problem, this paper proposes a method that solves unbound pronominal anaphoric expressions, automatically enabling the cohesiveness of the extractive summaries. The proposed method can be applied to two distinct scenarios. The first one aims to find and fix unbound anaphoric expressions present in the generated summaries at a post-processing stage; whereas the second one is performed at the preprocessing stage of the proposed pipeline and generates an intermediate version of the input document that resolves the unbound pronominal coreferences. The proposed solution was evaluated on the CNN news corpus using the seventeen summarization techniques most widely acknowledged in the literature and four state-of-the-art summarization systems. Moreover, it also provides a comparative evaluation concerning two distinct assessment scenarios which are compared to a baseline. The experiments performed achieved very encouraging quantitative and qualitative results.","['Jamilson Antunes', 'Rafael Dueire Lins', 'Rinaldo Lima', 'Hilário Oliveira', 'Marcelo Riss', 'Steven J. Simske']",November 2018,Computer Speech & Language,"['Automatic summarization', 'Cohesive summarization', 'Extractive summarization', 'Anaphoric expressions']",Automatic cohesive summarization with pronominal anaphora resolution
101,"Visual lip gestures observed whilst lipreading have a few working definitions, the most common two are: ‘the visual equivalent of a phoneme’ and ‘phonemes which are indistinguishable on the lips’. To date there is no formal definition, in part because to date we have not established a two-way relationship or mapping between visemes and phonemes. Some evidence suggests that visual speech is highly dependent upon the speaker. So here, we use a phoneme-clustering method to form new phoneme-to-viseme maps for both individual and multiple speakers. We test these phoneme to viseme maps to examine how similarly speakers talk visually and we use signed rank tests to measure the distance between individuals. We conclude that broadly speaking, speakers have the same repertoire of mouth gestures, where they differ is in the use of the gestures.","['Helen L. Bear', 'Richard Harvey']",November 2018,Computer Speech & Language,"['Visual speech', 'Lipreading', 'Recognition', 'Audio-visual', 'Speech', 'Classification', 'Viseme', 'Phoneme', 'Speaker identity']",Comparing heterogeneous visual gestures for measuring the diversity of visual speech signals☆
102,"In this paper, we have explored the framework of compressed sensing (CS) and sparse representation (SR) to reduce the footprint of unit selection based speech synthesis (USS) system. In the CS based framework, footprint reduction is achieved by storing either CS measurements or signs of CS measurements, instead of storing the raw speech waveforms. For efficient reconstruction using CS measurements, the speech signal should have a sparse representation over a predefined basis/dictionary. Hence, in this work, we have also studied the effectiveness of sparse representation for compressing the speech waveform. The experimental results are demonstrated using an analytical dictionary (DCT matrix), and several learned dictionaries, derived using K-singular value decomposition (KSVD), method of optimal directions (MOD), greedy adaptive dictionary (GAD) and principal component analysis (PCA) algorithms. To further increase compression in SR based framework of footprint reduction, the significant coefficients of sparse vector are selected adaptively, based on the type of speech segment (e.g., voiced, unvoiced etc.). Experimental studies on two different Indian languages suggest that CS/SR based footprint reduction methods can be used as an alternative to existing compression methods employed in USS system.","['Pulkit Sharma', 'Vinayak Abrol', 'Anil Kumar Nivedita']",November 2018,Computer Speech & Language,"['Sparse representation', 'Speech synthesis', 'Dictionary learning', 'Compressed sensing']",Reducing footprint of unit selection based text-to-speech system using compressed sensing and sparse representation☆
103,"Homophones pose serious issues for automatic speech recognition (ASR) as they have the same pronunciation but different meanings or spellings. Homophone disambiguation is usually done within a stochastic language model or by an analysis of the homophonous word’s context, similarly to word sense disambiguation. Whereas this method reaches good results in read speech, it fails in conversational, spontaneous speech, where utterances are often short, contain disfluencies and/or are realized syntactically incomplete. Phonetic studies, however, have shown that words that are homophonous in read speech often differ in their phonetic detail in spontaneous speech. Whereas humans use phonetic detail to disambiguate homophones, this linguistic information is usually not explicitly incorporated into ASR systems. In this paper, we show that phonetic detail can be used to automatically disambiguate homophones using the example of German pronouns. Using 3179 homophonous tokens from a corpus of spontaneous German and a set of acoustic features, we trained a random forest model. Our results show that homophones can be disambiguated reasonably well using acoustic features (74% F1, 92% accuracy). In particular, this model is able to outperform a model based on lexical context (48% F1, 89% accuracy). This paper is of relevance for speech technologists and linguists: amodule using phonetic detail similar to the presented model is suitable to be integrated in ASR systems in order to improve recognition. An approach similar to the work here that combines the automatic extraction of acoustic features with statistical analysis is suitable to be integrated in phonetic analysis aiming at finding out more about the contribution and interplay of acoustic features for functional categories.","['Barbara Schuppler', 'Tobias Schrank']",November 2018,Computer Speech & Language,"['Homophone disambiguation', 'Automatic speech recognition', 'Phonetic detail', 'Spontaneous speech', 'Random forests']",On the use of acoustic features for automatic disambiguation of homophones in spontaneous German☆
104,"Most of the state-of-the-art speaker recognition system use natural speech signal (i.e., real speech, spontaneous speech or contextual speech) from the subjects. In this paper, recognition of a person is attempted from his or her hum with the help of machines. This kind of application can be useful to design person-dependent Query-by-Humming (QBH) system and hence, plays an important role in music information retrieval (MIR) system. In addition, it can be also useful for other interesting speech technological applications such as human-computer interaction, speech prosody analysis of disordered speech, and speaker forensics. This paper develops new feature extraction technique to exploit perceptually meaningful (due to mel frequency warping to imitate human perception process for hearing) phase spectrum information along with magnitude spectrum information from the hum signal. In particular, the structure of state-of-the-art feature set, namely, Mel Frequency Cepstral Coefficients (MFCCs) is modified to capture the phase spectrum information. In addition, a new energy measure, namely, Variable length Teager Energy Operator (VTEO) is employed to compute subband energies of different time-domain subband signals (i.e., an output of 24 triangular-shaped filters used in the mel filterbank). We refer this proposed feature set as MFCC-VTMP (i.e., mel frequency cepstral coefficients to capture perceptually meaningful magnitude and phase information via VTEO)The polynomial classifier (which is in-principle similar to other discriminatively-trained classifiers such as support vector machine (SVM) with polynomial kernel) is used as the basis for all the experiments. The effectiveness of proposed feature set is evaluated and consistently found to be better than MFCCs feature set for several evaluation factors, such as, comparison with other phase-based features, the order of polynomial classifier, person (speaker) modeling approach (such as, GMM-UBM and i-vector), the dimension of feature vector, robustness under signal degradation conditions, static vs. dynamic features, feature discrimination measures and intersession variability.","['Hemant A. Patil', 'Maulik C. Madhavi']",November 2018,Computer Speech & Language,"['Music information retrieval (MIR)', 'Person recognition', 'Humming', 'Mel filterbank', 'Variable length Teager Energy Operator (VTEO)', 'Polynomial classifier']",Combining evidences from magnitude and phase information using VTEO for person recognition using humming☆
105,,[],September 2018,Computer Speech & Language,[],Editorial Board
106,"In emotion recognition from speech, huge amounts of training material are needed for the development of classification engines. As most current corpora do not supply enough material, a combination of different datasets is advisable. Unfortunately, data recording is done differently and various emotion elicitation and emotion annotation methods are used. Therefore, a combination of corpora is usually not possible without further effort. The manuscript’s aim is to answer the question which corpora are similar enough to jointly be used as training material. A corpus similarity measure based on PCA-ranked features is presented and similar datasets are identified. To evaluate our method we used nine well-known benchmark corpora and automatically identified a sub-set of six most similar datasets. To test that the identified most similar six datasets influence the classification performance, we conducted several cross-corpora emotion recognition experiments comparing our identified six most similar datasets with other combinations. Our most similar sub-set outperforms all other combinations of corpora, the combination of all nine datasets as well as feature normalization techniques. Also influencing side-effects on the recognition rate were excluded. Finally, the predictive power of our measure is shown: increasing similarity score, expressing decreasing similarity, result in decreasing recognition rates. Thus, our similarity measure answers the question which corpora should be included into joint training.","['Ingo Siegert', 'Ronald Böck', 'Andreas Wendemuth']",September 2018,Computer Speech & Language,"['PCA', 'Dataset similarity', 'Cross-corpus emotion recognition', 'Automatic similarity scoring']",Using a PCA-based dataset similarity measure to improve cross-corpus emotion recognition☆
107,"Viewing dialogue management as a reinforcement learning task enables a system to learn to act optimally by maximising a reward function. This reward function is designed to induce the system behaviour required for the target application and for goal-oriented applications, this usually means fulfilling the user’s goal as efficiently as possible. However, in real-world spoken dialogue system applications, the reward is hard to measure because the user’s goal is frequently known only to the user. Of course, the system can ask the user if the goal has been satisfied but this can be intrusive. Furthermore, in practice, the accuracy of the user’s response has been found to be highly variable. This paper presents two approaches to tackling this problem. Firstly, a recurrent neural network is utilised as a task success predictor which is pre-trained from off-line data to estimate task success during subsequent on-line dialogue policy learning. Secondly, an on-line learning framework is described whereby a dialogue policy is jointly trained alongside a reward function modelled as a Gaussian process with active learning. This Gaussian process operates on a fixed dimension embedding which encodes each varying length dialogue. This dialogue embedding is generated in both a supervised and unsupervised fashion using different variants of a recurrent neural network. The experimental results demonstrate the effectiveness of both off-line and on-line methods. These methods enable practical on-line training of dialogue policies in real-world applications.","['Pei-Hao Su', 'Milica Gašić', 'Steve Young']",September 2018,Computer Speech & Language,"['Dialogue systems', 'Reinforcement learning', 'Deep learning', 'Reward estimation', 'Gaussian process', 'Active learning']",Reward estimation for dialogue policy optimisation☆
108,"The paper presents a prosody model of native English (L1) continuous speech as corrective prosodic feedback for non-native learners. The model incorporates both hierarchical discourse association and information structure to (1) pinpoint the prosodic features of multi-phrase continuous speech, and (2) simulate native-like expressive speech using corpus of North American and Taiwan L2 English. The bottom-up, additive, data-driven model aims to generate L1-like expressive continuous speech with built-in phonetic and phonological specifications at the lexical level, syntactic/semantic specifications at the next higher phrase and sentence levels, and completed with patterned paragraph associations and prosodic projections of information allocation at higher levels. The hierarchical model successfully allows us to identify L1-L2 differences by prosodic modules/patterns as novel additional features “discourse structure” and “information density” reliably nail down L1-L2 prosodic differences related to phrase association as well as information placement. Our L1 prosodic model with the proposed predictors and optimized model trained from L1 speech corpus showed increase of prediction over existing methods. As a corrective feedback for L2 learners, these predicted L1 prosodic features were compared with a baseline model by objective evaluation (RMS error and correlation) then superimposed onto the L2 speech tokens. Resynthesized L2 tokens were subsequently compared with the original L2 tokens for degrees of perceived accent using subjective evaluation (native-listener perception test). We believe the proposed model can be an effective alternative for implementing computer-assisted language learning (CALL) systems that helps generate L1-like prosody from text, and at the same time serves as corrective feedback for L2 learners.","['Chao-yu Su', 'Chiu-yu Tseng', 'Jyh-Shing Roger Jang', 'Tanya Visceglia']",September 2018,Computer Speech & Language,"['Prosody', 'Discourse structure', 'Information structure', 'L2 english', 'Resynthesis', 'CALL', 'linguistic', 'Continuous speech']",A hierarchical linguistic information-based model of English prosody: L2 data analysis and implications for computer-assisted language learning☆
109,"We present a multilinear statistical model of the human tongue that captures anatomical and tongue pose related shape variations separately. The model is derived from 3D magnetic resonance imaging data of 11 speakers sustaining speech related vocal tract configurations. To extract model parameters, we use a minimally supervised method based on an image segmentation approach and a template fitting technique. Furthermore, we use image denoising to deal with possibly corrupt data, palate surface information reconstruction to handle palatal tongue contacts, and a bootstrap strategy to refine the obtained shapes. Our evaluation shows that, by limiting the degrees of freedom for the anatomical and speech related variations, to 5 and 4, respectively, we obtain a model that can reliably register unknown data while avoiding overfitting effects. Furthermore, we show that it can be used to generate plausible tongue animation by tracking sparse motion capture data.","['Alexander Hewer', 'Stefanie Wuhrer', 'Ingmar Steiner', 'Korin Richmond']",September 2018,Computer Speech & Language,"['Tongue', 'Vocal tract', 'MRI', 'Statistical model', 'Shape analysis']",A multilinear tongue model derived from speech related MRI data of the human vocal tract☆
110,"Bounded Component Analysis (BCA) solves the Blind Source Separation (BSS) problem based on geometric assumptions. This paper introduces a new proof of a BCA contrast function, derived from elementary matrices, Gauss–Jordan elimination and convex geometry. The new proof and further analysis provide additional insight into a key assumption of BCA. In addition, an interpretation is presented to clarify one of the limitations of the instantaneous BCA algorithm. Experiments on audio sources support our analysis.","['Wei Gao', 'Shen Fan', 'Roberto Togneri', 'Victor Sreeram']",September 2018,Computer Speech & Language,"['Bounded component analysis', 'Blind source separation', 'Independent component analysis', 'Elementary matrix', 'Convex geometry', 'Audio signals']",A new proof of a contrast function for bounded component analysis and further analysis☆
111,"One of the aims of assistive technologies is to help people with disabilities to communicate with others and to provide means of access to information. As an aid to Deaf people, in this work we present a novel prototype Rule Based Machine Translation (RBMT) system for the creation of large and quality written Greek Sign Language (GSL) glossed corpora from Greek text. In particular, the proposed RBMT system assists the professional GSL translator in speeding up the production of different kinds of GSL glossed corpora. Then each glossed corpus is used for the production/creation of Language Model (LM) n-grams. With the GSL glossed corpus from Greek text, we can build, test and evaluate different kinds of Language Models for different kinds of glossed GSL corpora. Here, it should be noted that it does not require grammar knowledge of GSL but only very basic GSL phenomena covered by manual RBMT rules as it assists the professional human translator. Furthermore, it should also be stressed that Language Models for written GSL gloss are missing from the scientific literature, thus this work is pioneer in this field. Evaluation of the proposed scheme is carried out for the weather reports domain, where 20,284 tokens and 1000 sentences have been produced. By using the BiLingual Evaluation Understudy (BLEU) metric score, our prototype RBMT system achieves a relative score of 0.84 (84%) for 4-grams and 0.9 (90%) for 1-grams.","['Dimitrios Kouremenos', 'Klimis Ntalianis', 'Stefanos Kollias']",September 2018,Computer Speech & Language,"['Machine translation', 'Greek Sign Language', 'GSL', 'Deaf people communication']",A novel rule based machine translation scheme from Greek to Greek Sign Language: Production of different types of large corpora and Language Models evaluation☆
112,,[],July 2018,Computer Speech & Language,[],Editorial Board
113,"Part-of-speech (POS) tagging is a fundamental task in natural language processing (NLP). A robust POS tagger plays an important role in most NLP problems and applications, including syntactic parsing, semantic parsing, machine translation, and question answering. Although a lot of efficient POS taggers has been developed for general, conventional text, little work has been done for social media text. In this paper, we present an empirical study on POS tagging for Vietnamese social media text, which shows several challenges compared with tagging for general text. Social media text does not always conform to formal grammars and correct spelling. It also uses abbreviations, foreign words, and emoticons frequently. A POS tagger developed for conventional text would perform poorly on such noisy data. We address this problem by proposing a tagging model based on Conditional Random Fields (CRFs) with various kinds of features for Vietnamese social media text. We also investigate the effect of features extracted from word clusters under the Brown and canonical correlation analysis (CCA) based clustering in semi-supervised settings. We introduce an annotated corpus for POS tagging, which consists of more than four thousand sentences from Facebook, the most popular social network in Vietnam. Using this corpus, we performed a series of experiments to evaluate the proposed model. Our model achieved 88.26% and 88.92% tagging accuracy in supervised and semi-supervised scenarios, respectively, which are nearly 12% improvement over vnTagger, a state-of-the-art and most widely used Vietnamese POS tagger developed for general, conventional text. In addition, the semi-supervised model outperformed, in terms of accuracy, the version of vnTagger trained on the same Facebook dataset, showing the usefulness of word cluster features.1","['Ngo Xuan Bach', 'Nguyen Dieu Linh', 'Tu Minh Phuong']",July 2018,Computer Speech & Language,"['Part-of-speech tagging', 'Social media text', 'Conditional random fields', 'Word clustering']",An empirical study on POS tagging for Vietnamese social media text☆
114,"As children of ages 5–8 often play with each other in small groups, their differences in social development and personality traits usually cause various levels of engagement among others. For example, one child may just observe without engaging at all with others while another child may be interested in both the other children as well as the activity. To develop child-friendly interaction technology such as social robots that can adapt robot behaviours to the social situation of a group of children and facilitate harmonious engagement, we aim to study how we can automatically detect these children’s engagement levels. In this paper, we present a novel automatic method that ranks children in a group according to their engagement level in a temporal way based on non-verbal cues that are robust in naturalistic group settings. Our method combines the omission probability of each rank transformed from discriminative outputs from an SVM ranking method and the transition probability between ranks in time. In comparing our proposed method to other existing methods (such as rule-based ranking, basic SVM, SVM ordinal regression, SVM ranking, and SVMHMM), we found that our novel method yields promising results.","['Jaebok Kim', 'Khiet P. Truong', 'Vanessa Evers']",July 2018,Computer Speech & Language,"['Engagement', 'Play', 'Children conversation', 'Non-verbal', 'Multi-modal']",Automatic temporal ranking of children’s engagement levels using multi-modal cues☆
115,"Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from human–human dyadic spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from a high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.","['Firoj Alam', 'Morena Danieli', 'Giuseppe Riccardi']",July 2018,Computer Speech & Language,"['Empathy', 'Emotion', 'Spoken conversation', 'Behavior analysis', 'Affective scene', 'Affect', 'Call center', 'Human–Human conversation']",Annotating and modeling empathy in spoken conversations☆
116,"Computer-Assisted Pronunciation Training (CAPT) systems aim to help a child learn the correct pronunciations of words. However, while there are many online commercial CAPT apps, there is no consensus among Speech Language Therapists (SLPs) or non-professionals about which CAPT systems, if any, work well. The prevailing assumption is that practicing with such programs is less reliable and thus does not provide the feedback necessary to allow children to improve their performance. The most common method for assessing pronunciation performance is the Goodness of Pronunciation (GOP) technique. Our paper proposes two new GOP techniques. We have found that pronunciation models that use explicit knowledge about error pronunciation patterns can lead to more accurate classification whether a phoneme was correctly pronounced or not. We evaluate the proposed pronunciation assessment methods against a baseline state of the art GOP approach, and show that the proposed techniques lead to classification performance that is more similar to that of a human expert.","['Shiran Dudy', 'Steven Bedrick', 'Meysam Asgari', 'Alexander Kain']",July 2018,Computer Speech & Language,"['Speech recognition', 'Goodness of Pronunciation', 'Educational software', 'Diagnostic tools', 'Speech disorders', 'Support Vector Machine']",Automatic analysis of pronunciations for children with speech sound disorders
117,"Speech conveys many things beyond content, including aspects of appraisal, feeling, and attitude that have not been much studied. In this work, we identify 14 aspects of stance that occur frequently in radio news stories and that could be useful for information retrieval, including indications of subjectivity, immediacy, local relevance, and newness. We observe that newsreaders often mark their stance with prosody. To model this, we treat each news story as a collection of overlapping 6-s patches, each of which may convey one or more aspects of stance by its prosody. The stance of a story is then estimated from the information in its patches. Experiments with English, Mandarin, and Turkish show that this technique enables automatic identification of many aspects of stance in news broadcasts.","['Nigel G. Ward', 'Jason C. Carlson', 'Olac Fuentes']",July 2018,Computer Speech & Language,"['Information retrieval', 'Attitude', 'Broadcast news', 'Prosody', 'American English', 'Mandarin', 'Turkish']",Inferring stance in news broadcasts from prosodic-feature configurations☆☆☆
118,"From both perspectives of speech production and speech perception, vowels as syllable nuclei can be considered as the most significant speech events. Detection of vowel events from a speech signal is usually performed by a two-step procedure. First, a temporal objective contour (TOC), as a time-varying measure of vowel similarity, is generated from the speech signal. Second, vowel landmarks, as the places of vowel events, are extracted by locating prominent peaks of the TOC.In this paper, by employing some spectral models in a sequential manner, we propose a new framework that directly addresses three possible errors in the vowel detection problem, namely vowel deletion, consonant insertion, and vowel insertion. The proposed framework consists of three main steps as follows. At the first step, two solutions are proposed to essentially reduce the initial vowel deletion error. The first solution is to use the peaks detected by a conventional energy-based TOC, but without utilizing TOC smoothing and peak thresholding processes. The peaks detected by a spectral-based TOC generated on the basis of GMM models are also put forward as the second solution for achieving a smaller vowel deletion error. At the second step, a two-class support vector machine (SVM) classifier is adopted to identify the consonant peaks from the vowel ones. Removing the peaks classified as consonants reduces the consonant insertion error. Finally, a two-class SVM classifier is proposed to classify the consecutive peaks detected within the same vowel from the others. The merging of the peaks classified as “same vowel” considerably reduces the vowel insertion error.Experiments are separately conducted on three standard speech corpora, namely FARSDAT, TIMIT and TFARSDAT. The effectiveness of the techniques proposed to reduce three types of detection errors is verified. The criteria of total error (as the summation of three detection errors) and F-measure, respectively result in about 9.7% and 95.1% for FARSDAT, 17.5% and 91.3% for TIMIT, and 19.6% and 90.2% for the TFARSDAT corpus. The evaluation results show that the proposed framework outperforms the existing well-known methods in terms of both total error and F-measure on both read and spontaneous speech corpora.","['Hamidreza Baradaran Kashani', 'Abolghasem Sayadiyan']",July 2018,Computer Speech & Language,"['Vowel landmark detection', 'Temporal objective contour (TOC)', 'Vowel deletion error', 'Consonant insertion error', 'Vowel insertion error']",Sequential use of spectral models to reduce deletion and insertion errors in vowel detection☆
119,"Children who are born with a profound hearing loss have no or only distorted acoustic speech target to imitate and compare their own production with. Computer based visual feedback, visual presentation of speech on screen has shown to be an effective supplement of incomplete or distorted auditory feedback in the case of children with grave hearing-impairment. In this paper, we introduce a novel prosody teaching system where intensity (accent), intonation and rhythm are presented visually for the students (in both separate and combined display mode) as visual feedback and automatic assessment scores are given jointly and separately for the goodness of intonation and rhythm. Evaluation of the automatic assessment was done with cooperation of experts in the field of treatment of hard of hearing children. The results showed that the automatic assessment scores correspond to the subjective evaluations given by the teachers. The evaluation of the whole system was done in a school for hard of hearing children, by comparing the development of a group of students using our prosody teaching system with the development of a control group. The speaking ability of students were compared by a subjective listening experiment after a 3 months teaching course. The students who used the computer based prosody teaching software could produce nicer prosody than the students in the control group.","['Dávid Sztahó', 'Gábor Kiss', 'Klára Vicsi']",July 2018,Computer Speech & Language,"['Speech prosody', 'Intonation', 'Speech recognition', 'Speech aid', 'CAPT']",Computer based speech prosody teaching system☆
120,"A speech signal contains important paralinguistic information, such as the identity, age, gender, language, accent, and the emotional state of the speaker. Automatic recognition of these types of information in adults’ speech has received considerable attention, however there has been little work on children’s speech. This paper focuses on speaker, gender, and age-group recognition from children’s speech. The performances of several classification methods are compared, including Gaussian Mixture Model–Universal Background Model (GMM–UBM), GMM–Support Vector Machine (GMM–SVM) and i-vector based approaches. For speaker recognition, error rate decreases as age increases, as one might expect. However for gender and age-group recognition the effect of age is more complex due mainly to consequences of the onset of puberty. Finally, the utility of different frequency bands for speaker, age-group and gender recognition from children’s speech is assessed.","['Saeid Safavi', 'Martin Russell', 'Peter Jančovič']",July 2018,Computer Speech & Language,"['Speaker recognition', 'Gender identification', 'Age-group identification', 'Children’s speech', 'Gaussian Mixture Model (GMM)', 'Support Vector Machine (SVM)', 'i-vector']","Automatic speaker, age-group and gender identification from children’s speech☆"
121,"In this study we investigated students' conversations with a virtual science tutor (Marni), either individually or in small groups. These constituted two treatment conditions. Students were presented with narrated multimedia science problems and explanations followed by question-answer dialogs with the virtual tutor. Students who received either one-on-one or small group tutoring received the same set of multimedia presentations and questions posed by the virtual tutor. Students in the small group condition discussed their answer before one student from that group responded to the tutor. We asked if students receiving tutoring using the virtual tutor in groups would demonstrate learning gains equivalent to those of students receiving one-on-one tutoring. We also asked if both groups would demonstrate greater learning gains from pretest to posttest than students in business-as-usual (control) classrooms who did not receive supplemental tutoring. One hundred eighty-three (183) students (in 13 classrooms at 4 schools) participated in the study. Of the 183 students, 114 were randomly assigned to tutoring in small groups using Marni; and 69 students received one-on-one tutoring with Marni. When compared with the control group, effect sizes for were d = 0.048 for the group tutoring condition and d = 0.51 for the one-on-one tutoring condition. A two-way ANOVA suggested a main effect for tutoring group, F = 16.8, df (41,171), p < 0.001. In general, students reported benefiting from listening to one another, and from the small group interactions, even though they sometimes disagreed with the answer reported by the small group. We conclude our findings with a vision for a next generation of virtual science tutors that can facilitate discourse and argumentation among students in small groups, leading students to build on each other's ideas to construct accurate science explanations.","['Ronald Cole', 'Cindy Buchenroth-Martin', 'Timothy Weston', 'Liam Devine', 'Jeannine Myatt', 'Brandon Helding', 'Sameer Pradhan', 'Margaret McKeown', 'Samantha Messier', 'Jennifer Borum', 'Wayne Ward']",July 2018,Computer Speech & Language,"['Virtual', 'Tutoring', 'Intelligent', 'Learning', 'Student', 'Multimedia']",One-on-one and small group conversations with an intelligent virtual science tutor☆
122,,[],May 2018,Computer Speech & Language,[],Editorial Board
123,"Near-end speech enhancement works by modifying speech prior to presentation in a noisy environment, typically operating under a constraint of limited or no increase in speech level. One issue is the extent to which near-end enhancement techniques require detailed estimates of the masking environment to function effectively. The current study investigated speech modification strategies based on reallocating energy statically across the spectrum using masker-specific spectral weightings. Weighting patterns were learned offline by maximising a glimpse-based objective intelligibility metric. Keyword scores in sentences in the presence of stationary and fluctuating maskers increased, in some cases by very substantial amounts, following the application of masker- and SNR-specific spectral weighting. A second experiment using generic masker-independent spectral weightings that boosted all frequencies above 1 kHz also led to significant gains in most conditions. These findings indicate that energy-neutral spectral weighting is a highly-effective near-end speech enhancement approach that places minimal demands on detailed masker estimation.","['Yan Tang', 'Martin Cooke']",May 2018,Computer Speech & Language,"['Speech', 'Intelligibility', 'Glimpsing', 'Noise', 'Pattern search', 'Spectral weighting']",Learning static spectral weightings for speech intelligibility enhancement in noise☆
124,"This paper addresses the viability of using Automatic Speech Recognition (ASR) errors as the predictor of difficulties in speech segments, thereby exploiting them to improve Partial and Synchronized Caption (PSC), which we have proposed to train second language (L2) listening skill by encouraging listening over reading. The system uses ASR technology to make word-level text-to-speech synchronization and generates a partial caption. The baseline system determines difficult words based on three features: speech rate, word frequency and specificity. While it encompasses most of the difficult words, it does not cover a wide range of features that hinder L2 listening. Therefore, we propose the use of ASR systems as a model of L2 listeners and hypothesize that ASR errors can predict challenging speech segments for these learners. Among different cases of ASR errors, annotation results suggest the usefulness of four categories of homophones, minimal pairs, negatives, and breached boundaries for L2 listeners. A preliminary experiment with L2 learners focusing on these four categories of the ASR errors revealed that these cases highlight the problematic speech regions for L2 listeners. Based on the findings, the PSC system is enhanced to incorporate these kinds of useful ASR errors. An experiment with L2 learners demonstrated that the enhanced version of PSC is not only preferable, but also more helpful to facilitate the L2 listening process.","['Maryam Sadat Mirzaei', 'Kourosh Meshgi', 'Tatsuya Kawahara']",May 2018,Computer Speech & Language,"['Computer-assisted language learning', 'Second language listening skill', 'Automatic speech recognition', 'Partial and synchronized caption']",Exploiting automatic speech recognition errors to enhance partial and synchronized caption for facilitating second language listening☆
125,"Multichannel linear filters, such as the Multichannel Wiener Filter (MWF) and the Generalized Eigenvalue (GEV) beamformer are popular signal processing techniques which can improve speech recognition performance. In this paper, we present an experimental study on these linear filters in a specific speech recognition task, namely the CHiME-4 challenge, which features real recordings in multiple noisy environments. Specifically, the rank-1 MWF is employed for noise reduction and a new constant residual noise power constraint is derived which enhances the recognition performance. To fulfill the underlying rank-1 assumption, the speech covariance matrix is reconstructed based on eigenvectors or generalized eigenvectors. Then the rank-1 constrained MWF is evaluated with alternative multichannel linear filters under the same framework, which involves a Bidirectional Long Short-Term Memory (BLSTM) network for mask estimation. The proposed filter outperforms alternative ones, leading to a 40% relative Word Error Rate (WER) reduction compared with the baseline Weighted Delay and Sum (WDAS) beamformer on the real test set, and a 15% relative WER reduction compared with the GEV-BAN method. The results also suggest that the speech recognition accuracy correlates more with the Mel-frequency cepstral coefficients (MFCC) feature variance than with the noise reduction or the speech distortion level.","['Ziteng Wang', 'Emmanuel Vincent', 'Romain Serizel', 'Yonghong Yan']",May 2018,Computer Speech & Language,"['Rank-1 Multichannel Wiener Filter', 'Speech recognition', 'Residual noise power', 'Deep neural network']",Rank-1 constrained Multichannel Wiener Filter for speech recognition in noisy environments
126,"Within the field of statistical machine translation, the neural approach (NMT) is currently pushing ahead the state of the art performance traditionally achieved by phrase-based approaches (PBMT), and is rapidly becoming the dominant technology in machine translation. Indeed, in the last IWSLT and WMT evaluation campaigns on machine translation, NMT outperformed well established state-of-the-art PBMT systems on many different language pairs. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based statistical machine translation outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. In this analysis, we focus on two language directions with different characteristics: English–German, known to be particularly hard because of morphology and syntactic differences, and English–French, where PBMT systems typically reach outstanding quality and thus represent a strong competitor for NMT. Our analysis provides useful insights on what linguistic phenomena are best modelled by neural models – such as the reordering of verbs and nouns – while pointing out other aspects that remain to be improved – like the correct translation of proper nouns.","['Luisa Bentivogli', 'Arianna Bisazza', 'Mauro Cettolo', 'Marcello Federico']",May 2018,Computer Speech & Language,"['Machine translation (MT)', 'Neural MT', 'Phrase-based MT', 'Evaluation']",Neural versus phrase-based MT quality: An in-depth analysis on English–German and English–French☆
127,"The research presented in the paper addresses conversational telephone speech recognition and keyword spotting for the Lithuanian language. Lithuanian can be considered a low e-resourced language as little transcribed audio data, and more generally, only limited linguistic resources are available electronically. Part of this research explores the impact of reducing the amount of linguistic knowledge and manual supervision when developing the transcription system. Since designing a pronunciation dictionary requires language-specific expertise, the need for manual supervision was assessed by comparing phonemic and graphemic units for acoustic modeling. Although the Lithuanian language is generally described in the linguistic literature with 56 phonemes, under low-resourced conditions some phonemes may not be sufficiently observed to be modeled. Therefore different phoneme inventories were explored to assess the effects of explicitly modeling diphthongs, affricates and soft consonants. The impact of using Web data for language modeling and additional untranscribed audio data for semi-supervised training was also measured. Out-of-vocabulary (OOV) keywords are a well-known challenge for keyword search. While word-based keyword search is quite effective for in-vocabulary words, OOV keywords are largely undetected. Morpheme-based subword units are compared with character n-gram-based units for their capacity to detect OOV keywords. Experimental results are reported for two training conditions defined in the IARPA Babel program: the full language pack and the very limited language pack, for which, respectively, 40 h and 3 h of transcribed training data are available. For both conditions, grapheme-based and phoneme-based models are shown to obtain comparable transcription and keyword spotting results. The use of Web texts for language modeling is shown to significantly improve both speech recognition and keyword spotting performance. Combining full-word and subword units leads to the best keyword spotting results.","['Rasa Lileikytė', 'Lori Lamel', 'Jean-Luc Gauvain', 'Arseniy Gorin']",May 2018,Computer Speech & Language,"['Conversational telephone speech', 'Lithuanian', 'Speech-to-text', 'Keyword spotting']",Conversational telephone speech recognition for Lithuanian☆
128,"In the field of human speech capturing systems, a fundamental role is played by the source localization algorithms. In this paper a Speaker Localization algorithm (SLOC) based on Deep Neural Networks (DNN) is evaluated and compared with state-of-the art approaches. The speaker position in the room under analysis is directly determined by the DNN, leading the proposed algorithm to be fully data-driven. Two different neural network architectures are investigated: the Multi Layer Perceptron (MLP) and Convolutional Neural Networks (CNN). GCC-PHAT (Generalized Cross Correlation-PHAse Transform) Patterns, computed from the audio signals captured by the microphone are used as input features for the DNN. In particular, a multi-room case study is dealt with, where the acoustic scene of each room is influenced by sounds emitted in the other rooms. The algorithm is tested by means of the home recorded DIRHA dataset, characterized by multiple wall and ceiling microphone signals for each room. In detail, the focus goes to speaker localization task in two distinct neighboring rooms.As term of comparison, two algorithms proposed in literature for the addressed applicative context are evaluated, the Crosspower Spectrum Phase Speaker Localization (CSP-SLOC) and the Steered Response Power using the Phase Transform speaker localization (SRP-SLOC). Besides providing an extensive analysis of the proposed method, the article shows how DNN-based algorithm significantly outperforms the state-of-the-art approaches evaluated on the DIRHA dataset, providing an average localization error, expressed in terms of Root Mean Square Error (RMSE), equal to 324 mm and 367 mm, respectively, for the Simulated and the Real subsets.","['Fabio Vesperini', 'Paolo Vecchiotti', 'Emanuele Principi', 'Stefano Squartini', 'Francesco Piazza']",May 2018,Computer Speech & Language,"['Acoustic source localization', 'Speaker localization', 'GCC-PHAT', 'Deep Neural Networks', 'Convolutional Neural Networks', 'Computational Audio Processing']",Localizing speakers in multiple rooms by using Deep Neural Networks☆
129,"Speaker and language recognition and characterization is an exciting area of research that has gained importance in the field of speech science and technology. This special issue features, among other contributions, some of the most remarkable ideas presented and discussed at Odyssey 2016: the Speaker and Language Recognition Workshop, held in Bilbao, Spain, in June 2016. This introduction provides an overview of the selected papers in the context of current challenges.","['Eduardo Lleida', 'Luis Javier Rodriguez-Fuentes']",May 2018,Computer Speech & Language,[],PrefaceSpeaker and language recognition and characterization: Introduction to the CSL special issue
130,"In this paper, we address issues in situated language understanding in a moving car. More specifically, we propose a reference resolution method to identify user queries about specific target objects in their surroundings. We investigate methods of predicting which target object is likely to be queried given a visual scene and what kind of linguistic cues users naturally provide to describe a given target object in a situated environment. We propose methods to incorporate the visual saliency of the visual scene as a prior. Crowdsourced statistics of how people describe an object are also used as a prior. We have collected situated utterances from drivers using our research system, which was embedded in a real vehicle. We demonstrate that the proposed algorithms improve target identification rate by 15.1% absolute over the baseline method that does not use visual saliency-based prior and depends on public database with a limited number of category information.",['Teruhisa Misu'],March 2018,Computer Speech & Language,"['Situated dialog', 'In-car interaction', 'Visual saliency', 'Crowdsourcing', 'Multimodal interaction']",Situated reference resolution using visual saliency and crowdsourcing-based priors for a spoken dialog system within vehicles☆
131,"In this paper we investigate the audio-visual processing of linguistic prosody, more precisely the detection of word prominence, and examine how the additional visual information can be used to increase the robustness when acoustic background noise is present. We evaluate the detection performance for each modality individually and perform experiments using feature and decision fusion. For the latter we also consider the adaptive fusion with fusion weights adjusted to the current acoustic noise level. Our experiments are based on a corpus with 11 English speakers which contains in addition to the speech signal also videos of the speakers’ heads. From the acoustic signal we extract features which are well known to capture word prominence like loudness, fundamental frequency and durational features. The analysis of the visual signal is based on features derived from the speaker’s rigid head movements and movements of the speaker’s mouth. We capture the rigid head movements by tracking the speaker’s nose. Via a two-dimensional Discrete Cosine Transform (DCT) calculated from the mouth region we represent the movements of the speaker’s mouth. The results show that the rigid head movements as well as movements inside the mouth region can be used to discriminate prominent from non-prominent words. The audio-only detection yields an Equal Error Rate (EER) averaged over all speakers of 13%. Based only on the visual features we obtain 20% of EER. When we combine the visual and the acoustic features we only see a small improvement compared to the audio-only detection for clean speech. To simulate background noise we added 4 different noise types at varying SNR levels to the acoustic stream. The results indicate that word prominence detection is quite robust against additional background noise. Even at a severe Signal to Noise Ratio (SNR) of −10 dB the EER only rises to 35%. Despite this the audio-visual fusion leads to notable improvements for the detection from noisy speech. We observe relative reductions of the EER of up to 79%.",['Martin Heckmann'],March 2018,Computer Speech & Language,[],Audio-visual word prominence detection from clean and noisy speech☆
132,"Recent works on the vulnerability of automatic speaker verification (ASV) systems confirm that malicious spoofing attacks using synthetic speech can provoke significant increase in false acceptance rate. A reliable detection of synthetic speech is key to develop countermeasure for synthetic speech based spoofing attacks. In this paper, we targeted that by focusing on three major types of artifacts related to magnitude, phase and pitch variation, which are introduced during the generation of synthetic speech. We proposed a new approach to detect synthetic speech using score-level fusion of front-end features namely, constant Q cepstral coefficients (CQCCs), all-pole group delay function (APGDF) and fundamental frequency variation (FFV). CQCC and APGDF were individually used earlier for spoofing detection task and yielded the best performance among magnitude and phase spectrum related features, respectively. The novel FFV feature introduced in this paper to extract pitch variation at frame-level, provides complementary information to CQCC and APGDF. Experimental results show that the proposed approach produces the best stand-alone spoofing detection performance using Gaussian mixture model (GMM) based classifier on ASVspoof 2015 evaluation dataset. An overall equal error rate of 0.05% with a relative performance improvement of 76.19% over the next best-reported results is obtained using the proposed method. In addition to outperforming all existing baseline features for both known and unknown attacks, the proposed feature combination yields superior performance for ASV system (GMM with universal background model/i-vector) integrated with countermeasure framework. Further, the proposed method is found to have relatively better generalization ability when either one or both of copy-synthesized data and limited spoofing data are available a priori in the training pool.","['Monisankha Pal', 'Dipjyoti Paul', 'Goutam Saha']",March 2018,Computer Speech & Language,"['All-pole group delay function (APGDF)', 'Anti-spoofing', 'Constant Q cepstral coefficient (CQCC)', 'Fundamental frequency variation (FFV)', 'Score-level fusion', 'Spoofing attack']",Synthetic speech detection using fundamental frequency variation and spectral features☆
133,"An accurate objective prediction of human speech intelligibility is of interest for many applications such as the evaluation of signal processing algorithms. To predict the speech recognition threshold (SRT) of normal-hearing listeners, an automatic speech recognition (ASR) system is employed that uses a deep neural network (DNN) to convert the acoustic input into phoneme predictions, which are subsequently decoded into word transcripts. ASR results are obtained with and compared to data presented in Schubotz et al. (2016), which comprises eight different additive maskers that range from speech-shaped stationary noise to a single-talker interferer and responses from eight normal-hearing subjects. The task for listeners and ASR is to identify noisy words from a German matrix sentence test in monaural conditions. Two ASR training schemes typically used in applications are considered: (A) matched training, which uses the same noise type for training and testing and (B) multi-condition training, which covers all eight maskers. For both training schemes, ASR-based predictions outperform established measures such as the extended speech intelligibility index (ESII), the multi-resolution speech envelope power spectrum model (mr-sEPSM) and others. This result is obtained with a speaker-independent model that compares the word labels of the utterance with the ASR transcript, which does not require separate noise and speech signals. The best predictions are obtained for multi-condition training with amplitude modulation features, which implies that the noise type has been seen during training. Predictions and measurements are analyzed by comparing speech recognition thresholds and individual psychometric functions to the DNN-based results.","['Constantin Spille', 'Stephan D. Ewert', 'Birger Kollmeier', 'Bernd T. Meyer']",March 2018,Computer Speech & Language,"['Speech intelligibility prediction', 'Deep neural networks', 'Automatic speech recognition']",Predicting speech intelligibility with deep neural networks☆
134,"This paper presents a technique that applies the pairwise variability index (PVI), a rhythm metric that quantifies variability in speech rhythm, to the classification of speech varieties. The technique combines the Particle Swarm Optimization (PSO) algorithm with a generalization of several rhythm metrics that are based on the PVI. The performance of this optimization-oriented classification is compared with classification that uses conventional (both PVI-based and interval-based) rhythm metrics. Application is made to the classification of native and non-native Arabic speech using data are from the West Point Arabic Speech Corpus; experiments are based on segmental durations and use Support Vector Machine (SVM) classification. Results show that the optimization-oriented classification provides a better discrimination between native and non-native speech varieties than classification based of the conventional rhythm metrics. When added to different combinations of these conventional metrics, the optimization-oriented procedure consistently improves classification rates.","['Soumaya Gharsellaoui', 'Sid Ahmed Selouani', 'Wladyslaw Cichocki', 'Yousef Alotaibi', 'Adel Omar Dahmane']",March 2018,Computer Speech & Language,"['Speech rhythm', 'Rhythm metrics', 'Pairwise variability index', 'Classification', 'Modern standard Arabic', 'Particle swarm optimization', 'Non-native accent']",Application of the pairwise variability index of speech rhythm with particle swarm optimization to the classification of native and non-native accents☆
135,"This paper identifies and models a phenomenon observed across low-resource languages in keyword search results from speech retrieval systems where the speech recognition has high error rate, due to very limited training data. High confidence correct detections (hccds) of keywords are rare, yet often succeed one another closely in time. We refer to these close sequences of hccds as keyword hotspots. The ability to predict keyword hotspots could support speech retrieval, and provide new insights into the behavior of speech recognition systems. We treat hotspot prediction as a binary classification task on all word-sized time intervals in an audio file of a telephone conversation, using prosodic features as predictors. Rare events that follow this pattern are often modeled as a self-exciting point process (sepp), meaning the occurrence of a rare event excites a following one. To label successive points in time as occurring within a hotspot or not, we fit a sepp function to the distribution of hccds in the keyword search output. Two major learning challenges are that the size of the positive class is very small, and the training and test data have dissimilar distributions. To address these challenges, we develop a novel data selection framework that chooses training data with good generalization properties. Results exhibit superior generalization performance.","['Jie Gao', 'Axinia Radeva', 'Chuyao Shen', 'Shiqi Wang', 'Qianbo Wang', 'Rebecca J. Passonneau']",March 2018,Computer Speech & Language,"['Self-exciting point process', 'Data selection', 'Keyword search', 'Speech retrieval']",Prediction of a hotspot pattern in keyword search results☆
136,"On account of large acoustic mismatch, automatic speech recognition (ASR) systems trained using adults’ speech data yield poor recognition performance when evaluated on children’s speech data. Despite the use of common speaker normalization techniques like feature-space maximum likelihood regression (fMLLR) and vocal tract length normalization (VTLN), a significant gap remains between the recognition rates for matched and mismatched testing. Our earlier works have already highlighted the sensitivity of salient front-end features including the popular Mel-frequency cepstral coefficient (MFCC) to gross pitch variation across adult and child speakers. Motivated by that, in this work, we explore pitch-adaptive front-end signal processing in deriving the MFCC features to reduce the sensitivity to pitch variation. For this purpose, first an existing vocoder approach known as STRAIGHT spectral analysis is employed for obtaining the smoothed spectrum devoid of pitch harmonics. Secondly, a much simpler spectrum smoothing approach exploiting pitch adaptive-liferting is also presented. The proposed approach is noted to be less sensitive to errors in the pitch estimation than the STRAIGHT-based approach. Both these approaches result in significant improvements for children’s mismatch ASR. The effectiveness of the proposed adaptive-liftering-based approach is also demonstrated in the context of acoustic modeling paradigms based on the subspace Gaussian mixture model (SGMM) and the deep neural network (DNN). Further, it has been shown that the effectiveness of existing speaker normalization techniques remain intact even with the use of proposed pitch-adaptive MFCCs, thus leading to additional gains.","['Rohit Sinha', 'S. Shahnawazuddin']",March 2018,Computer Speech & Language,"['Children’s ASR', 'Acoustic mismatch', 'Pitch-adaptive features', 'STRAIGHT MFCC', 'SGMM', 'DNN']",Assessment of pitch-adaptive front-end signal processing for children’s speech recognition☆
137,"English lexical stress is acoustically related to combination of duration, intensity, fundamental frequency (F0) and vowel quality. At phonetic level, the current study investigates L1 Bengali speakers’ acoustic manipulation of English lexical stress to produce English lexical stress contrast. This study compares the use of these correlates in the production of English lexical stress contrasts by 10 L1 English and 20 L1 Bengali speakers. The result showed that, although L1 Bengali speakers did use all four acoustic correlates to distinguish stressed from unstressed syllables, they produced significantly less native-like stress patterns. In particular, there was a significant difference in formant patterns across speaker groups, where L1 Bengali speakers produced English like vowel reduction in some unstressed syllables, but in other cases, L1 Bengali speakers had tendency to either not reduce or incorrectly reduce vowels in unstressed syllables. The results suggest that L1 Bengali speakers’ production of English lexical stress contrast is influenced by L1 language experience and L1 phonology.","['Shambhu Nath Saha', 'Shyamal Kumar Das Mandal']",January 2018,Computer Speech & Language,"['Acoustic correlates', 'Lexical stress contrasts', 'Phonology', 'Reliable cue', 'Vowel quality', 'Vowel reduction']",Phonetic realization of English lexical stress by native (L1) Bengali speakers compared to native (L1) English speakers☆
138,"Over the last few years, i-vectors have been the state-of-the-art technique in speaker recognition. Recent advances in Deep Learning (DL) technology have improved the quality of i-vectors but the DL techniques in use are computationally expensive and need phonetically labeled background data. The aim of this work is to develop an efficient alternative vector representation of speech by keeping the computational cost as low as possible and avoiding phonetic labels, which are not always accessible. The proposed vectors will be based on both Gaussian Mixture Models (GMM) and Restricted Boltzmann Machines (RBM) and will be referred to as GMM–RBM vectors. The role of RBM is to learn the total speaker and session variability among background GMM supervectors. This RBM, which will be referred to as Universal RBM (URBM), will then be used to transform unseen supervectors to the proposed low dimensional vectors. The use of different activation functions for training the URBM and different transformation functions for extracting the proposed vectors are investigated. At the end, a variant of Rectified Linear Units (ReLU) which is referred to as variable ReLU (VReLU) is proposed. Experiments on the core test condition 5 of NIST SRE 2010 show that comparable results with conventional i-vectors are achieved with a clearly lower computational load in the vector extraction process.","['Omid Ghahabi', 'Javier Hernando']",January 2018,Computer Speech & Language,"['Restricted Boltzmann machine', 'Deep learning', 'Variable rectified linear unit', 'Speaker recognition', 'GMM–RBM vector', 'i-vector']",Restricted Boltzmann machines for vector representation of speech in speaker recognition☆
139,"In this paper an uncertainty weighting scheme for DNN–HMM-based speech recognition is proposed to increase discriminability in the decoding process. To this end, the DNN pseudo-log-likelihoods are weighted according to the uncertainty variance assigned to the acoustic observation. The results presented here suggest that substantial reduction in WER is achieved with clean training. Moreover, modelling the uncertainty propagation through the DNN is not required and no approximations for non-linear activation functions are made. The presented method can be applied to any network topology that delivers log-likelihood-like scores. It can be combined with any noise removal technique and adds a minimal computational cost. This technique was exhaustively evaluated and combined with uncertainty-propagation-based schemes for computing the pseudo-log-likelihoods and uncertainty variance at the DNN output. Two proposed methods optimized the parameters of the weighting function by leveraging the grid search either on a development database representing the given task or on each utterance based on discrimination metrics. Experiments with Aurora-4 task showed that, with clean training, the proposed weighting scheme can reduce WER by a maximum of 21% compared with a baseline system with spectral subtraction and uncertainty propagation using the unscented transform. The uncertainty weighting method reduced the gap between clean and multi-noise/multi-condition training. This can be useful when it is not easy to train a DNN–HMM system in conditions that are similar to the testing ones. Finally, the presented results on the use of uncertainty are very competitive with those published elsewhere using the same database as the one employed here.","['José Novoa', 'Josué Fredes', 'Víctor Poblete', 'Néstor Becerra Yoma']",January 2018,Computer Speech & Language,"['Automatic speech recognition', 'Deep neural network', 'Uncertainty weighting', 'Uncertainty propagation', 'DNN–HMM']",Uncertainty weighting and propagation in DNN–HMM-based speech recognition☆
140,"In this paper, we propose to combine the posterior probabilities of voice activity derived from different statistical model-based algorithms for enhanced voice activity detection. For this, the Dempster-Shafer (DS) theory of evidence is employed to represent and combine the different probabilities estimated by three different statistical model-based VAD algorithms including the Sohn’s likelihood ratio test (LRT)-based method, smoothed LRT-based method, and multiple observation LRT-based method. By considering a generalization of the Bayesian framework and permitting the characterization of uncertainty and ignorance through the DS theory, the probability of an ignorant state is eliminated through the orthogonal sum of several speech presence probabilities, which results in the performance improvement when detecting voice activity. According to objective test results, it is discovered the proposed DS theory-based VAD method offers significant improvements over the conventional approaches.","['Tae-Jun Park', 'Joon-Hyuk Chang']",January 2018,Computer Speech & Language,"['Dempster-Shafer theory', 'Voice activity detection', 'Likelihood ratio test']",Dempster-Shafer theory for enhanced statistical model-based voice activity detection☆
141,"Paraphrase identification consists in the process of verifying if two sentences are semantically equivalent or not. It is applied in many natural language tasks, such as text summarization, information retrieval, text categorization, and machine translation. In general, methods for assessing paraphrase identification perform three steps. First, they represent sentences as vectors using bag of words or syntactic information of the words present the sentence. Next, this representation is used to measure different similarities between two sentences. In the third step, these similarities are given as input to a machine learning algorithm that classifies these two sentences as paraphrase or not. However, two important problems in the area of paraphrase identification are not handled: (i) the meaning problem: two sentences sharing the same meaning, composed of different words; and (ii) the word order problem: the order of the words in the sentences may change the meaning of the text. This paper proposes a paraphrase identification system that represents each pair of sentence as a combination of different similarity measures. These measures extract lexical, syntactic and semantic components of the sentences encompassed in a graph. The proposed method was benchmarked using the Microsoft Paraphrase Corpus, which is the publicly available standard dataset for the task. Different machine learning algorithms were applied to classify a sentence pair as paraphrase or not. The results show that the proposed method outperforms state-of-the-art systems.","['Rafael Ferreira', 'George D.C. Cavalcanti', 'Fred Freitas', 'Rafael Dueire Lins', 'Steven J. Simske', 'Marcelo Riss']",January 2018,Computer Speech & Language,"['Sentence similarity', 'Paraphrase identification', 'Sentence simplification', 'Graph-based model']",Combining sentence similarities measures to identify paraphrases☆
142,"Reading while listening to texts (RWL) is a promising way to improve the learning benefits provided by a reading experience. In an exploratory study, we investigated the effect of synchronizing the highlighting of words (visual) with their auditory (speech) counterpart during a RWL task. Forty French children from 3rd to 5th grade read short stories in their native language while hearing the story spoken by a narrator. In the non-synchronized (S−) condition the text was written in black on a white background, whereas in the synchronized (S+) RWL, the text was written in grey and the words were dynamically written in black when they were aurally displayed, in a karaoke-like fashion. The children were then unexpectedly tested on their memory for the orthographic form and semantic category of pseudowords that were included in the stories. The effect of synchronizing was null in the orthographic task and negative in the semantic task. Children's preference was mainly for the S− condition, except for the poorest readers who tended to prefer the S+ condition. In addition, the children's eye movements were recorded during reading. Gaze was affected by synchronization, with fewer but longer fixations on words, and fewer regressive saccades in the S+ condition compared to the S− condition. Thus, the S+ condition presumably captured the children's attention toward the currently heard word, which forced the children to be strictly aligned with the oral modality.","['Emilie Gerbier', 'Gérard Bailly', 'Marie Line Bosse']",January 2018,Computer Speech & Language,"['Audio-assisted reading', 'Supported e-text', 'Assistive reading software', 'Reading while listening', 'Audio–visual synchrony']",Audio–visual synchronization in reading while listening to texts: Effects on visual behavior and verbal learning☆
143,"This article introduces a new methodology to enhance an existing traditional Spoken Dialogue System (SDS) with optimal turn-taking capabilities in order to increase dialogue efficiency. A new approach for transforming the traditional dialogue architecture into an incremental one at a low cost is presented: a new turn-taking decision module called the Scheduler is inserted between the Client and the Service. It is responsible for handling turn-taking decisions. Then, a User Simulator which is able to interact with the system using this new architecture has been implemented and used to train a new Reinforcement Learning turn-taking strategy. Compared to a non-incremental and a handcrafted incremental baselines, it is shown to perform better in simulation and in a real live experiment.","['Hatim Khouzaimi', 'Romain Laroche', 'Fabrice Lefèvre']",January 2018,Computer Speech & Language,"['Spoken Dialogue Systems', 'Turn-taking', 'Incremental dialogue', 'Reinforcement Learning']",A methodology for turn-taking capabilities enhancement in Spoken Dialogue Systems using Reinforcement Learning☆
144,"In recent years, unsupervised, graph-based ranking algorithms have been successfully applied to keyphrase extraction tasks. These methods have the advantage of taking into account global information, such as text structure and relations between words, phrases, and sentences, rather than relying solely on local, vertex-specific information. Graph-based approaches for keyphrase extraction, however, have a particular drawback, which comes from their frequency-based analysis methods. The weakness is that many common, less relevant terms may get a higher ranking, particularly in short articles. The converse situation also occurs, where less common (and possibly more relevant) terms obtain lower rankings. We propose an unsupervised method—RankUp—that enhances graph-based keyphrase extraction approaches by applying an error-feedback mechanism similar to the concept of backpropagation. Experiments have been performed on almost 3,300 short texts from a variety of domains. Our experiments show that error-feedback propagation can boost the quality of keyphrases in graph-based keyphrase extraction techniques.","['Gerardo Figueroa', 'Po-Chi Chen', 'Yi-Shin Chen']",January 2018,Computer Speech & Language,"['Keyphrase extraction', 'Graph-based methods', 'Backpropagation', 'Error feedback', 'Short articles']",RankUp: Enhancing graph-based keyphrase extraction methods with error-feedback propagation☆
145,"We propose a simple speech activity detector (SAD) based on recording-specific Gaussian mixture modeling (GMM) of speech and non-speech frames. We extend the conventional expectation-maximization (EM) algorithm for GMM training using semi-supervised learning. It provides a methodology to incorporate unlabeled data into the SAD training process, leading to more accurate statistical models by exploiting the structure of data distribution. It fits naturally to off-line applications that may require partial human assistance, or applications that involve processing large quantities of audio data, such as text-independent speaker verification, speaker diarization or audio surveillance. The proposed SAD does not require any off-line training data as supervised SADs do. Rather, it employs initial labels produced from a tiny fraction of a given audio recording with the help of another simpler SAD (or a human operator). The proposed SAD is analyzed for the different covariance types, the initialization methods for speech and non-speech class, the amount of labeled data required for initialization, and the speech features. In experiments with a stand-alone SAD system, we observe increased accuracy on the challenging dataset from the recent NIST OpenSAD evaluation. Our extensive automatic speaker verification (ASV) experiments, including text-independent experiments with NIST 2010 speaker recognition evaluation (SRE) data and text-dependent experiments with RSR2015 and RedDots corpora, show benefits of the new approach for the long speech segments containing non-stationary noise. For the shorter data conditions in the text-dependent experiments, simpler unsupervised SADs perform however better. Further, we study the impact of SAD misses and false alarms to ASV performance on the NIST 2010 SRE data. By deriving an empirical cost function with the two SAD errors, we have observed that ASV error rate reaches a minimum value around the same SAD operating point irrespective of SAD method and signal-to-noise ratio (SNR). The optimum ASV performance occurs approximately at an SAD operating region where falsely included non-speech is considered 4–5 times more costly than missed speech. Importantly, the proposed semi-supervised SAD is relatively less dependent on the SAD decision threshold compared to the other contrastive SAD methods.","['Alexey Sholokhov', 'Md Sahidullah', 'Tomi Kinnunen']",January 2018,Computer Speech & Language,"['Speech activity detection', 'Semi-supervised learning', 'Gaussian mixture model', 'Speaker recognition', 'NIST OpenSAD', 'NIST SRE']",Semi-supervised speech activity detection with an application to automatic speaker verification☆
146,"Electromagnetic articulography (EMA) is one of the technological solutions, widely used to measure the articulatory movement useful for speech production research. EMA is typically used to track articulatory flesh points by placing sensors, often heuristically, on the key articulators including lips, jaw, tongue and velum in the mid-sagittal plane. In this work, we address the problem of optimal placement of EMA sensors by posing it as the optimal selection of points for minimizing the reconstruction error of the air-tissue boundaries in the real-time magnetic resonance imaging (rtMRI) video frames of vocal tract (VT) in the mid-sagittal plane. We propose an algorithm for optimal placement of EMA sensors using dynamic programming. Experiments are performed using rtMRI video frames for read speech from four subjects with upper and lower lips as two fixed points. One optimal sensor on the upper VT boundary is found to be at an average distance of 21.41(±25.54) mm from the velum tip. Similarly, for the lower VT boundary, one optimal sensor is found at the lower incisor at a distance of 26.37(±8.08) mm from lower lip and three optimal sensors on tongue – at tongue tip (19.93(±11.45) mm from tongue base) and 38.2(±11.52) mm and 80.51(±13.51) mm away from the tongue tip.","['Ashok Kumar Pattem', 'Aravind Illa', 'Amber Afshan', 'Prasanta Kumar Ghosh']",January 2018,Computer Speech & Language,"['Electromagnetic articulography', 'Sensor placement', 'Speech production']",Optimal sensor placement in electromagnetic articulography recording for speech production study☆
147,"Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.","['Christophe Cerisara', 'Pavel Král', 'Ladislav Lenc']",January 2018,Computer Speech & Language,"['Dialogue act', 'Deep learning', 'LSTM', 'Word embeddings', 'Word2vec']",On the effects of using word2vec representations in neural networks for dialogue act recognition☆
148,"Although there has been good progress in English sentiment analysis and resources, studies in English cannot be directly used in Chinese owing to the nature of Chinese language. Previous studies suggested adopting linguistic information, such as grammar and morpheme information, to assist in sentiment analysis for Chinese text. However, morpheme-based approaches have a problem in identifying seeds. In addition, these methods do not take advantage of radicals in the characters, which contain a great deal of semantic information. A Chinese word is composed of one or more characters, each of which has its radical part. We can interpret the partial meaning of a character by analyzing that of the radical in the character. Therefore, we not only consider the radical information as the semantic root of a character, but also consider the radical parts between characters in a word as an appropriate linguistic unit for conducting sentiment analysis.In this study, we conducted a series of experiments using radicals as the feature unit in sentiment analysis. Using segmented results from part-of-speech tools as a meaningful linguistic unit (word) in Chinese, we conducted analyses of single-feature word (unigram) and frequently seen two words (pointwise mutual information collocated bigrams) through various sentiment analysis measures. It is concluded that radical features could work better than word features and would consume less computing memory and time. An extended study of the extraction of seeds was also conducted, and the results indicated that 50 seed radical features performed well. A cross-corpus comparison was also conducted; the results demonstrated that the use of 50 extracted radical features as domain-dependent keywords worked better than other sentiment analysis strategies. This study confirmed that radical information could be adopted as a feature unit in sentiment analysis and that domain-dependent radicals could be reused in different corpora.","['August F.Y. Chao', 'Heng-Li Yang']",January 2018,Computer Speech & Language,"['Sentiment analysis', 'Chinese radical', 'Restaurant review analysis', 'Domain-dependent seed']",Using Chinese radical parts for sentiment analysis and domain-dependent seed set extraction☆
149,"Recognizer Output Voting Error Reduction (ROVER) has been widely used for system combination in automatic speech recognition (ASR). In order to select the most appropriate words to insert at each position in the output transcriptions, some ROVER extensions rely on critical information such as confidence scores and other ASR decoder features. This information, which is not always available, highly depends on the decoding process and sometimes tends to overestimate the real quality of the recognized words. In this paper we propose a novel variant of ROVER that takes advantage of ASR quality estimation (QE) for ranking the transcriptions at “segment level” instead of: i) relying on confidence scores, or ii) feeding ROVER with randomly ordered hypotheses. We first introduce an effective set of features to compensate for the absence of ASR decoder information. Then, we apply QE techniques to perform accurate hypothesis ranking at segment-level before starting the fusion process. The evaluation is carried out on two different tasks, in which we respectively combine hypotheses coming from independent ASR systems and multi-microphone recordings. In both tasks, it is assumed that the ASR decoder information is not available. The proposed approach significantly outperforms standard ROVER and it is competitive with two strong oracles that exploit prior knowledge about the real quality of the hypotheses to be combined. Compared to standard ROVER, the absolute WER improvements in the two evaluation scenarios range from 0.5% to 7.3%.","['Shahab Jalalvand', 'Matteo Negri', 'Daniele Falavigna', 'Marco Matassoni', 'Marco Turchi']",January 2018,Computer Speech & Language,"['Automatic speech recognition', 'Quality estimation', 'System combination']",Automatic quality estimation for ASR system combination☆
150,"The performance of state-of-the-art i-vector speaker verification systems relies on a large amount of training data for probabilistic linear discriminant analysis (PLDA) modeling. During the evaluation, it is also crucial that the target condition data is matched well with the development data used for PLDA training. However, in many practical scenarios, these systems have to be developed, and trained, using data that is often outside the domain of the intended application, since the collection of a significant amount of in-domain data is often difficult. Experimental studies have found that PLDA speaker verification performance degrades significantly due to this development/evaluation mismatch. This paper introduces a domain-invariant linear discriminant analysis (DI-LDA) technique for out-domain PLDA speaker verification that compensates domain mismatch in the LDA subspace. We also propose a domain-invariant probabilistic linear discriminant analysis (DI-PLDA) technique for domain mismatch modeling in the PLDA subspace, using only a small amount of in-domain data. In addition, we propose the sequential and score-level combination of DI-LDA, and DI-PLDA to further improve out-domain speaker verification performance. Experimental results show the proposed domain mismatch compensation techniques yield at least 27% and 14.5% improvement in equal error rate (EER) over a pooled PLDA system for telephone-telephone and interview-interview conditions, respectively. Finally, we show that the improvement over the baseline pooled system can be attained even when significantly reducing the number of in-domain speakers, down to 30 in most of the evaluation conditions.","['Md Hafizur Rahman', 'Ahilan Kanagasundaram', 'Ivan Himawan', 'David Dean', 'Sridha Sridharan']",January 2018,Computer Speech & Language,"['Speaker verification', 'I-vector', 'Domain mismatch compensation', 'DI-LDA', 'DI-PLDA', 'DI-PLDA[DI-LDA]', 'Score fusion']",Improving PLDA speaker verification performance using domain mismatch compensation techniques☆
151,"In this paper, we propose pass-phrase dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) to integrate the pass-phrase identification process into the conventional TD-SV system, where a PBM is derived from a text-independent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV systems. The performance of the proposed method is compared to conventional text-independent background model based TD-SV systems using either Gaussian mixture model (GMM)-universal background model (UBM) or hidden Markov model (HMM)-UBM or i-vector paradigms. In addition, we consider two approaches to build PBMs: speaker-independent and speaker-dependent. We show that the proposed method significantly reduces the error rates of text-dependent speaker verification for the non-target types: target-wrong and impostor-wrong while it maintains comparable TD-SV performance when impostors speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases that consist of short utterances.","['Achintya Kumar Sarkar', 'Zheng-Hua Tan']",January 2018,Computer Speech & Language,"['Pass-phrase dependent background models (PBMs)', 'GMM-UBM', 'HMM-UBM', 'I-vector', 'Text-dependent', 'Speaker verification']",Incorporating pass-phrase dependent background models for text-dependent speaker verification☆
152,"We investigate algorithms and tools for the semi-automatic authoring of grammars for spoken dialogue systems (SDS) proposing a framework that spans from corpora creation to grammar induction algorithms. A realistic human-in-the-loop approach is followed balancing automation and human intervention to optimize cost to performance ratio for grammar development. Web harvesting is the main approach investigated for eliciting spoken dialogue textual data, while crowdsourcing is also proposed as an alternative method. Several techniques are presented for constructing web queries and filtering the acquired corpora. We also investigate how the harvested corpora can be used for the automatic and semi-automatic (human-in-the-loop) induction of grammar rules. SDS grammar rules and induction algorithms are grouped into two types, namely, low- and high-level. Two families of algorithms are investigated for rule induction: one based on semantic similarity and distributional semantic models, and the other using more traditional statistical modeling approaches (e.g., slot-filling algorithms using Conditional Random Fields). Evaluation results are presented for two domains and languages. High-level induction precision scores up to 60% are obtained. Results advocate the portability of the proposed features and algorithms across languages and domains.","['Elias Iosif', 'Ioannis Klasinas', 'Georgia Athanasopoulou', 'Elisavet Palogiannidi', 'Spiros Georgiladakis', 'Katerina Louka', 'Alexandros Potamianos']",January 2018,Computer Speech & Language,"['Spoken dialogue systems', 'Grammar induction', 'Corpora creation', 'Semantic similarity', 'Web mining', 'Crowdsourcing']",Speech understanding for spoken dialogue systems: From corpus harvesting to grammar rule induction☆
153,"Acoustic properties of speech samples can provide important cues in the assessment of voice pathology and cognitive function. The goal of this study is to develop novel algorithms for robust and accurate estimation of speech features and employ them to build probabilistic speech models for characterizing and analyzing clinical speech. Toward this goal, we adopt a harmonic model (HM) of speech. We overcome certain drawbacks of this model and introduce an improved version of HM that leads us to accurate and reliable estimation of voiced segments, fundamental frequency, HNR, jitter, and shimmer. We evaluate the performance of our improved HM in the context of voicing detection and pitch estimation with other state-of-the-art techniques on the Keele data set. Through extensive experiments on several noisy conditions, we demonstrate that the proposed improvements provide substantial gains over other popular methods under different noise levels and environments. Next, we investigate the utility of developed measures on the speech-based assessment of cognitive impairments including clinical depression and autism spectrum disorder (ASD). Our preliminary results on two clinical tasks demonstrate the promise of our improved HM features in practical applications.","['Meysam Asgari', 'Izhak Shafran']",January 2018,Computer Speech & Language,"['Pitch tracking', 'Voice activity detection', 'Modified harmonic model']",Improvements to harmonic model for extracting better speech features in clinical applications☆
154,"Shifting from a single to a multi-microphone setting, distant speech recognition can be benefited from the multiple instances of the same utterance in many ways. An effective approach, especially when microphones are not organized in an array fashion, is given by channel selection (CS), which assumes that for each utterance there is at least one channel that can improve the recognition results when compared to the decoding of the remaining channels. In order to identify this most favourable channel, a possible approach is to estimate the degree of distortion that characterizes each microphone signal. In a reverberant environment, this distortion can vary significantly across microphones, for instance due to the orientation of the speaker’s head. In this work, we investigate on the application of cepstral distance as a distortion measure that turns out to be closely related to properties of the room acoustics, such as reverberation time and direct-to-reverberant ratio. From this measure, a blind CS method is derived, which relies on a reference computed by averaging log magnitude spectra of all the microphone signals. Another aim of our study is to propose a novel methodology to analyze CS under a wide set of experimental conditions and setup variations, which depend on the sound source position, its orientation, and the microphone network configuration. Based on the use of prior information, we introduce an informed technique to predict CS performance. Experimental results show both the effectiveness of the proposed blind CS method and the value of the aforementioned analysis methodology. The experiments were conducted using different sets of real and simulated data, the latter ones derived from synthetic and from measured impulse responses. It is demonstrated that the proposed blind CS method is well related to the oracle selection of the best recognized channel. Moreover, our method outperforms a state-of-the-art one, especially on real data.","['Cristina Guerrero Flores', 'Georgina Tryfou', 'Maurizio Omologo']",January 2018,Computer Speech & Language,"['Distant speech recognition', 'Channel selection', 'Cepstral distance', 'Reverberation', 'Direct to reverberant ratio', 'T60']",Cepstral distance based channel selection for distant speech recognition☆
155,"In this work, we propose sparse representation based features for speech units classification tasks. In order to effectively capture the variations in a speech unit, the proposed method employs multiple class specific dictionaries. Here, the training data belonging to each class is clustered into multiple clusters, and a principal component analysis (PCA) based dictionary is learnt for each cluster. It has been observed that coefficients corresponding to middle principal components can effectively discriminate among different speech units. Exploiting this observation, we propose to use a transformation function known as weighted decomposition (WD) of principal components, which is used to emphasize the discriminative information present in the PCA-based dictionary. In this paper, both raw speech samples and mel frequency cepstral coefficients (MFCC) are used as an initial representation for feature extraction. For comparison, various popular dictionary learning techniques such as K-singular value decomposition (KSVD), simultaneous codeword optimization (SimCO) and greedy adaptive dictionary (GAD) are also employed in the proposed framework. The effectiveness of the proposed features is demonstrated using continuous density hidden Markov model (CDHMM) based classifiers for (i) classification of isolated utterances of E-set of English alphabet, (ii) classification of consonant-vowel (CV) segments in Hindi language and (iii) classification of phoneme from TIMIT phonetic corpus.","['Pulkit Sharma', 'Vinayak Abrol', 'A.D. Dileep', 'Anil Kumar Sao']",January 2018,Computer Speech & Language,"['Sparse representation', 'Dictionary learning', 'Speech recognition']",Sparse coding based features for speech units classification☆
156,"We investigate the problem of automatic detection of annotation errors in single-speaker read-speech corpora used for speech synthesis. For the purpose of annotation error detection, we adopt an anomaly detection framework in which correctly annotated words are considered as normal examples on which the detection methods are trained. Misannotated words are then taken as anomalous examples which do not conform to normal patterns of the trained detection models. We propose and evaluate several anomaly detection models – Gaussian distribution based detectors, Grubbs’ test based detector, and one-class support vector machine based detector. Word-level feature sets including basic features derived from forced alignment and various acoustic, spectral, phonetic, and positional features are examined to find an optimal set of features for each anomaly detector. The results with F1 score being almost 89% show that anomaly detection could help detecting annotation errors in read-speech corpora for speech synthesis. Furthermore, dimensionality reduction techniques are also examined to automatically reduce the number of features used to describe the annotated words. We show that the automatically reduced feature sets achieve statistically similar results as the hand-crafted feature sets. We also conducted additional experiments to investigate both robustness of the proposed anomaly detection framework with respect to particular data sets used for development and evaluation and the influence of the number of examples needed for anomaly detection. We show that a reasonably good detection performance could be reached with using significantly fewer examples during the detector development phase. We also propose a concept of a voting detector – a combination of anomaly detectors in which each “single” detector “votes” on whether or not a testing word is annotated correctly, and the final decision is then made by aggregating the votes. Our results show that the voting detector has a potential to overcome each of the single anomaly detectors. Furthermore, we compare the proposed anomaly detection framework to a classification-based approach (which, unlike anomaly detection, needs to use anomalous examples during training) and we show that both approaches lead to statistically comparable results when all available anomalous examples are utilized during detector/classifier development. However, when a smaller number of anomalous examples are used, the proposed anomaly detection framework clearly outperforms the classification-based approach. A final listening test showed the effectiveness of the proposed anomaly-based annotation error detection for improving the quality of synthetic speech.","['Jindřich Matoušek', 'Daniel Tihelka']",November 2017,Computer Speech & Language,"['Annotation error detection', 'Anomaly detection', 'Read speech corpora', 'Speech synthesis']",Anomaly-based annotation error detection in speech-synthesis corpora☆
157,"Speaker de-identification approaches must accomplish three main goals: universality, naturalness and reversibility. The main drawback of the traditional approach to speaker de-identification using voice conversion techniques is its lack of universality, since a parallel corpus between the input and target speakers is necessary to train the conversion parameters. It is possible to make use of a synthetic target to overcome this issue, but this harms the naturalness of the resulting de-identified speech. Hence, a technique is proposed in this paper in which a pool of pre-trained transformations between a set of speakers is used as follows: given a new user to de-identify, its most similar speaker in this set of speakers is chosen as the source speaker, and the speaker that is the most dissimilar to the source speaker is chosen as the target speaker. Speaker similarity is measured using the i-vector paradigm, which is usually employed as an objective measure of speaker de-identification performance, leading to a system with high de-identification accuracy. The transformation method is based on frequency warping and amplitude scaling, in order to obtain natural sounding speech while masking the identity of the speaker. In addition, compared to other voice conversion approaches, the proposed method is easily reversible. Experiments were conducted on Albayzin database, and performance was evaluated in terms of objective and subjective measures. These results showed a high success when de-identifying speech, as well as a great naturalness of the transformed voices. In addition, when making the transformation parameters available to a trusted holder, it is possible to invert the de-identification procedure, hence recovering the original speaker identity. The computational cost of the proposed approach is small, making it possible to produce de-identified speech in real-time with a high level of naturalness.","['Carmen Magariños', 'Paula Lopez-Otero', 'Laura Docio-Fernandez', 'Eduardo Rodriguez-Banga', 'Daniel Erro', 'Carmen Garcia-Mateo']",November 2017,Computer Speech & Language,"['Speaker de-identification', 'Voice transformation', 'Speaker re-identification', 'Frequency warping', 'Amplitude scaling', 'i-vector']",Reversible speaker de-identification using pre-trained transformation functions☆
158,"Inspired by the success of Deep Neural Networks (DNN) in text-independent speaker recognition, we have recently demonstrated that similar ideas can also be applied to the text-dependent speaker verification task. In this paper, we describe new advances with our state-of-the-art i-vector based approach to text-dependent speaker verification, which also makes use of different DNN techniques. In order to collect sufficient statistics for i-vector extraction, different frame alignment models are compared such as GMMs, phonemic HMMs or DNNs trained for senone classification. We also experiment with DNN based bottleneck features and their combinations with standard MFCC features. We experiment with few different DNN configurations and investigate the importance of training DNNs on 16 kHz speech. The results are reported on RSR2015 dataset, where training material is available for all possible enrollment and test phrases. Additionally, we report results also on more challenging RedDots dataset, where the system is built in truly phrase-independent way.","['Hossein Zeinali', 'Hossein Sameti', 'Lukáš Burget', 'Jan “Honza” Černocký']",November 2017,Computer Speech & Language,"['Deep Neural Network', 'Text-dependent', 'Speaker verification', 'i-Vector', 'Frame alignment', 'Bottleneck features']","Text-dependent speaker verification based on i-vectors, Neural Networks and Hidden Markov Models☆"
159,"The introduction of factor analysis techniques in a speaker diarization system enhances its performance by facilitating the use of speaker specific information, by improving the suppression of nuisance factors such as phonetic content, and by facilitating various forms of adaptation. This paper describes a state-of-the-art iVector-based diarization system which employs factor analysis and adaptation on all levels. The diarization modules relevant for this work are: the speaker segmentation which searches for speaker boundaries and the speaker clustering which aims at grouping speech segments of the same speaker. The speaker segmentation relies on speaker factors which are extracted on a frame-by-frame basis using eigenvoices. We incorporate soft voice activity detection in this extraction process as the speaker change detection should be based on speaker information only and we want it to disregard the non-speech frames by applying speech posteriors. Potential speaker boundaries are inserted at positions where rapid changes in speaker factors are witnessed. By employing Mahalanobis distances, the effect of the phonetic content can be further reduced, which results in more accurate speaker boundaries. This iVector-based segmentation significantly outperforms more common segmentation methods based on the Bayesian Information Criterion (BIC) or speech activity marks. The speaker clustering employs two-step Agglomerative Hierarchical Clustering (AHC): after initial BIC clustering, the second cluster stage is realized by either an iVector Probabilistic Linear Discriminant Analysis (PLDA) system or Cosine Distance Scoring (CDS) of extracted speaker factors. The segmentation system is made adaptive on a file-by-file basis by iterating the diarization process using eigenvoice matrices adapted (unsupervised) on the output of the previous iteration. Assuming that for most use cases material similar to the recording in question is readily available, unsupervised domain adaptation of the speaker clustering is possible as well. We obtain this by expanding the eigenvoice matrix used during speaker factor extraction for the CDS clustering stage with a small set of new eigenvoices that, in combination with the initial generic eigenvoices, models the recurring speakers and acoustic conditions more accurately. Experiments on the COST278 multilingual broadcast news database show the generation of significantly more accurate speaker boundaries by using adaptive speaker segmentation which also results in more accurate clustering. The obtained speaker error rate (SER) can be further reduced by another 13% relative to 7.4% via domain adaptation of the CDS clustering.","['Brecht Desplanques', 'Kris Demuynck', 'Jean-Pierre Martens']",November 2017,Computer Speech & Language,"['Speaker diarization', 'Speaker segmentation', 'iVector', 'Domain adaptation', 'Factor analysis']",Adaptive speaker diarization of broadcast news based on factor analysis☆
160,"Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric—the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.","['Janez Starc', 'Dunja Mladenić']",November 2017,Computer Speech & Language,"['Natural Language Inference', 'Natural language generation', 'Machine learning', 'Dataset construction', 'Generative neural network', 'Recurrent neural network']",Constructing a Natural Language Inference dataset using generative neural networks☆
161,"Automatic extraction and analysis of meaning-related information from natural language data has been an important issue in a number of research areas, such as natural language processing (NLP), text mining, corpus linguistics, and data science. An important aspect of such information extraction and analysis is the semantic annotation of language data using a semantic tagger. In practice, various semantic annotation tools have been designed to carry out different levels of semantic annotation, such as topics of documents, semantic role labeling, named entities or events. Currently, the majority of existing semantic annotation tools identify and tag partial core semantic information in language data, but they tend to be applicable only for modern language corpora. While such semantic analyzers have proven useful for various purposes, a semantic annotation tool that is capable of annotating deep semantic senses of all lexical units, or all-words tagging, is still desirable for a deep, comprehensive semantic analysis of language data. With large-scale digitization efforts underway, delivering historical corpora with texts dating from the last 400 years, a particularly challenging aspect is the need to adapt the annotation in the face of significant word meaning change over time. In this paper, we report on the development of a new semantic tagger (the Historical Thesaurus Semantic Tagger), and discuss challenging issues we faced in this work. This new semantic tagger is built on existing NLP tools and incorporates a large-scale historical English thesaurus linked to the Oxford English Dictionary. Employing contextual disambiguation algorithms, this tool is capable of annotating lexical units with a historically-valid highly fine-grained semantic categorization scheme that contains about 225,000 semantic concepts and 4,033 thematic semantic categories. In terms of novelty, it is adapted for processing historical English data, with rich information about historical usage of words and a spelling variant normalizer for historical forms of English. Furthermore, it is able to make use of knowledge about the publication date of a text to adapt its output. In our evaluation, the system achieved encouraging accuracies ranging from 77.12% to 91.08% on individual test texts. Applying time-sensitive methods improved results by as much as 3.54% and by 1.72% on average.","['Scott Piao', 'Fraser Dallachy', 'Alistair Baron', 'Jane Demmen', 'Steve Wattam', 'Philip Durkin', 'James McCracken', 'Paul Rayson', 'Marc Alexander']",November 2017,Computer Speech & Language,"['Semantic annotation', 'Natural language processing', 'Historical thesaurus', 'Semantic lexicon', 'Corpus annotation', 'Language technology']",A time-sensitive historical thesaurus-based semantic tagger for deep semantic annotation☆
162,"The estimation of glottal closure instants (GCIs) plays a vital role in several glottal synchronous applications, and may not be restricted to clean speech. This necessitates the development of a GCI estimation algorithm that performs as well on degraded speech as on clean speech. Degradations in speech may be in the form of spectral or temporal perturbations. This could result in several spurious discontinuities, as in noisy speech, excitations that are not well-defined, as in band-limited speech, aperiodicity and variations in amplitude, as in pathological speech, thereby making the task of identifying GCIs more challenging. In this regard, a conditional group-delay/phase-difference-based (PD) algorithm that was initially proposed for use on clean speech is extended to operate on degraded speech, specifically telephone, noisy, and pathological speech. The performance of this algorithm is compared with six existing algorithms, in terms of identification, false alarm, and miss rates, and identification accuracy. It is observed that the PD algorithm is robust to degradations in speech and performs better than or on par with existing algorithms in all cases considered. Further, it is also observed that unlike existing algorithms, the PD algorithm scarcely estimates GCIs in non-voiced regions and this is verified in terms of a new metric proposed in the paper, namely, the spurious instants rate in non-voiced regions.","['G. Anushiya Rachel', 'P. Vijayalakshmi', 'T. Nagarajan']",November 2017,Computer Speech & Language,"['Glottal closure instants', 'Phase difference', 'Group delay', 'Degraded speech', 'Telephone', 'Noisy', 'Pathological']",Estimation of glottal closure instants from degraded speech using a phase-difference-based algorithm☆
163,"Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units—effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported—in the order of 70–80% for speaker-dependent and 80–95% for speaker-independent systems—highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system’s discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.","['Herman Kamper', 'Aren Jansen', 'Sharon Goldwater']",November 2017,Computer Speech & Language,"['Unsupervised speech processing', 'Representation learning', 'Segmentation', 'Clustering', 'Language acquisition']",A segmental framework for fully-unsupervised large-vocabulary speech recognition
164,,"['Christoph Draxler', 'Jonathan Harrington', 'Florian Schiel']",November 2017,Computer Speech & Language,[],Towards the next generation of speech tools and corpora
165,"In this paper, we evaluate how speaker familiarity influences the engagement times and performance of blind children and young adults when playing audio games made with different synthetic voices. We also show how speaker familiarity influences speaker and synthetic speech recognition. For the first experiment we develop synthetic voices of school children, their teachers and of speakers that are unfamiliar to them and use each of these voices to create variants of two audio games: a memory game and a labyrinth game. Results show that pupils have significantly longer engagement times and better performance when playing games that use synthetic voices built with their own voices. These findings can be used to improve the design of audio games and lecture books for blind and visually impaired children and young adults. In the second experiment we show that blind children and young adults are better in recognizing synthetic voices than their visually impaired companions. We also show that the average familiarity with a speaker and the similarity between a speaker’s synthetic and natural voice are correlated to the speaker’s synthetic voice recognition rate.","['Michael Pucher', 'Bettina Zillinger', 'Markus Toman', 'Dietmar Schabus', 'Cassia Valentini-Botinhao', 'Junichi Yamagishi', 'Erich Schmid', 'Thomas Woltron']",November 2017,Computer Speech & Language,"['Speech perception', 'Speech synthesis', 'Audio games', 'Blind individuals', 'Child speech synthesis']",Influence of speaker familiarity on blind and visually impaired children’s and young adults’ perception of synthetic voices☆
166,"Change in voice quality (VQ) is one of the first precursors of Parkinson’s disease (PD). Specifically, impacted phonation and articulation causes the patient to have a breathy, husky-semiwhisper and hoarse voice. A goal of this paper is to characterise a VQ spectrum – the composition of non-modal phonations – of voice in PD. The paper relates non-modal healthy phonations: breathy, creaky, tense, falsetto and harsh, with disordered phonation in PD. First, statistics are learned to differentiate the modal and non-modal phonations. Statistics are computed using phonological posteriors, the probabilities of phonological features inferred from the speech signal using a deep learning approach. Second, statistics of disordered speech are learned from PD speech data comprising 50 patients and 50 healthy controls. Third, Euclidean distance is used to calculate similarity of non-modal and disordered statistics, and the inverse of the distances is used to obtain the composition of non-modal phonation in PD. Thus, pathological voice quality is characterised using healthy non-modal voice quality “base/eigenspace”. The obtained results are interpreted as the voice of an average patient with PD and can be characterised by the voice quality spectrum composed of 30% breathy voice, 23% creaky voice, 20% tense voice, 15% falsetto voice and 12% harsh voice. In addition, the proposed features were applied for prediction of the dysarthria level according to the Frenchay assessment score related to the larynx, and significant improvement is obtained for reading speech task. The proposed characterisation of VQ might also be applied to other kinds of pathological speech.","['Milos Cernak', 'Juan Rafael Orozco-Arroyave', 'Frank Rudzicz', 'Heidi Christensen', 'Juan Camilo Vásquez-Correa', 'Elmar Nöth']",November 2017,Computer Speech & Language,"['Phonological features', 'Non-modal phonation', 'Parkinson’s disease']",Characterisation of voice quality of Parkinson’s disease using differential phonological posterior features☆
167,"We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: it involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.","['Taehwan Kim', 'Jonathan Keane', 'Weiran Wang', 'Hao Tang', 'Jason Riggle', 'Gregory Shakhnarovich', 'Diane Brentari', 'Karen Livescu']",November 2017,Computer Speech & Language,"['American Sign Language', 'Fingerspelling recognition', 'Segmental model', 'Deep neural network', 'Adaptation']","Lexicon-free fingerspelling recognition from video: Data, models, and signer adaptation☆"
168,"In this paper an unsupervised clustering algorithm is developed for acoustic data in the context of speech recognition tasks. One of the key features of the algorithm is scalability to large data sets. Specifically, given the unlabeled training and test sets, the class-labels of the utterances are obtained in an automatic manner. The extracted labels may correspond to the speakers in the speech corpus if the data is relatively clean. The proposed scheme is attractive from an industrial perspective as it alleviates the need to store the speaker-labels manually, saving considerable amount of human efforts and expenses. The core of the algorithm comprises a three-stage architecture that processes the input data one after the other, while each stage is designed to perform a well-defined and specific task. In more detail, the first-pass involves a bottom-up clustering mechanism, the second-pass comprises a cluster splitting operation and the third-pass consists of a cluster refining process. Each of the stages allows for data parallelization using multiple CPUs that leads to faster computation. Two alternative forms of the algorithm are presented – the first considers Gaussian distributions and the other i-Vectors – to facilitate the clustering. Although the algorithm may find applications in various realms of speech recognition, in this paper, the effectiveness of the schemes are evaluated by means of speaker adaptive training (SAT) and speaker-aware training of DNN-HMM acoustic models. In particular, experiments are conducted on the Switchboard task to extract the speaker-labels for the utterances in the training and test sets. It is shown that the SAT DNN-HMM trained using the Gaussian based scheme yields a 7.2% relative improvement in the ASR accuracy over the speaker independent DNN-HMM, whereas the i-Vector approach provides an additional improvement, amounting to a 10.8% relative gain overall. The standard SAT DNN-HMM developed using the ground-truth speaker-labels is found to be only 2.7% relative better than the proposed scheme. Similar observation is made as with speaker-aware training. The analysis of computational complexity, conducted stage by stage, demonstrates the scalability of the proposed algorithms.",['Shakti P. Rath'],November 2017,Computer Speech & Language,"['Unsupervised clustering', 'Bottom-up clustering', 'Large scale training', 'DNN-HMM', 'Speaker adaptive training', 'i-Vector']",Scalable algorithms for unsupervised clustering of acoustic data for speech recognition☆
169,"In recent years, the interest in research in speech understanding and spoken interaction has soared due to the emergence of virtual personal assistants. However, while the ability of these agents to recognise conversational speech is maturing rapidly, their ability to understand and interact is still limited. At the same time we have witnessed the development of the number of models based on machine learning that made a huge impact on spoken language understanding accuracies and the interaction quality overall. This special issue brings together a number of articles that tackle different aspects of spoken language understanding and interaction: clarifications in dialogues, adaptation to different domains, semantic tagging and error handling. These studies all have a common purpose of building human-like conversational systems.","['Milica Gašić', 'Dilek Hakkani-Tür', 'Asli Celikyilmaz']",November 2017,Computer Speech & Language,[],EditorialSpoken language understanding and interaction: machine learning for human-like conversational systems
170,"Multilingual training of neural networks has proven to be simple yet effective way to deal with multilingual training corpora. It allows to use several resources to jointly train a language independent representation of features, which can be encoded into low-dimensional feature set by embedding narrow bottleneck layer to the network. In this paper, we analyze such features on the task of spoken language recognition (SLR), focusing on practical aspects of training bottleneck networks and analyzing their integration in SLR. By comparing properties of mono and multilingual features we show the suitability of multilingual training for SLR. The state-of-the-art performance of these features is demonstrated on the NIST LRE09 database.","['Radek Fér', 'Pavel Matějka', 'František Grézl', 'Oldřich Plchot', 'Karel Veselý', 'Jan Honza Černocký']",November 2017,Computer Speech & Language,"['Multilingual training', 'Bottleneck features', 'Spoken language recognition']",Multilingually trained bottleneck features in spoken language recognition☆
171,"In this article, we present the first child emotional speech corpus in Russian, called “EmoChildRu”, collected from 3 to 7 years old children. The base corpus includes over 20 K recordings (approx. 30 h), collected from 120 children. Audio recordings are carried out in three controlled settings by creating different emotional states for children: playing with a standard set of toys; repetition of words from a toy-parrot in a game store setting; watching a cartoon and retelling of the story, respectively. This corpus is designed to study the reflection of the emotional state in the characteristics of voice and speech and for studies of the formation of emotional states in ontogenesis. A portion of the corpus is annotated for three emotional states (comfort, discomfort, neutral). Additional data include the results of the adult listeners’ analysis of child speech, questionnaires, as well as annotation for gender and age in months. We also provide several baselines, comparing human and machine estimation on this corpus for prediction of age, gender and comfort state. While in age estimation, the acoustics-based automatic systems show higher performance, they do not reach human perception levels in comfort state and gender classification. The comparative results indicate the importance and necessity of developing further linguistic models for discrimination.","['Heysem Kaya', 'Albert Ali Salah', 'Alexey Karpov', 'Olga Frolova', 'Aleksey Grigorev', 'Elena Lyakso']",November 2017,Computer Speech & Language,"['Emotional child speech', 'Perception experiments', 'Spectrographic analysis', 'Emotional states', 'Age recognition', 'Gender recognition', 'Computational paralinguistics']","Emotion, age, and gender classification in children’s speech by humans and machines☆"
172,"Despite recent advances in automatic speech recognition, one of the main stumbling blocks to the widespread adoption of Spoken Dialogue Systems is the lack of reliability of automatic speech recognizers. In this paper, we offer a two-tier error-correction process that harnesses syntactic, semantic and pragmatic information to improve the understanding of spoken referring expressions, specifically descriptions of objects in physical spaces. A syntactic-semantic tier offers generic corrections to perceived ASR errors on the basis of syntactic expectations of a semantic model, and passes the corrected texts to a language understanding system. The output of this system, which consists of pragmatic interpretations, is then refined by a contextual-phonetic tier, which prefers interpretations that are phonetically similar to the mis-heard words. Our results, obtained on a corpus of 341 referring expressions, show that syntactic-semantic error correction significantly improves interpretation performance, and contextual-phonetic refinements yield further improvements.","['Ingrid Zukerman', 'Andisheh Partovi']",November 2017,Computer Speech & Language,"['Spoken language understanding', 'Referring expressions', 'Error correction', 'Pragmatic interpretation', 'Physical spaces', 'Syntactic-semantic model', 'Contextual-phonetic model']",Improving the understanding of spoken referring expressions through syntactic-semantic and contextual-phonetic error-correction☆
173,"In this paper, we introduce a simple unsupervised framework for pre-training hidden-unit conditional random fields (HUCRFs), i.e., learning initial parameter estimates for HUCRFs prior to supervised training.Our framework exploits the model structure of HUCRFs to make effective use of unlabeled data from the same domain or labeled data from a different domain. The key idea is to use the separation of HUCRF parameters between observations and labels: this allows us to pre-train observation parameters independently of label parameters. Pre-training is achieved by creating pseudo-labels from such resources. In the case of unlabeled data, we cluster observations and use the resulting clusters as pseudo-labels. Observation parameters can be trained on these resources and then transferred to initialize the supervised training process on the target labeled data. Experiments on various sequence labeling tasks demonstrate that the proposed pre-training method consistently yields significant improvement in performance. The core idea could be extended to other learning techniques including deep learning. We applied the proposed technique to recurrent neural networks (RNN) with long short term memory (LSTM) architecture and obtained similar gains.","['Young-Bum Kim', 'Karl Stratos', 'Ruhi Sarikaya']",November 2017,Computer Speech & Language,"['Pre-training', 'Transfer learning', 'Spoken language understanding', 'Sequence labeling', 'Conditional random fiends', 'Multi-sense clustering', 'Word embedding', 'Hidden unit conditional random fields', 'LSTMs']",A Framework for pre-training hidden-unit conditional random fields and its extension to long short term memory networks☆
174,"Phone tokenisers are used in spoken language recognition (SLR) to obtain elementary phonetic information. We present a study on the use of deep neural network tokenisers. Unsupervised crosslingual adaptation was performed to adapt the baseline tokeniser trained on English conversational telephone speech data to different languages. Two training and adaptation approaches, namely cross-entropy adaptation and state-level minimum Bayes risk adaptation, were tested in a bottleneck i-vector and a phonotactic SLR system. The SLR systems using the tokenisers adapted to different languages were combined using score fusion, giving 7–18% reduction in minimum detection cost function (minDCF) compared with the baseline configurations without adapted tokenisers. Analysis of results showed that the ensemble tokenisers gave diverse representation of phonemes, thus bringing complementary effects when SLR systems with different tokenisers were combined. SLR performance was also shown to be related to the quality of the adapted tokenisers.","['Raymond W.M. Ng', 'Mauro Nicolao', 'Thomas Hain']",November 2017,Computer Speech & Language,"['Language recognition', 'Unsupervised adaptation', 'Crosslingual adaptation', 'Tokenisation', 'Phonotactic SLR']",Unsupervised crosslingual adaptation of tokenisers for spoken language recognition☆
175,"Peer-Led Team Learning (PLTL) is a learning methodology where a peer-leader co-ordinate a small-group of students to collaboratively solve technical problems. PLTL have been adopted for various science, engineering, technology and maths courses in several US universities. This paper proposed and evaluated a speech system for behavioral analysis of PLTL groups. It could help in identifying the best practices for PLTL. The CRSS-PLTL corpus was used for evaluation of developed algorithms. In this paper, we developed a robust speech activity detection (SAD) by fusing the outputs of a DNN-based pitch extractor and an unsupervised SAD based on voicing measures. Robust speaker diarization system consisted of bottleneck features (from stacked autoencoder) and informed HMM-based joint segmentation and clustering system. Behavioral characteristics such as participation, dominance, emphasis, curiosity and engagement were extracted by acoustic analyses of speech segments belonging to all students. We proposed a novel method for detecting question inflection and performed equal error rate analysis on PLTL corpus. In addition, a robust approach for detecting emphasized speech regions was also proposed. Further, we performed exploratory data analysis for understanding the distortion present in CRSS-PLTL corpus as it was collected in naturalistic scenario. The ground-truth Likert scale ratings were used for capturing the team dynamics in terms of student’s responses to a variety of evaluation questions. Results suggested the applicability of proposed system for behavioral analysis of small-group conversations such as PLTL, work-place meetings etc..","['Harishchandra Dubey', 'Abhijeet Sangwan', 'John H.L. Hansen']",November 2017,Computer Speech & Language,"['Behavioral speech processing', 'Bottleneck features', 'Curiosity', 'Deep neural network', 'Dominance', 'Auto-encoder', 'Emphasis', 'Engagement', 'Peer-led team learning', 'Speaker diarization', 'Small-group conversations']",Using speech technology for quantifying behavioral characteristics in peer-led team learning sessions☆
176,"Deep learning is revolutionizing speech and natural language technologies since it is offering an effective way to train systems and obtaining significant improvements. The main advantage of deep learning is that, by developing the right architecture, the system automatically learns features from data without the need of explicitly designing them. This machine learning perspective is conceptually changing how speech and natural language technologies are addressed. In the case of Machine Translation (MT), deep learning was first introduced in standard statistical systems. By now, end-to-end neural MT systems have reached competitive results. This special issue introductory paper addresses how deep learning has been gradually introduced in MT. This introduction covers all topics contained in the papers included in this special issue, which basically are: integration of deep learning in statistical MT; development of the end-to-end neural MT system; and introduction of deep learning in interactive MT and MT evaluation. Finally, this introduction sketches some research directions that MT is taking guided by deep learning.","['Marta R. Costa-jussà', 'Alexandre Allauzen', 'Loïc Barrault', 'Kyunghun Cho', 'Holger Schwenk']",November 2017,Computer Speech & Language,"['Machine translation', 'Deep learning,']",Introduction to the special issue on deep learning approaches for machine translation☆
177,"Acoustic beamforming can greatly improve the performance of Automatic Speech Recognition(ASR) and speech enhancement systems when multiple channels are available. We recently proposed a way to support the model-based Generalized Eigenvalue beamforming operation with a powerful neural network for spectral mask estimation. The enhancement system has a number of desirable properties. In particular, neither assumptions need to be made about the nature of the acoustic transfer function (e.g., being anechonic), nor does the array configuration need to be known. While the system has been originally developed to enhance speech in noisy environments, we show in this article that it is also effective in suppressing reverberation, thus leading to a generic trainable multi-channel speech enhancement system for robust speech processing. To support this claim, we consider two distinct datasets: The CHiME 3challenge, which features challenging real-world noise distortions, and the Reverbchallenge, which focuses on distortions caused by reverberation. We evaluate the system both with respect to a speech enhancement and a recognition task. For the first task we propose a new way to cope with the distortions introduced by the Generalized Eigenvalue beamformer by renormalizing the target energy for each frequency bin, and measure its effectiveness in terms of the PESQ score. For the latter we feed the enhanced signal to a strong DNN back-end and achieve state-of-the-art ASR results on both datasets. We further experiment with different network architectures for spectral mask estimation: One small feed-forward network with only one hidden layer, one Convolutional Neural Network and one bi-directional Long Short-Term Memory network, showing that even a small network is capable of delivering significant performance improvements.","['Jahn Heymann', 'Lukas Drude', 'Reinhold Haeb-Umbach']",November 2017,Computer Speech & Language,"['Robust speech recognition', 'Acoustic beamforming', 'Multi-channel speech enhancement', 'Deep neural network']",A generic neural acoustic beamforming architecture for robust multi-channel speech processing☆
178,,"['Jon Barker', 'Ricard Marxer', 'Emmanuel Vincent', 'Shinji Watanabe']",November 2017,Computer Speech & Language,[],EditorialMulti-microphone speech recognition in everyday environments
179,"Speech recognition in adverse real-world environments is highly affected by reverberation and non-stationary background noise. A well-known strategy to reduce such undesired signal components in multi-microphone scenarios is spatial filtering of the microphone signals. In this article, we demonstrate that an additional coherence-based postfilter, which is applied to the beamformer output signal to remove diffuse interference components from the latter, is an effective means to further improve the recognition accuracy of modern deep learning speech recognition systems. To this end, the 3rd CHiME Speech Separation and Recognition Challenge (CHiME-3) baseline speech enhancement system is extended by a coherence-based postfilter and the postfilter’s impact on the Word Error Rates (WERs) of a state-of-the-art automatic speech recognition system is investigated for the realistic noisy environments provided by CHiME-3. To determine the time- and frequency-dependent postfilter gains, we use Direction-of-Arrival (DOA)-dependent and (DOA)-independent estimators of the coherent-to-diffuse power ratio as an approximation of the short-time signal-to-noise ratio. Our experiments show that incorporating coherence-based postfiltering into the CHiME-3 baseline speech enhancement system leads to a significant reduction of the WERs, with relative improvements of up to 11.31%.","['Hendrik Barfuss', 'Christian Huemmer', 'Andreas Schwarz', 'Walter Kellermann']",November 2017,Computer Speech & Language,"['Robust speech recognition', 'Postfiltering', 'Spectral enhancement', 'Coherence-to-diffuse power ratio', 'Wiener filter']",Robust coherence-based spectral enhancement for speech recognition in adverse real-world environments☆
180,"This paper gives an in-depth presentation of the multi-microphone speech recognition system we submitted to the 3rd CHiME speech separation and recognition challenge (CHiME-3) and its extension. The proposed system takes advantage of recurrent neural networks (RNNs) throughout the model from the front-end speech enhancement to the language modeling. Three different types of beamforming are used to combine multi-microphone signals to obtain a single higher-quality signal. The beamformed signal is further processed by a single-channel long short-term memory (LSTM) enhancement network, which is used to extract stacked mel-frequency cepstral coefficients (MFCC) features. In addition, the beamformed signal is processed by two proposed noise-robust feature extraction methods. All features are used for decoding in speech recognition systems with deep neural network (DNN) based acoustic models and large-scale RNN language models to achieve high recognition accuracy in noisy environments. Our training methodology includes multi-channel noisy data training and speaker adaptive training, whereas at test time model combination is used to improve generalization. Results on the CHiME-3 benchmark show that the full set of techniques substantially reduced the word error rate (WER). Combining hypotheses from different beamforming and robust-feature systems ultimately achieved 5.05% WER for the real-test data, an 84.7% reduction relative to the baseline of 32.99% WER and a 44.5% reduction from our official CHiME-3 challenge result of 9.1% WER. Furthermore, this final result is better than the best result (5.8% WER) reported in the CHiME-3 challenge.","['Takaaki Hori', 'Zhuo Chen', 'Hakan Erdogan', 'John R. Hershey', 'Jonathan Le Roux', 'Vikramjit Mitra', 'Shinji Watanabe']",November 2017,Computer Speech & Language,"['CHiME-3', 'Robust speech recognition', 'Beamforming', 'Noise robust feature', 'System combination,']","Multi-microphone speech recognition integrating beamforming, robust feature extraction, and advanced DNN/RNN backend☆"
181,"The paper focuses on the design of a practical system pipeline for always-listening, far-field spoken command recognition in everyday smart indoor environments that consist of multiple rooms equipped with sparsely distributed microphone arrays. Such environments, for example domestic and multi-room offices, present challenging acoustic scenes to state-of-the-art speech recognizers, especially under always-listening operation, due to low signal-to-noise ratios, frequent overlaps of target speech, acoustic events, and background noise, as well as inter-room interference and reverberation. In addition, recognition of target commands often needs to be accompanied by their spatial localization, at least at the room level, to account for users in different rooms, providing command disambiguation and room-localized feedback. To address the above requirements, the use of parallel recognition pipelines is proposed, one per room of interest. The approach is enabled by a room-dependent speech activity detection module that employs appropriate multichannel features to determine speech segments and their room of origin, feeding them to the corresponding room-dependent pipelines for further processing. These consist of the traditional cascade of far-field spoken command detection and recognition, the former based on the detection of “activating” key-phrases. Robustness to the challenging environments is pursued by a number of multichannel combination and acoustic modeling techniques, thoroughly investigated in the paper. In particular, channel selection, beamforming, and decision fusion of single-channel results are considered, with the latter performing best. Additional gains are observed, when the employed acoustic models are trained on appropriately simulated reverberant and noisy speech data, and are channel-adapted to the target environments. Further issues investigated concern the inter-dependencies of the various system components, demonstrating the superiority of joint optimization of the component tunable parameters over their separate or sequential optimization. The proposed approach is developed for the Greek language, exhibiting promising performance in real recordings in a four-room apartment, as well as a two-room office. For example, in the latter, a 76.6% command recognition accuracy is achieved on a speaker-independent test, employing a 180-sentence decoding grammar. This result represents a 46% relative improvement over conventional beamforming.","['Isidoros Rodomagoulakis', 'Athanasios Katsamanis', 'Gerasimos Potamianos', 'Panagiotis Giannoulis', 'Antigoni Tsiami', 'Petros Maragos']",November 2017,Computer Speech & Language,"['Smart homes', 'Distant speech recognition', 'Speech activity detection', 'Keyword spotting', 'Multichannel processing', 'Decision fusion', 'Beamforming', 'Channel selection']","Room-localized spoken command recognition in multi-room, multi-microphone environments☆☆☆"
182,"Robustness to reverberation is a key concern for distant-microphone ASR. Various approaches have been proposed, including single-channel or multichannel dereverberation, robust feature extraction, alternative acoustic models, and acoustic model adaptation. However, to the best of our knowledge, a detailed study of these techniques in varied reverberation conditions is still missing in the literature. In this paper, we conduct a series of experiments to assess the impact of various dereverberation and acoustic model adaptation approaches on the ASR performance in the range of reverberation conditions found in real domestic environments. We consider both established approaches such as WPE and newer approaches such as learning hidden unit contribution (LHUC) adaptations, whose performance has not been reported before in this context, and we employ them in combination. Our results indicate that performing weighted prediction error (WPE) dereverberation on a reverberated test speech utterance and decoding using a deep neural network (DNN) acoustic model trained with multi-condition reverberated speech with feature-space maximum likelihood linear regression (fMLLR) transformed features, outperforms more recent approaches and helps significantly reduce the word error rate (WER).","['Sunit Sivasankaran', 'Emmanuel Vincent', 'Irina Illina']",November 2017,Computer Speech & Language,"['Robust ASR', 'Dereverberation', 'Acoustic model adaptation', 'Evaluation']",A combined evaluation of established and new approaches for speech recognition in varied reverberation conditions☆
183,"Our goal for this study is to enable the development of discrete deep neural networks (NNs), some parameters of which are discretized, as small-footprint and fast NNs for acoustic models. Three essential requirements should be met for achieving this goal; 1) the reduction in discretization errors, 2) implementation for fast processing and 3) node-size reduction of DNNs. We propose a weight-parameter model and its training algorithm for 1), an implementation scheme using a look-up table on general-purpose CPUs for 2), and a layer-biased node-pruning method for 3). The first proposed method can set proper boundaries of discretization at each NN node, resulting in reduction in discretization errors. The second method can reduce the memory usage of NNs within the cache size of the CPU by encoding the parameters of NNs. The last method can reduce the network size of the quantized DNNs by measuring the activity of each node at each layer and pruning them with a layer-dependent score. Experiments with 2-bit discrete NNs showed that our training algorithm maintained almost the same word accuracy as with 8-bit discrete NNs. We achieved a 95% reduction of memory usage and a 74% increase in speed of an NN’s forward calculation.","['Ryu Takeda', 'Kazuhiro Nakadai', 'Kazunori Komatani']",November 2017,Computer Speech & Language,"['Deep neural network', 'Acoustic model', 'Small-footprint', 'Quantization', 'Node-pruning']",Acoustic model training based on node-wise weight boundary model for fast and small-footprint deep neural networks☆☆☆
184,"We propose a multi-style learning (multi-style training + deep learning) procedure that relies on deep denoising autoencoders (DAEs) to extract and organize the most discriminative information in a training database. Traditionally, multi-style training procedures require either collecting or artificially creating data samples (e.g., by noise injection or data combination) and training a deep neural network (DNN) with all of these different conditions. To expand the applicability of deep learning, the present study instead adopts a DAE to augment the original training set. First, a DAE is utilized to synthesize data that captures useful structure in the input distribution. Next, this synthetic data is combined and mixed within the original training set to exploit the powerful capabilities of DNN classifiers to learn the complex decision boundaries in heterogeneous conditions. By assigning a DAE to synthesize additional examples of representative variations, multi-style learning makes class boundaries less sensitive to corruptions by enforcing back-end DNNs to emphasize on the most discriminative patterns. Moreover, this deep learning technique mitigates the cost and time of data collection and is easy to incorporate into the internet of things (IoT). Results showed these data-mixed DNNs provided consistent performance improvements without even requiring any preprocessing on the test sets.","['Payton Lin', 'Dau-Cheng Lyu', 'Fei Chen', 'Syu-Siang Wang', 'Yu Tsao']",November 2017,Computer Speech & Language,"['Deep learning', 'Deep neural networks', 'Multi-style training', 'Deep denoising autoencoders', 'Mixed training', 'Representation learning', 'Data combination', 'Data synthesis', 'Noise injection theory', 'Feature compensation', 'Automatic speech recognition', 'Internet of things (IoT)']",Multi-style learning with denoising autoencoders for acoustic modeling in the internet of things (IoT)☆
185,"Because speech recorded by distant microphones in real-world environments is contaminated by both additive noise and reverberation, the automatic speech recognition (ASR) performance is seriously degraded due to the mismatch between the training and testing environments. In the previous studies, some of the authors proposed a Bayesian feature enhancement (BFE) method with re-estimation of reverberation filter parameters for reverberant speech recognition and a BFE method employing independent vector analysis (IVA) to deal with speech corrupted by additive noise. Although both of them accomplish significant improvements in either reverberation- or noise-robust ASR, most of the real-world environments involve both additive noise and reverberation. For robust ASR in the noisy reverberant environments, in this paper, we present a hidden-Markov-model (HMM)-based BFE method using IVA and reverberation parameter re-estimation (RPR) to remove additive and reverberant distortion components in speech acquired by multi-microphones effectively by introducing Bayesian inference in the observation model of input speech features. Experimental results show that the presented method can further reduce the word error rates (WERs) compared with the BFE methods based on conventional noise and/or reverberation models and combinations of the BFE methods for reverberation- or noise-robust ASR.","['Ji-Won Cho', 'Jong-Hyeon Park', 'Joon-Hyuk Chang', 'Hyung-Min Park']",November 2017,Computer Speech & Language,"['Robust speech recognition', 'Feature enhancement', 'Bayesian inference', 'Independent vector analysis', 'Reverberation', 'Hidden Markov model']",Bayesian feature enhancement using independent vector analysis and reverberation parameter re-estimation for noisy reverberant speech recognition
186,"We present an information fusion approach to the robust recognition of multi-microphone speech. It is based on a deep learning framework with a large deep neural network (DNN) consisting of subnets designed from different perspectives. Multiple knowledge sources are then reasonably integrated via an early fusion of normalized noisy features with multiple beamforming techniques, enhanced speech features, speaker-related features, and other auxiliary features concatenated as the input to each subnet to compensate for imperfect front-end processing. Furthermore, a late fusion strategy is utilized to leverage the complementary natures of the different subnets by combining the outputs of all subnets to produce a single output set. Testing on the CHiME-3 task of recognizing microphone array speech, we demonstrate in our empirical study that the different information sources complement each other and that both early and late fusions provide significant performance gains, with an overall word error rate of 10.55% when combining 12 systems. Furthermore, by utilizing an improved technique for beamforming and a powerful recurrent neural network (RNN)-based language model for rescoring, a WER of 9.08% can be achieved for the best single DNN system with one-pass decoding among all of the systems submitted to the CHiME-3 challenge.","['Yan-Hui Tu', 'Jun Du', 'Qing Wang', 'Xiao Bao', 'Li-Rong Dai', 'Chin-Hui Lee']",November 2017,Computer Speech & Language,"['CHiME challenge', 'Deep learning', 'Information fusion', 'Microphone array', 'Robust speech recognition']",An information fusion framework with multi-channel feature concatenation and multi-perspective system combination for the deep-learning-based robust recognition of microphone array speech
187,"Speech enhancement and automatic speech recognition (ASR) are most often evaluated in matched (or multi-condition) settings where the acoustic conditions of the training data match (or cover) those of the test data. Few studies have systematically assessed the impact of acoustic mismatches between training and test data, especially concerning recent speech enhancement and state-of-the-art ASR techniques. In this article, we study this issue in the context of the CHiME-3 dataset, which consists of sentences spoken by talkers situated in challenging noisy environments recorded using a 6-channel tablet based microphone array. We provide a critical analysis of the results published on this dataset for various signal enhancement, feature extraction, and ASR backend techniques and perform a number of new experiments in order to separately assess the impact of different noise environments, different numbers and positions of microphones, or simulated vs. real data on speech enhancement and ASR performance. We show that, with the exception of minimum variance distortionless response (MVDR) beamforming, most algorithms perform consistently on real and simulated data and can benefit from training on simulated data. We also find that training on different noise environments and different microphones barely affects the ASR performance, especially when several environments are present in the training data: only the number of microphones has a significant impact. Based on these results, we introduce the CHiME-4 Speech Separation and Recognition Challenge, which revisits the CHiME-3 dataset and makes it more challenging by reducing the number of microphones available for testing.","['Emmanuel Vincent', 'Shinji Watanabe', 'Aditya Arie Nugraha', 'Jon Barker', 'Ricard Marxer']",November 2017,Computer Speech & Language,"['Robust ASR', 'Speech enhancement', 'Train/test mismatch', 'Microphone array']","An analysis of environment, microphone and data simulation mismatches in robust speech recognition"
188,"The paper describes a system for automatic speech recognition (ASR) that is benchmarked with data of the 3rd CHiME challenge, a dataset comprising distant microphone recordings of noisy acoustic scenes in public environments. The proposed ASR system employs various methods to increase recognition accuracy and noise robustness. Two different multi-channel speech enhancement techniques are used to eliminate interfering sounds in the audio stream. One speech enhancement method aims at separating the target speaker's voice from background sources based on non-negative matrix factorization (NMF) using variational Bayesian (VB) inference to estimate NMF parameters. The second technique is based on a time-varying minimum variance distortionless response (MVDR) beamformer that uses spatial information to suppress sound signals not arriving from a desired direction. Prior to speech enhancement, a microphone channel failure detector is applied that is based on cross-comparing channels using a modulation-spectral representation of the speech signal. ASR feature extraction employs the amplitude modulation filter bank (AMFB) that implicates prior information of speech to analyze its temporal dynamics. AMFBs outperform the commonly used frame splicing technique of filter bank features in conjunction with a deep neural network (DNN) based ASR system, which denotes an equivalent data-driven approach to extract modulation-spectral information. In addition, features are speaker adapted, a recurrent neural network (RNN) is employed for language modeling, and hypotheses of different ASR systems are combined to further enhance the recognition accuracy. The proposed ASR system achieves an absolute word error rate (WER) of 5.67% on the real evaluation test data, which is 0.16% lower compared to the best score reported within the 3rd CHiME challenge.","['Niko Moritz', 'Kamil Adiloğlu', 'Jörn Anemüller', 'Stefan Goetze', 'Birger Kollmeier']",November 2017,Computer Speech & Language,"['Speech enhancement', 'Non-negative matrix factorization', 'Feature extraction', 'Modulation frequency analysis', 'CHiME', 'Amplitude modulation filter bank']",Multi-Channel Speech Enhancement and Amplitude Modulation Analysis for Noise Robust Automatic Speech Recognition
189,"Automatic speech recognition in everyday environments must be robust to significant levels of reverberation and noise. One strategy to achieve such robustness is multi-microphone speech enhancement. In this study, we present results of an evaluation of different speech enhancement pipelines using a state-of-the-art ASR system for a wide range of reverberation and noise conditions. The evaluation exploits the recently released ACE Challenge database which includes measured multichannel acoustic impulse responses from 7 different rooms with reverberation times ranging from 0.33 to 1.34 s. The reverberant speech is mixed with ambient, fan and babble noise recordings made with the same microphone setups in each of the rooms. In the first experiment, performance of the ASR without speech processing is evaluated. Results clearly indicate the deleterious effect of both noise and reverberation. In the second experiment, different speech enhancement pipelines are evaluated with relative word error rate reductions of up to 82%. Finally, the ability of selected instrumental metrics to predict ASR performance improvement is assessed. The best performing metric, Short-Time Objective Intelligibility Measure, is shown to have a Pearson correlation coefficient of 0.79, suggesting that it is a useful predictor of algorithm performance in these tests.","['A.H. Moore', 'P. Peso Parada', 'P.A. Naylor']",November 2017,Computer Speech & Language,"['Automatic speech recognition', 'Speech enhancement', 'Dereverberation', 'Beamforming', 'Realistic environments', 'Microphone array signal processing']",Speech enhancement for robust automatic speech recognition: Evaluation using a baseline system and instrumental measures
190,"In this paper we propose to exploit the automatic Quality Estimation (QE) of ASR hypotheses to perform the unsupervised adaptation of a deep neural network modeling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i) automatically transcribing the evaluation data we are currently trying to recognise, and ii) selecting from it a subset of “good quality” instances based on the word error rate (WER) scores predicted by a QE component. To validate this hypothesis, we run several experiments on the evaluation data sets released for the CHiME-3 challenge. First, we operate in oracle conditions in which manual transcriptions of the evaluation data are available, thus allowing us to compute the true sentence WER. In this scenario, we perform the adaptation with variable amounts of data, which are characterised by different levels of quality. Then, we move to realistic conditions in which the manual transcriptions of the evaluation data are not available. In this case, the adaptation is performed on data selected according to the WER scores predicted by a QE component. Our results indicate that: i) QE predictions allow us to closely approximate the adaptation results obtained in oracle conditions, and ii) the overall ASR performance based on the proposed QE-driven adaptation method is significantly better than the strong, most recent, CHiME-3 baseline.","['Daniele Falavigna', 'Marco Matassoni', 'Shahab Jalalvand', 'Matteo Negri', 'Marco Turchi']",November 2017,Computer Speech & Language,"['Deep neural networks', 'DNN adaptation', 'ASR quality estimation']",DNN adaptation by automatic quality estimation of ASR hypotheses
191,"This paper presents the design and outcomes of the CHiME-3 challenge, the first open speech recognition evaluation designed to target the increasingly relevant multichannel, mobile-device speech recognition scenario. The paper serves two purposes. First, it provides a definitive reference for the challenge, including full descriptions of the task design, data capture and baseline systems along with a description and evaluation of the 26 systems that were submitted. The best systems re-engineered every stage of the baseline resulting in reductions in word error rate from 33.4% to as low as 5.8%. By comparing across systems, techniques that are essential for strong performance are identified. Second, the paper considers the problem of drawing conclusions from evaluations that use speech directly recorded in noisy environments. The degree of challenge presented by the resulting material is hard to control and hard to fully characterise. We attempt to dissect the various ‘axes of difficulty’ by correlating various estimated signal properties with typical system performance on a per session and per utterance basis. We find strong evidence of a dependence on signal-to-noise ratio and channel quality. Systems are less sensitive to variations in the degree of speaker motion. The paper concludes by discussing the outcomes of CHiME-3 in relation to the design of future mobile speech recognition evaluations.","['Jon Barker', 'Ricard Marxer', 'Emmanuel Vincent', 'Shinji Watanabe']",November 2017,Computer Speech & Language,"['Noise-robust ASR', 'Microphone array', '‘CHiME’ challenge']",The third ‘CHiME’ speech separation and recognition challenge: Analysis and outcomes
192,"Speaker diarization is a problem of separating unknown speakers in a conversation into homogeneous parts in the speaker sense. State-of-the-art diarization systems are based on i-vector methodologies. However, these approaches require large quantities of training data, which must be obtained from an environment that is similar to that of the conversation being diarized. In this paper we present a diarization system that does not require such training data but instead can suffice with some development data for parameter-tuning. This system is a generalization of the well-known hidden Markov model (HMM), a popular clustering algorithm trained by Viterbi statistics. Our proposed model, referred to as a hidden distortion model (HDM), is based on state distortion models and transition costs, for which probabilistic calculations are not mandatory, in contrast to the case of HMM. We provide a mathematical basis for our approach, and we demonstrate that Viterbi-based HMM can be seen as a special case of HDM. This proximity allows us to apply similar approaches for state-model training when the new paradigm is used to learn sequence dependencies.We carry out diarizations of two-speaker telephone conversations in order to evaluate the performance of HDM. When applied to conversations from the LDC CALLHOME database, HDM improves on the performance of a baseline HMM system by about 26% (relative improvement). Moreover, when applied to the NIST 2005 database, it yields a small improvement over the HMM system.","['Itshak Lapidot', 'Alon Shoa', 'Tal Furmanov', 'Lidiya Aminov', 'Ami Moyal', 'Jean-François Bonastre']",September 2017,Computer Speech & Language,"['Time-series clustering', 'Hidden Markov model (HMM)', 'Hidden-distortion-model (HDM)', 'Speaker diarization']",Generalized Viterbi-based models for time-series segmentation and clustering applied to speaker diarization
193,"Previous studies support the idea of merging auditory-based Gabor features with deep learning architectures to achieve robust automatic speech recognition, however, the cause behind the gain of such combination is still unknown. We believe these representations provide the deep learning decoder with more discriminable cues. Our aim with this paper is to validate this hypothesis by performing experiments with three different recognition tasks (Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the information encoded by Gabor filterbank features. Additionally, to identify the contribution of low, medium and high temporal modulation frequencies subsets of the Gabor filterbank were used as features (dubbed LTM, MTM and HTM, respectively). With temporal modulation frequencies between 16 and 25 Hz, HTM consistently outperformed the remaining ones in every condition, highlighting the robustness of these representations against channel distortions, low signal-to-noise ratios and acoustically challenging real-life scenarios with relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To explain the results, a measure of similarity between phoneme classes from DNN activations is proposed and linked to their acoustic properties. We find this measure to be consistent with the observed error rates and highlight specific differences on phoneme level to pinpoint the benefit of the proposed features.","['Angel Mario Castro Martinez', 'Sri Harish Mallidi', 'Bernd T. Meyer']",September 2017,Computer Speech & Language,"['Auditory features', 'Spectro-temporal processing', 'Deep neural networks', 'Automatic speech recognition']",On the relevance of auditory-based Gabor features for deep learning in robust speech recognition☆
194,"This paper presents a novel methodology to characterize the style of different speakers or groups of speakers. This methodology uses sequences of prosodic labels (automatic Sp_ToBI labels) to compare and differentiate these speaking styles. A set of metrics based on conditional entropy is used to compute the distance between two speakers or group of speakers depending on the use of sequences of prosodic labels. Additionally, the most contrastive sequences of labels are identified as characteristic patterns of the speaking styles represented in a given corpus. When this methodology is applied to a corpus of radio news items, the result is that the most frequent prosodic patterns coincide with those previously characterized in studies about radio style. Finally, a perceptual test verifies that the participants attribute these characteristic patterns to the radio news style.","['David Escudero', 'César González', 'Yurena Gutiérrez', 'Emma Rodero']",September 2017,Computer Speech & Language,"['Comparing prosody', 'Prosodic labeling', 'Radio news style', 'Informational distance', 'Entropy analysis', 'Sp_ToBI']",Identifying characteristic prosodic patterns through the analysis of the information of Sp_ToBI label sequences
195,"Query-by-Example approach of spoken content retrieval has gained much attention because of its feasibility in the absence of speech recognition and its applicability in a multilingual matching scenario. This approach to retrieve spoken content is referred to as Query-by-Example Spoken Term Detection (QbE-STD). The state-of-the-art QbE-STD system performs matching between the frame sequence of query and test utterance via Dynamic Time Warping (DTW) algorithm. In realistic scenarios, there is a need to retrieve the query which does not appear exactly in the spoken document. However, the appeared instance of query might have the different suffix, prefix or word order. The DTW algorithm monotonically aligns the two sequences and hence, it is not suitable to perform partial matching between the frame sequence of query and test utterance. In this paper, we propose novel partial matching approach between spoken query and utterance using modified DTW algorithm where multiple warping paths are constructed for each query and test utterance pair. Next, we address the research issue associated with search complexity of DTW and suggest two approaches, namely, feature reduction approach and Bag-of-Acoustic-Words (BoAW) model. In feature reduction approach, the number of feature vectors is reduced by averaging across the consecutive frames within phonetic boundaries. Thus, a lesser number of feature vectors require fewer number of comparisons and hence, DTW speeds up the search computation. The search computation time gets reduced by 46–49% with a slight degradation in performance as compared to no feature reduction case. In BoAW model, we construct term frequency-inverse document frequency (tf−idf) vectors at segment-level to retrieve audio documents. The proposed segment-level BoAW model is used to match test utterance with a query using (tf−idf) vectors and the scores obtained are used to rank the test utterance. The BoAW model gave more than 80% recall value on 70% top retrieval. To re-score the detection, we further employ DTW search or modified DTW search to retrieve the spoken query from the selected utterances using BoAW model. QbE-STD experiments are conducted on different international benchmarks, namely, MediaEval spoken web search SWS 2013 and MediaEval query-by-example search on speech QUESST 2014.","['Maulik C. Madhavi', 'Hemant A. Patil']",September 2017,Computer Speech & Language,"['Query-by-Example Spoken Term Detection', 'Dynamic time warping', 'Non-exact DTW matching', 'Phonetic posteriorgrams', 'Search space reduction', 'Phonetic segmentation', 'Bag-of-Acoustic-Word model']",Partial matching and search space reduction for QbE-STD☆
196,"Although i-vectors together with probabilistic LDA (PLDA) have achieved a great success in speaker verification, how to suppress the undesirable effects caused by the variability in utterance length and background noise level is still a challenge. This paper aims to improve the robustness of i-vector based speaker verification systems by compensating for the utterance-length variability and noise-level variability. Inspired by the recent findings that noise-level variability can be modeled by a signal-to-noise ratio (SNR) subspace and that duration variability can be modeled as additive noise in the i-vector space, we propose to add an SNR factor and a duration factor to the PLDA model. In this framework, we assume that i-vectors derived from utterances with comparable durations share similar duration-specific information and that i-vectors extracted from utterances within a narrow SNR range have similar SNR-specific information. Based on these assumptions, an i-vector can be represented as a linear combination of four components: speaker, SNR, duration, and channel. A variational Bayes algorithm is developed to infer this latent variable model via a discriminative subspace training procedure. In the testing stage, different variabilities are compensated for when computing the likelihood ratio. Experiments on Common Conditions 1 and 4 in NIST 2012 SRE show that the proposed model outperforms the conventional PLDA and SNR-invariant PLDA. Results also show that the proposed model performs better than the uncertainty-propagation PLDA (UP-PLDA) for long test utterances.","['Na Li', 'Man-Wai Mak', 'Wei-Wei Lin', 'Jen-Tzung Chien']",September 2017,Computer Speech & Language,"['Speaker verification', 'Duration variation', 'SNR mismatch', 'Variational Bayes', 'I-vector', 'PLDA']",Discriminative subspace modeling of SNR and duration variabilities for robust speaker verification☆
197,"Once the i-vector paradigm has been introduced in the field of speaker recognition, many techniques have been proposed to deal with additive noise within this framework. Due to the complexity of its effect in the i-vector space, a lot of effort has been put into dealing with noise in other domains (speech enhancement, feature compensation, robust i-vector extraction and robust scoring). As far as we know, there was no serious attempt to handle the noise problem directly in the i-vector space without relying on data distributions computed on a prior domain. The aim of this paper is twofold. First, it proposes a full-covariance Gaussian modeling of the clean i-vectors and noise distribution in the i-vector space and introduces a technique to estimate a clean i-vector given the noisy version and the noise density function using the MAP approach. Based on NIST data, we show that it is possible to improve by up to 60% the baseline system performance. Second, in order to make this algorithm usable in a real application and reduce the computational time needed by i-MAP, we propose an extension that requires building a noise distribution database in the i-vector space in an off-line step and using it later in the test phase. We show that it is possible to achieve comparable results using this approach (up to 57% of relative EER improvement) with a sufficiently large noise distribution database.","['Waad Ben Kheder', 'Driss Matrouf', 'Pierre-Michel Bousquet', 'Jean-François Bonastre', 'Moez Ajili']",September 2017,Computer Speech & Language,"['i-vectors', 'MAP adaptation', 'Speaker recognition', 'Additive noise']",Fast i-vector denoising using MAP estimation and a noise distributions database for robust speaker recognition
198,"Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in text-to-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.","['Antti Suni', 'Juraj Šimko', 'Daniel Aalto', 'Martti Vainio']",September 2017,Computer Speech & Language,"['Phonetics', 'Prosody', 'Speech synthesis', 'Wavelets']",Hierarchical representation and estimation of prosody using continuous wavelet transform
199,"Recent advances in end-to-end neural machine translation models have achieved promising results on high-resource language pairs such as En→ Fr and En→ De. One of the major factor behind these successes is the availability of high quality parallel corpora. We explore two strategies on leveraging abundant amount of monolingual data for neural machine translation. We observe improvements by both combining scores from neural language model trained only on target monolingual data with neural machine translation model and fusing hidden-states of these two models. We obtain up to 2 BLEU improvement over hierarchical and phrase-based baseline on low-resource language pair, Turkish→ English. Our method was initially motivated towards tasks with less parallel data, but we also show that it extends to high resource languages such as Cs→ En and De→ En translation tasks, where we obtain 0.39 and 0.47 BLEU improvements over the neural machine translation baselines, respectively.","['Caglar Gulcehre', 'Orhan Firat', 'Kelvin Xu', 'Kyunghyun Cho', 'Yoshua Bengio']",September 2017,Computer Speech & Language,"['Neural machine translation', 'Monolingual data', 'Language models', 'Low resource machine translation', 'Deep learning', 'Neural network']",On integrating a language model into neural machine translation☆
200,"We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En–Fr and En–De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.","['Heeyoul Choi', 'Kyunghyun Cho', 'Yoshua Bengio']",September 2017,Computer Speech & Language,"['Neural machine translation', 'Contextualization', 'Symbolization']",Context-dependent word representation for neural machine translation☆
201,"We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i) we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et al., 2014); (ii) we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data.We evaluated our models on the standard task of translating English-to-German and Arabic-to-English TED talks. The NDAM models achieved better perplexities and modest BLEU improvements compared to the baseline NNJM, trained either on in-domain or on a concatenation of in- and out-domain data. On the other hand, the NFM models obtained significant improvements of up to +0.9 and +0.7 BLEU points, respectively. We also demonstrate improvements over existing adaptation methods such as instance weighting, phrasetable fill-up, linear and log-linear interpolations.","['Shafiq Joty', 'Nadir Durrani', 'Hassan Sajjad', 'Ahmed Abdelali']",September 2017,Computer Speech & Language,"['Machine translation', 'Domain adaptation', 'Neural network joint model', 'Distributed representation of texts', 'Noise contrastive estimation']",Domain adaptation using neural network joint model
202,"We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the WMT Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an MT evaluation metric that correlates with human judgments, and is on par with the state of the art.","['Francisco Guzmán', 'Shafiq Joty', 'Lluís Màrquez', 'Preslav Nakov']",September 2017,Computer Speech & Language,"['Machine translation', 'Reference-based MT evaluation', 'Deep neural networks', 'Distributed representation of texts', 'Textual similarity']",Machine translation evaluation with neural networks
203,"Despite the promising results achieved in last years by statistical machine translation, and more precisely, by the neural machine translation systems, this technology is still not error-free. The outputs of a machine translation system must be corrected by a human agent in a post-editing phase. Interactive protocols foster a human–computer collaboration, in order to increase productivity. In this work, we integrate the neural machine translation into the interactive machine translation framework. Moreover, we propose new interactivity protocols, in order to provide the user an enhanced experience and a higher productivity. Results obtained over a simulated benchmark show that interactive neural systems can significantly improve the classical phrase-based approach in an interactive-predictive machine translation scenario.","['Álvaro Peris', 'Miguel Domingo', 'Francisco Casacuberta']",September 2017,Computer Speech & Language,"['Neural machine translation', 'Interactive-predictive machine translation', 'Recurrent neural networks']",Interactive neural machine translation
204,"Long sentences with complex syntax and long-distance dependencies pose difficulties for machine translation systems. Short sentences, on the other hand, are usually easier to translate. We study the potential of addressing this mismatch using text simplification: given a simplified version of the full input sentence, can we use it in addition to the full input to improve translation? We show that the spaces of original and simplified translations can be effectively combined using translation lattices and compare two decoding approaches to process both inputs at different levels of integration. We demonstrate on source-annotated portions of WMT test sets and on top of strong baseline systems combining hierarchical and neural translation for two language pairs that source simplification can help to improve translation quality.","['Eva Hasler', 'Adrià de Gispert', 'Felix Stahlberg', 'Aurelien Waite', 'Bill Byrne']",September 2017,Computer Speech & Language,"['Hierarchical machine translation', 'Text simplification', 'Neural machine translation']",Source sentence simplification for statistical machine translation
205,"We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT’15 simultaneously and observe clear performance improvements over models trained on only one language pair. We empirically evaluate the proposed model on low-resource language translation tasks. In particular, we observe that the proposed multilingual model outperforms strong conventional statistical machine translation systems on Turkish-English and Uzbek-English by incorporating the resources of other language pairs.","['Orhan Firat', 'Kyunghyun Cho', 'Baskaran Sankaran', 'Fatos T. Yarman Vural', 'Yoshua Bengio']",September 2017,Computer Speech & Language,"['Neural machine translation', 'Multi-lingual', 'Low resource translation']","Multi-way, multilingual neural machine translation☆"
206,"There have been enormous technical advances in the use of imaging techniques in speech production research in terms of resolution and frame rates. However, a major bottleneck lies in the lack of appropriate data reduction and quantification methods which allow for a parsimonious representation of the high-dimensional image data. Particularly the rapid increase in frame rates seen in data acquisition makes traditional, error-prone methods of contour tracking unwieldy due to the high amount of manual intervention required. We discuss recent developments in methods that obviate contour tracking but instead process the entire image. Specifically, we focus on one such approach by demonstrating the application of Principal Component Analysis to ultrasound images. This method not only exploits the information present in the entire image, but it also straightforwardly allows for the representation of the temporal evolution of an utterance by reducing a series of images to a time-varying single-value representation. In an illustrative example, inspired by the seminal work of Öhman (1966), we show how characteristic patterns of coarticulation between vowels and consonants can be captured in this way.","['Philip Hoole', 'Marianne Pouplier']",September 2017,Computer Speech & Language,"['Articulatory data', 'Imaging techniques', 'Time series analysis', 'Ultrasound', 'Principal component analysis', 'Coarticulation']",Öhman returns: New horizons in the collection and analysis of imaging data in speech production research☆
207,"Methods from automatic speech recognition (ASR), such as segmentation and forced alignment, have facilitated the rapid annotation and analysis of very large adult speech databases and databases of caregiver–infant interaction, enabling advances in speech science that were unimaginable just a few decades ago. This paper centers on two main problems that must be addressed in order to have analogous resources for developing and exploiting databases of young children’s speech. The first problem is to understand and appreciate the differences between adult and child speech that cause ASR models developed for adult speech to fail when applied to child speech. These differences include the fact that children’s vocal tracts are smaller than those of adult males and also changing rapidly in size and shape over the course of development, leading to between-talker variability across age groups that dwarfs the between-talker differences between adult men and women. Moreover, children do not achieve fully adult-like speech motor control until they are young adults, and their vocabularies and phonological proficiency are developing as well, leading to considerably more within-talker variability as well as more between-talker variability. The second problem then is to determine what annotation schemas and analysis techniques can most usefully capture relevant aspects of this variability. Indeed, standard acoustic characterizations applied to child speech reveal that adult-centered annotation schemas fail to capture phenomena such as the emergence of covert contrasts in children’s developing phonological systems, while also revealing children’s nonuniform progression toward community speech norms as they acquire the phonological systems of their native languages. Both problems point to the need for more basic research into the growth and development of the articulatory system (as well as of the lexicon and phonological system) that is oriented explicitly toward the construction of age-appropriate computational models.","['Mary E. Beckman', 'Andrew R. Plummer', 'Benjamin Munson', 'Patrick F. Reidy']",September 2017,Computer Speech & Language,"['Child speech development', 'Phonetic transcription', 'Spectral kinematics', 'Automatic speech recognition', 'Big data corpora']","Methods for eliciting, annotating, and analyzing databases for child speech development"
208,"Much of what is known about prosody is based on native speaker intuitions of idealized speech, or on prosodic annotations from trained annotators whose auditory impressions are augmented by visual evidence from speech waveforms, spectrograms and pitch tracks. Expanding the prosodic data currently available to cover more languages, and to cover a broader range of unscripted speech styles, is prohibitive due to the time, money and human expertise needed for prosodic annotation. We describe an alternative approach to prosodic data collection, with coarse-grained annotations from a cohort of untrained annotators performing rapid prosody transcription (RPT) using LMEDS, an open-source software tool we developed to enable large-scale, crowd-sourced data collection with RPT. Results from three RPT experiments are reported. The reliability of RPT is analysed comparing kappa statistics for lab-based and crowd-sourced annotations for American English, comparing annotators from the same (US) versus different (Indian) dialect groups, and comparing each RPT annotator with a ToBI annotation. Results show better reliability for same-dialect annotators (US), and the best overall reliability from crowd-sourced US annotators, though lab-based annotations are the most similar to ToBI annotations. A generalized additive mixed model is used to test differences among annotator groups in the factors that predict prosodic annotation. Results show that a common set of acoustic and contextual factors predict prosodic labels for all annotator groups, with only small differences among the RPT groups, but with larger effects on prosodic marking for ToBI annotators. The findings suggest methods for optimizing the efficiency of RPT annotations. Overall, crowd-sourced prosodic annotation is shown to be efficient, and to rely on established cues to prosody, supporting its use for prosody research across languages, dialects, speaker populations, and speech genres.","['Jennifer Cole', 'Timothy Mahrt', 'Joseph Roy']",September 2017,Computer Speech & Language,"['Prosody', 'Annotation', 'Crowd-sourcing', 'Generalized mixed effects model', 'Inter-rater reliability', 'Speech transcription']",Crowd-sourcing prosodic annotation☆
209,"A new software paradigm `Software as a Service' based on web services is proposed for multilingual linguistic tools and exemplified with the BAS CLARIN web services. Instead of traditional tool development and distribution the tool functionality is implemented on a highly available server that users or applications access via HTTP requests. As examples we describe in detail five multilingual web services for speech science operational since 2012 and discuss the benefits and drawbacks of the new paradigm as well as our experiences with user acceptance and implementation problems. The services include automatic segmentation of speech, grapheme-to-phoneme conversion, syllabification, speech synthesis, and optimal symbol sequence alignment.","['Thomas Kisler', 'Uwe Reichel', 'Florian Schiel']",September 2017,Computer Speech & Language,"['Web service', 'Speech processing', 'Automatic segmentation', 'Grapheme-to-phoneme', 'Software as a service']",Multilingual processing of speech via web services
210,"Sharing speech corpora and their annotations is desirable, in order to maximise the value gained from the expense and hard work involved in transcribing and annotating them. However, differences in conventions and format are barriers to sharing of data; text conventions conflict, file formats differ, and annotation ontologies do not match up. Using a ‘pivot’ form to store annotations in a tool and format neutral manner can alleviate many of these difficulties. There are several possibilities for the pivot form, including the Annotation Graph model, which meets most of the requirements to be a pivot. The LaBB-CAT software’s implementation of Annotation Graphs incorporates some extensions to the model, which handle the remaining unmet requirements, and create the possibility of defining an annotation API that makes automation of conversion, querying, and manipulation of annotations easier.",['Robert Fromont'],September 2017,Computer Speech & Language,"['Speech corpora', 'Languane annotation', 'Annotation graph', 'Interoperability']",Toward a format-neutral annotation store
211,"Reproducibility is an important part of scientific research and studies published in speech and language research usually make some attempt at ensuring that the work reported could be reproduced by other researchers. This paper looks at the current practice in the field relating to the citation and availability of both data and software methods. It is common to use widely available shared datasets in this field which helps to ensure that studies can be reproduced; however a brief survey of recent papers shows a wide range of styles of citation of data only some of which clearly identify the exact data used in the study. Similarly, practices in describing and sharing software artefacts vary considerably from detailed descriptions of algorithms to linked repositories. The Alveo Virtual Laboratory is a web based platform to support research based on collections of text, speech and video. Alveo provides a central repository for language data and provides a set of services for discovery and analysis of data. We argue that some of the features of the Alveo platform may make it easier for researchers to share their data more precisely and cite the exact software tools used to develop published results. Alveo makes use of ideas developed in other areas of science and we discuss these and how they can be applied to speech and language research.","['Steve Cassidy', 'Dominique Estival']",September 2017,Computer Speech & Language,"['Corpus infrastructure', 'Data citation', 'Reproducibility', 'Research methods', 'eResearch', 'Research workflow']",Supporting accessibility and reproducibility in language research in the Alveo virtual laboratory
212,"The amount and complexity of the often very specialized tools necessary for working with spoken language databases has continually evolved and grown over the years. The speech and spoken language research community is expected to be well versed in multiple software tools and have the ability to switch seamlessly between the various tools, sometimes even having to script ad-hoc solutions to solve interoperability issues. In this paper, we present a set of tools that strive to provide an all-in-one solution for generating, manipulating, querying, analyzing and managing speech databases. The tools presented here are centered around the R language and environment for statistical computing and graphics (R Core Team, 2016), which benefits users by significantly reducing the number of tools the researchers have to familiarize themselves with. This paper introduces the next iteration of the EMU system that, although based on the core concepts of the legacy system, is a newly designed and almost entirely rewritten set of modern spoken language database management tools.","['Raphael Winkelmann', 'Jonathan Harrington', 'Klaus Jänsch']",September 2017,Computer Speech & Language,"['EMU-SDMS', 'emuR', 'wrassp', 'EMU-webApp', 'Speech databases', 'Speech annotation', 'Speech database management']",EMU-SDMS: Advanced speech database management and analysis in R
213,"This paper extends upon a previous work using Mean Shift algorithm to perform speaker clustering on i-vectors generated from short speech segments. In this paper we examine the effectiveness of probabilistic linear discriminant analysis (PLDA) scoring as the metric of the mean shift clustering algorithm in the presence of different numbers of speakers. Our proposed method, combined with k-nearest neighbors (kNN) for bandwidth estimation, yields better and more robust results in comparison to the cosine similarity with fixed neighborhood bandwidth for clustering segments of large numbers of speakers. In the case of 30 speakers, we achieved significant improvement in cluster and speaker purity with the PLDA-based mean shift algorithm compared to the cosine-based baseline system.","['Itay Salmun', 'Ilya Shapiro', 'Irit Opher', 'Itshak Lapidot']",September 2017,Computer Speech & Language,"['Speaker clustering', 'Mean shift clustering', 'Probabilistic linear discriminant analysis', 'Two-covariance model', 'K-nearest neighbors', 'I-vectors', 'Short segments']",PLDA-based mean shift speakers' short segments clustering☆
214,"Whispered speech is a natural speaking style that despite its reduced perceptibility, still contains relevant information regarding the intended message (i.e., intelligibility), as well as the speaker identity and gender. Given the acoustic differences between whispered and normally-phonated speech, however, speech applications trained on the latter but tested with the former exhibit unacceptable performance levels. Within an automated speaker verification task, previous research has shown that i) conventional features (e.g., mel-frequency cepstral coefficients, MFCCs) do not convey sufficient speaker discrimination cues across the two vocal efforts, and ii) multi-condition training, while improving the performance for whispered speech, tends to deteriorate the performance for normal speech. In this paper, we aim to tackle both shortcomings by proposing three innovative features, which when fused at the score level, are shown to result in reliable results for both normal and whispered speech. Overall, relative improvements of 66% and 63% are obtained for whispered and normal speech, respectively, over a baseline system based on MFCCs and multi-condition training.","['Milton Sarria-Paja', 'Tiago H. Falk']",September 2017,Computer Speech & Language,"['Whispered speech', 'Speaker verification', 'Modulation spectrum', 'Mutual information', 'System fusion']",Fusion of auditory inspired amplitude modulation spectrum and cepstral features for whispered and normal speech speaker verification☆
215,"There are many factors affecting the variability of an i-vector extracted from a speech segment such as the acoustic content, segment duration, handset type and background noise. The language being spoken is one of the sources of variation which has received limited focus due to the lack of multilingual resources available. Consequently, the discrimination performance is much lower under multilingual trial condition. Standard session-compensation techniques such as Within-Class Covariance Normalization (WCCN), Linear Discriminant Analysis (LDA) and Probabilistic LDA (PLDA) cannot robustly compensate for language source of variation as the amount of data is limited to represent such variability. Source normalization technique which was developed to compensate for speech-source-variation, offered superior performance in cross-language trials by providing better estimation of within-speaker scatter matrix in WCCN and LDA techniques. However, neither language normalization nor the state-of-the-art PLDA algorithm is capable of modeling language variability on a dataset with insufficient multilingual utterances for each speaker, resulting in a poor performance in cross-language trial condition.This study is an extension to our initial developments of a language-independent PLDA training algorithm which aimed at reducing the effect of language as a source of variability on the performance of speaker recognition. We will provide a thorough analysis of how the proposed approach can utilize multilingual training data from bilingual speakers to robustly compensate for the effect of languages. Evaluated on multilingual trial condition, the proposed solution demonstrated over 10% EER and 13% minimum DCF relative improvement on NIST 2008 speaker recognition evaluation as well as 12.4% EER and 23% minimum DCF on PRISM evaluation set over the baseline system while also providing improvement in other trial conditions.","['Abbas Khosravani', 'Mohammad M. Homayounpour']",September 2017,Computer Speech & Language,"['Speaker recognition', 'PLDA', 'Language mismatch', 'Cross-Language', 'Multilingual', 'NIST SRE']",A PLDA approach for language and text independent speaker recognition☆
216,"Examples are given of forensic voice comparison with higher level features in real-world cases and research. A pilot experiment relating to estimation of strength of evidence in forensic voice comparison is described which explores the use of higher-level features extracted over a disyllabic word as a whole, rather than over individual monosyllables as conventionally practiced. The trajectories of the first three formants and tonal F0 of the hexaphonic disyllabic Cantonese word daihyat ‘first’ from controlled but natural non-contemporaneous recordings of 23 male speakers are modeled with polynomials, and multivariate likelihood ratios estimated from their coefficients. Evaluation with the log likelihood ratio cost validity metric Cllr shows an optimum performance is obtained, surprisingly, with lower order polynomials, with F2 requiring a cubic fit, and F1 and F3 quadratic. Fusion of F-pattern and tonal F0 results in considerable improvement over the individual features, reducing the Cllr to ca. 0.1. The forensic potential of the daihyat data is demonstrated by fusion with three other Cantonese higher-level features: the F-pattern of /i/, short-term F0, and syllabic nasal cepstral spectrum, which reduces the Cllr still further to 0.03. Important pros and cons of higher-level features and likelihood ratios are discussed, the latter illustrated with data from Japanese, and three varieties of English in real forensic casework.",['Phil Rose'],September 2017,Computer Speech & Language,"['Forensic voice comparison', 'Likelihood ratio', 'Cantonese', 'Higher-level features', 'F-pattern trajectories', 'Tonal F0 trajectory', 'Short term F0', 'Segmental cepstrum']",Likelihood ratio-based forensic voice comparison with higher level features: research and reality☆
217,"The i-vector/PLDA framework has gained huge popularity in text-independent speaker verification. This approach, however, lacks the ability to represent the reliability of i-vectors. As a result, the framework performs poorly when presented with utterances of arbitrary duration. To address this problem, a method called uncertainty propagation (UP) was proposed to explicitly model the reliability of an i-vector by an utterance-dependent loading matrix. However, the utterance-dependent matrix greatly complicates the evaluation of likelihood scores. As a result, PLDA with UP, or PLDA-UP in short, is far more computational intensive than the conventional PLDA. In this paper, we propose to group i-vectors with similar reliability, and for each group the utterance-dependent loading matrices are replaced by a representative one. This arrangement allows us to pre-compute a set of representative matrices that cover all possible i-vectors, thereby greatly reducing the computational cost of PLDA-UP while preserving its ability in discriminating the reliability of i-vectors. Experiments on NIST 2012 SRE show that the proposed method can perform as good as the PLDA with UP while the scoring time is only 3.18% of it.","['Wei-wei Lin', 'Man-Wai Mak', 'Jen-Tzung Chien']",September 2017,Computer Speech & Language,"['Speaker verification', 'i-Vector/PLDA', 'Uncertainty Propagation', 'Duration mismatch']",Fast scoring for PLDA with uncertainty propagation via i-vector grouping☆
218,"Recent evaluations such as ASVspoof 2015 and the similarly-named AVspoof have stimulated a great deal of progress to develop spoofing countermeasures for automatic speaker verification. This paper reports an approach which combines speech signal analysis using the constant Q transform with traditional cepstral processing. The resulting constant Q cepstral coefficients (CQCCs) were introduced recently and have proven to be an effective spoofing countermeasure. An extension of previous work, the paper reports an assessment of CQCCs generalisation across three different databases and shows that they deliver state-of-the-art performance in each case. The benefit of CQCC features stems from a variable spectro-temporal resolution which, while being fundamentally different to that used by most automatic speaker verification system front-ends, also captures reliably the tell-tale signs of manipulation artefacts which are indicative of spoofing attacks. The second contribution relates to a cross-database evaluation. Results show that CQCC configuration is sensitive to the general form of spoofing attack and use case scenario. This finding suggests that the past single-system pursuit of generalised spoofing detection may need rethinking.","['Massimiliano Todisco', 'Héctor Delgado', 'Nicholas Evans']",September 2017,Computer Speech & Language,"['Spoofing countermeasures', 'Presentation attack detection', 'Automatic speaker verification', 'Constant Q transform', 'Cepstral analysis']",Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification☆
219,"We hypothesize that conversational implicatures are a rich source of clarification requests, and in this paper we do two things. First, we motivate the hypothesis in theoretical, practical and empirical terms and formulate it as a concrete clarification potential principle: implicatures may become explicit as fourth-level clarification requests. Second, we present a framework for generating the clarification potential of an instruction by inferring its conversational implicatures with respect to a particular context. We evaluate the framework and illustrate its performance using a human–human corpus of situated conversations. Much of the inference required can be handled using classical planning, though as we shall note, other forms of means-ends analysis are also required. Our framework leads us to view discourse structure as emerging via opportunistic responses to task structure.","['Luciana Benotti', 'Patrick Blackburn']",September 2017,Computer Speech & Language,"['Clarification requests', 'Level-sensitive Gabsdil test', 'Conversational implicatures', 'Dialogue systems', 'Classical planning', 'Micro-planning', 'Negotiability', 'Tacit acts', 'Explicatures', 'Emergent structure', 'Opportunistic theories of communication']",Modeling the clarification potential of instructions: Predicting clarification requests and other reactions
220,"Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or out-perform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.","['Milica Gašić', 'Nikola Mrkšić', 'Lina M. Rojas-Barahona', 'Pei-Hao Su', 'Stefan Ultes', 'David Vandyke', 'Tsung-Hsien Wen', 'Steve Young']",September 2017,Computer Speech & Language,"['Dialogue systems', 'Reinforcement learning', 'Gaussian process']",Dialogue manager domain adaptation using Gaussian process reinforcement learning
221,"Spoken term detection (STD), the process of finding all occurrences of a specified search term in a large amount of speech segments, has many applications in multimedia search and retrieval of information. It is known that use of video information in the form of lip movements can improve the performance of STD in the presence of audio noise. However, research in this direction has been hampered by the unavailability of large annotated audio visual databases for development. We propose a novel approach to develop audio visual spoken term detection when only a small (low resource) audio visual database is available for development. First, cross database training is proposed as a novel framework using the fused hidden Markov modeling (HMM) technique, which is used to train an audio model using extensive large and publicly available audio databases; then it is adapted to the visual data of the given audio visual database. This approach is shown to perform better than standard HMM joint-training method and also improves the performance of spoken term detection when used in the indexing stage. In another attempt, the external audio models are first adapted to the audio data of the given audio visual database and then they are adapted to the visual data. This approach also improves both phone recognition and spoken term detection accuracy. Finally, the cross database training technique is used as HMM initialization, and an extra parameter re-estimation step is applied on the initialized models using Baum Welch technique. The proposed approaches for audio visual model training have allowed for benefiting from both large extensive out of domain audio databases that are available and the small audio visual database that is given for development to create more accurate audio-visual models.","['Shahram Kalantari', 'David Dean', 'Sridha Sridharan']",July 2017,Computer Speech & Language,"['Spoken term detection', 'Synchronous hidden Markov model', 'Cross-database training', 'Phone recognition']",Cross database audio visual speech adaptation for phonetic spoken term detection
222,"Sparse coding, as a successful representation method for many signals, has been recently employed in speech enhancement. This paper presents a new learning-based speech enhancement algorithm via sparse representation in the wavelet packet transform domain. We propose sparse dictionary learning procedures for training data of speech and noise signals based on a coherence criterion, for each subband of decomposition level. Using these learning algorithms, self-coherence between atoms of each dictionary and mutual coherence between speech and noise dictionary atoms are minimized along with the approximation error. The speech enhancement algorithm is introduced in two scenarios, supervised and semi-supervised. In each scenario, a voice activity detector scheme is employed based on the energy of sparse coefficient matrices when the observation data is coded over corresponding dictionaries. In the proposed supervised scenario, we take advantage of domain adaptation techniques to transform a learned noise dictionary to a dictionary adapted to noise conditions captured based on the test environment circumstances. Using this step, observation data is sparsely coded, based on the current situation of the noisy space, with low sparse approximation error. This technique has a prominent role in obtaining better enhancement results particularly when the noise is non-stationary. In the proposed semi-supervised scenario, adaptive thresholding of wavelet coefficients is carried out based on the variance of the estimated noise in each frame of different subbands. The proposed approaches lead to significantly better speech enhancement results in comparison with the earlier methods in this context and the traditional procedures, based on different objective and subjective measures as well as a statistical test.","['Samira Mavaddaty', 'Seyed Mohammad Ahadi', 'Sanaz Seyedin']",July 2017,Computer Speech & Language,"['Speech enhancement', 'Dictionary learning', 'Sparse representation', 'Domain adaptation', 'Voice activity detector', 'Wavelet packet transform']",Speech enhancement using sparse dictionary learning in wavelet packet transform domain
223,"The i-vector representation and modeling technique has been successfully applied in spoken language identification (SLI). The advantage of using the i-vector representation is that any speech utterance with a variable duration length can be represented as a fixed length vector. In modeling, a discriminative transform or classifier must be applied to emphasize the variations correlated to language identity since the i-vector representation encodes several types of the acoustic variations (e.g., speaker variation, transmission channel variation, etc.). Owing to the strong nonlinear discriminative power, the neural network model has been directly used to learn the mapping function between the i-vector representation and the language identity labels. In most studies, only the point-wise feature-label information is fed to the model for parameter learning that may result in model overfitting, particularly when with limited training data. In this study, we propose to integrate pair-wise distance metric learning as the regularization of model parameter optimization. In the representation space of nonlinear transforms in the hidden layers, a distance metric learning is explicitly designed to minimize the pair-wise intra-class variation and maximize the inter-class variation. Using the pair-wise distance metric learning, the i-vectors are transformed to a new feature space, wherein they are much more discriminative for samples belonging to different languages while being much more similar for samples belonging to the same language. We tested the algorithm on an SLI task, and obtained promising results, which outperformed conventional regularization methods.","['Xugang Lu', 'Peng Shen', 'Yu Tsao', 'Hisashi Kawai']",July 2017,Computer Speech & Language,"['Neural network model', 'Cross entropy', 'Pair-wise distance metric learning', 'Spoken language identification,']",Regularization of neural network model with distance metric learning for i-vector based spoken language identification
224,"Named Entity Recognition (NER) is a key NLP task, which is all the more challenging on Web and user-generated content with their diverse and continuously changing language. This paper aims to quantify how this diversity impacts state-of-the-art NER methods, by measuring named entity (NE) and context variability, feature sparsity, and their effects on precision and recall. In particular, our findings indicate that NER approaches struggle to generalise in diverse genres with limited training data. Unseen NEs, in particular, play an important role, which have a higher incidence in diverse genres such as social media than in more regular genres such as newswire. Coupled with a higher incidence of unseen features more generally and the lack of large training corpora, this leads to significantly lower F1 scores for diverse genres as compared to more regular ones. We also find that leading systems rely heavily on surface forms found in training data, having problems generalising beyond these, and offer explanations for this observation.","['Isabelle Augenstein', 'Leon Derczynski', 'Kalina Bontcheva']",July 2017,Computer Speech & Language,"['Natural language processing', 'Information extraction', 'Named entity recognition', 'Generalisation', 'Entity drift', 'Social media', 'Quantitative study']",Generalisation in named entity recognition: A quantitative analysis
225,"This work presents a novel use of the sparse coding over redundant dictionary for fast adaptation of the acoustic models in the hidden Markov model-based automatic speech recognition (ASR) systems. The presented work is an extension of the existing acoustic model-interpolation-based fast adaptation approaches. In these methods, the basis (model) weights are estimated using an iterative procedure employing the maximum-likelihood (ML) criterion. For effective adaptation, typically a number of bases are selected and as a result of that the latency of the iterative weight estimation process becomes high for those ASR tasks that involve human-machine interactions. To address this issue, we propose the use of sparse coding of the target mean supervector over a speaker-specific (exemplar) redundant dictionary. In this approach, the employed greedy sparse coding not only selects the desired bases but also compresses them into a single supervector, which is then ML scaled to yield the adapted mean parameters. Thus reducing the latency in the basis weight estimation in comparison to the existing fast adaptation techniques. Further, to address the loss in information due to reduced degrees of freedom, we have also extended the proposed approach using separate sparse codings over multiple (exemplar and learned) redundant dictionaries. In adapting an ASR task involving human-computer interactions, the proposed approach is found to be as effective as the existing techniques but with a substantial reduction in the computational cost.","['S. Shahnawazuddin', 'Rohit Sinha']",May 2017,Computer Speech & Language,"['Fast adaptation', 'Acoustic model interpolation', 'Sparse coding', 'Exemplar and learned speaker dictionary']",Sparse coding over redundant dictionaries for fast adaptation of speech recognition system
226,"This paper investigates the use of multi-distribution deep neural networks (MD-DNNs) for automatic intonation classification in second-language (L2) English speech. If a classified intonation is different from the target one, we consider that mispronunciation is detected and appropriate diagnostic feedback can be provided thereafter. To transcribe speech data for intonation classification, we propose the RULF labels which are used to transcribe an intonation as rising, upper, lower or falling. These four types of labels can be further merged into two groups – rising and falling. Based on the annotated data from 100 Mandarin and 100 Cantonese learners, we develop an intonation classifier, which considers only 8 frames (i.e., 80 ms) of pitch value prior to the end of the pitch contour over an intonational phrase (IP). This classifier determines the intonation of L2 English speech as either rising or falling with an accuracy of 93.0%.","['Kun Li', 'Xixin Wu', 'Helen Meng']",May 2017,Computer Speech & Language,"['Language learning', 'L2 English speech', 'Suprasegmental', 'Intonation', 'Pitch accent', 'Deep neural networks']",Intonation classification for L2 English speech using multi-distribution deep neural networks
227,"Extracting structured information from unstructured text is important for the qualitative data analysis. Leveraging NLP techniques for qualitative data analysis will effectively accelerate the annotation process, allow for large-scale analysis and provide more insights into the text to improve the performance. The first step for gaining insights from the text is Named Entity Recognition (NER). A significant challenge that directly impacts the performance of the NER process is the domain diversity in qualitative data. The represented text varies according to its domain in many aspects including taxonomies, length, formality and format. In this paper we discuss and analyse the performance of state-of-the-art tools across domains to elaborate their robustness and reliability. In order to do that, we developed a standard, expandable and flexible framework to analyse and test tools performance using corpora representing text across various domains. We performed extensive analysis and comparison of tools across various domains and from various perspectives. The resulting comparison and analysis are of significant importance for providing a holistic illustration of the state-of-the-art tools.","['Zahraa S. Abdallah', 'Mark Carman', 'Gholamreza Haffari']",May 2017,Computer Speech & Language,"['Named entity recognition', 'Multi-domain evaluation', 'Qualitative data analysis', 'Benchmark evaluation']",Multi-domain evaluation framework for named entity recognition tools
228,"Lack of parallel corpora have diverted the direction of research towards exploring other arenas to fill in the dearth. Comparable corpora have proved to be a valuable resource in this regard. Interestingly other than the parallel sentences extracted from comparable corpora, parallel phrase fragments have also proved to be beneficial for statistical machine translation. We present a novel approach based on an efficient framework for parallel fragment extraction from comparable corpora. Using the fragments as additional corpus for translation, we are able to obtain an improvement of 0.88 and 0.89 BLEU points on test data for Arabic–English and French–English systems respectively. We have also conducted a detailed analysis of impact of fragments extracted from related vs non-related corpus. A comparison of impact of parallel fragments vs. parallel sentences is also presented highlighting the significance of parallel segments for statistical machine translation. The article concludes with a crude comparative analysis of our approach with an existing fragment extraction technique at various stages of the fragment extraction pipeline.","['Sadaf Abdul-Rauf', 'Holger Schwenk', 'Mohammad Nawaz']",May 2017,Computer Speech & Language,"['Parallel fragments', 'Statistical machine translation', 'Comparable corpus']",Parallel fragments : Measuring their impact on translation performance
229,"Accurate estimation of acoustic speech features from noisy speech and from different speakers is an ongoing problem in speech processing. Many methods have been proposed to estimate acoustic features but errors increase as signal-to-noise ratios fall. This work proposes a robust statistical framework to estimate an acoustic speech vector (comprising voicing, fundamental frequency and spectral envelope) from an intermediate feature that is extracted from a noisy time-domain speech signal. The initial approach is accurate in clean conditions but deteriorates in noise and with changing speaker. Adaptation methods are then developed to adjust the acoustic models to the noise conditions and speaker. Evaluations are carried out in stationary and nonstationary noises and at SNRs from −5 dB to clean conditions. Comparison with conventional methods of estimating fundamental frequency, voicing and spectral envelope reveals the proposed framework to have lowest errors in all conditions tested.","['Philip Harding', 'Ben Milner']",March 2017,Computer Speech & Language,"['Voicing', 'Fundamental frequency', 'Spectral envelope', 'Noise adaptation', 'Speaker adaptation']",Estimating acoustic speech features in low signal-to-noise ratios using a statistical framework
230,"State-of-the-art speaker verification systems are vulnerable to spoofing attacks using speech synthesis. To solve the issue, high-performance synthetic speech detectors (SSDs) for attack methods have been proposed recently. Here, as opposed to developing new detectors, we investigate new attack strategies. Investigating new techniques that are specifically tailored for spoofing attacks that can spoof the voice verification system and are difficult to detect is expected to increase the security of voice verification systems by enabling the development of better detectors. First, we investigated the vulnerability of an i-vector based verification system to attacks using statistical speech synthesis (SSS), with a particular focus on the case where the attacker has only a very limited amount of data from the target speaker. Even with a single adaptation utterance, the false alarm rate was found to be 23%. Still, SSS-generated speech is easy to detect (Wu et al., 2015a, 2015b), which dramatically reduces its effectiveness. For more effective attacks with limited data, we propose a hybrid statistical/concatenative synthesis approach and show that hybrid synthesis significantly increases the false alarm rate in the verification system compared to the baseline SSS method. Moreover, proposed hybrid synthesis makes detecting synthetic speech more difficult compared to SSS even when very limited amount of original speech recordings are available to the attacker. To further increase the effectiveness of the attacks, we propose a linear regression method that transforms synthetic features into more natural features. Even though the regression approach is more effective at spoofing the detectors, it is not as effective as the hybrid synthesis approach in spoofing the verification system. An interpolation approach is proposed to combine the linear regression and hybrid synthesis methods, which is shown to provide the best spoofing performance in most cases.","['Ali Khodabakhsh', 'Amir Mohammadi', 'Cenk Demiroglu']",March 2017,Computer Speech & Language,"['Statistical speech synthesis', 'Hybrid speech synthesis', 'Spoofing verification systems', 'Speaker adaptation', 'Synthetic speech detection']",Spoofing voice verification systems with statistical speech synthesis using limited adaptation data
231,"This paper aims at developing an HMM-based speech synthesis system capable of generating creaky voice in addition to modal voice. Generation of creaky voice is carried out by addressing two main issues, namely, an automatic prediction of creaky voice and appropriate modelling of the excitation signal of creaky voice. An automatic creaky voice detection method is proposed based on the analysis of variation of epoch parameters for different voicing regions. A neural network classifier is trained using the variances of epoch parameters for detection of creaky regions. A hybrid source model which is an extension of recently developed time-domain deterministic plus noise model is proposed for modelling creaky excitation signal. In the proposed hybrid source model, the pitch-synchronous analysis is performed on the creaky excitation signal of every phone. From the creaky residual frames of every phonetic class, the deterministic and noise components are estimated. The creaky deterministic components of all phonetic classes are stored in the database. The noise components are parameterized in terms of spectral and amplitude envelopes and are modelled by HMMs. During synthesis, the appropriate deterministic component is selected from the database, and the noise component is constructed from the parameters generated from HMMs. The creaky deterministic and noise components are pitch-synchronously overlap-added to produce the creaky excitation signal. Subjective evaluation results indicate that the incorporation of creaky voice has improved the naturalness of the synthetic speech of two male speakers, and the quality is slightly better than the basic time-domain deterministic plus noise model meant for only modal excitation.","['N.P. Narendra', 'K. Sreenivasa Rao']",March 2017,Computer Speech & Language,"['HMM-based speech synthesis', 'Detection of creaky voice', 'Zero-frequency filtering', 'Epoch parameters', 'Synthesis of creaky voice', 'Deterministic plus noise model', 'Hybrid source modelling']",Generation of creaky voice for improving the quality of HMM-based speech synthesis
232,"Integration of in-domain knowledge into an out-of-domain statistical machine translation (SMT) system poses challenges due to the lack of resources. Lack of in-domain bilingual corpora is one such issue. In this paper, we propose a simplification–translation–restoration (STR) framework for domain adaptation in SMT systems. An SMT system to translate medical records from English to Chinese is taken as a case study. We identify the critical segments in a medical sentence and simplify them to alleviate the data sparseness problem in the out-of-domain SMT system. After translating the simplified sentence, the translations of these critical segments are restored to their proper positions. Besides the simplification pre-processing step and the restoration post-processing step, we also enhance the translation and language models in the STR framework by using pseudo bilingual corpora generated by the background MT system. In the experiments, we adapt an SMT system from a government document domain to a medical record domain. The results show the effectiveness of the STR framework.","['Han-Bin Chen', 'Hen-Hsen Huang', 'An-Chang Hsieh', 'Hsin-Hsi Chen']",March 2017,Computer Speech & Language,"['Cross-domain SMT', 'Domain adaptation', 'Statistical machine translation', 'Medical document processing']",A simplification–translation–restoration framework for domain adaptation in statistical machine translation: A case study in medical record translation
233,"This paper proposes an unsupervised method for analyzing speaker roles in multi-participant conversational speech. First, features for characterizing the differences of various roles are extracted from the outputs of speaker diarization. Then, an algorithm of role clustering based on the criterion of maximizing the inter-cluster distance without using any convergence threshold is proposed to obtain the number of roles and to merge the utterances belonging to the same role into one cluster. The contributions of different combinations of individual feature subsets are compared for the proposed method on the outputs from speaker diarization, and the combined feature subsets obtain higher F scores than the individual ones for clustering speaker roles. The impacts of both speaker diarization errors and feature dimensions on the performance of the proposed method are also discussed. Experiments are done on the outputs of both manual annotations and automatic speaker diarization to compare the proposed method with both the state-of-the-art clustering method and the supervised method. Evaluations show that the proposed method is superior to the previous clustering method and close to the conventional supervised method in terms of F scores under two different experimental conditions.","['Yanxiong Li', 'Qin Wang', 'Xue Zhang', 'Wei Li', 'Xinchao Li', 'Jichen Yang', 'Xiaohui Feng', 'Qian Huang', 'Qianhua He']",March 2017,Computer Speech & Language,"['Speaker role', 'Speaker diarization', 'Role clustering', 'Multi-participant conversational speech']",Unsupervised classification of speaker roles in multi-participant conversational speech
234,"Using phonological speech vocoding, we propose a platform for exploring relations between phonology and speech processing, and in broader terms, for exploring relations between the abstract and physical structures of a speech signal. Our goal is to make a step towards bridging phonology and speech processing and to contribute to the program of Laboratory Phonology.We show three application examples for laboratory phonology: compositional phonological speech modelling, a comparison of phonological systems and an experimental phonological parametric text-to-speech (TTS) system. The featural representations of the following three phonological systems are considered in this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English (SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded speech, we conclude that the latter achieves slightly better results than the former. However, GP – the most compact phonological speech representation – performs comparably to the systems with a higher number of phonological features. The parametric TTS based on phonological speech representation, and trained from an unlabelled audiobook in an unsupervised manner, achieves intelligibility of 85% of the state-of-the-art parametric speech synthesis.We envision that the presented approach paves the way for researchers in both fields to form meaningful hypotheses that are explicitly testable using the concepts developed and exemplified in this paper. On the one hand, laboratory phonologists might test the applied concepts of their theoretical models, and on the other hand, the speech processing community may utilize the concepts developed for the theoretical phonological models for improvements of the current state-of-the-art applications.","['Miloš Cerňak', 'Štefan Beňuš', 'Alexandros Lazaridis']",March 2017,Computer Speech & Language,"['Phonological speech representation', 'Parametric speech synthesis', 'Laboratory phonology']",Speech vocoding for laboratory phonology
235,"We introduce a set of benchmark corpora of conversational English speech derived from the Switchboard-I and Fisher datasets. Traditional automatic speech recognition (ASR) research requires considerable computational resources and has slow experimental turnaround times. Our goal is to introduce these new datasets to researchers in the ASR and machine learning communities in order to facilitate the development of novel speech recognition techniques on smaller but still acoustically rich, diverse, and hence interesting corpora. We select these corpora to maximize an acoustic quality criterion while limiting the vocabulary size (from 10 words up to 10,000 words), where both “acoustic quality” and vocabulary size are adeptly measured via various submodular functions. We also survey numerous submodular functions that could be useful to measure both “acoustic quality” and “corpus complexity” and offer guidelines on when and why a scientist may wish use to one vs. another. The corpora selection process itself is naturally performed using various state-of-the-art submodular function optimization procedures, including submodular level-set constrained submodular optimization (SCSC/SCSK), difference-of-submodular (DS) optimization, and unconstrained submodular minimization (SFM), all of which are fully defined herein. While the focus of this paper is on the resultant speech corpora, and the survey of possible objectives, a consequence of the paper is a thorough empirical comparison of the relative merits of these modern submodular optimization procedures. We provide baseline word recognition results on all of the resultant speech corpora for both Gaussian mixture model (GMM) and deep neural network (DNN)-based systems, and we have released all of the corpora definitions and Kaldi training recipes for free in the public domain.","['Yuzong Liu', 'Rishabh Iyer', 'Katrin Kirchhoff', 'Jeff Bilmes']",March 2017,Computer Speech & Language,"['Submodular function optimization', 'Automatic speech recognition', 'Speech corpus']",SVitchboard-II and FiSVer-I: Crafting high quality and low complexity conversational english speech corpora using submodular function optimization
236,"An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations. Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of accuracy gain, with the purpose of measuring how much work is needed to achieve a certain degree of performance. The second relates the comparison of efficiency between systems at training time, with the objective of completing this task only for the one that best suits our requirements. The prediction of accuracy is also a valuable item of information for customizing systems, since we can estimate in advance the impact of settings on both the performance and the development costs. Using the generation of part-of-speech taggers as an example application, the experimental results are consistent with our expectations.","['Manuel Vilares Ferro', 'Víctor Manuel Darriba Bilbao', 'Francisco José Ribadas Pena']",January 2017,Computer Speech & Language,"['Correctness', 'Functional sequences', 'Learning curves', 'POS tagging', 'Proximity criterion', 'Robustness']",Modeling of learning curves with applications to POS tagging
237,"This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.","['Gary Geunbae Lee', 'Ho-Young Lee', 'Jieun Song', 'Byeongchang Kim', 'Sechun Kang', 'Jinsik Lee', 'Hyosung Hwang']",January 2017,Computer Speech & Language,"['Sentence stress', 'Sentence stress feedback system', 'Stress prediction model', 'Stress detection model', 'Stress feedback provision model', 'CALL']",Automatic sentence stress feedback for non-native English learners
238,"Referring to visually perceivable objects is a very common occurrence in everyday language use. In order to produce expressions that refer, the speaker needs to be able to pick out visual properties that the referred object has and determine the words that name those properties, such that the expression can direct a listener's attention to the intended object. The speaker can aid the listener by looking in the direction of the object and by providing a pointing gesture to indicate it. In order to resolve the reference, the listener has a difficult job to do: simultaneously use all of the linguistic and non-linguistic information; the words of the referring expression that denote properties of the object, such as its colour or shape, need to already be known, and the non-linguistic gaze direction and pointing gesture of the speaker need to be incorporated. Crucially, the listener does not wait until the end of the referring expression before she begins to resolve it; rather, she is interpreting it as it unfolds. A model that resolves referring expressions as the listener must be able to do all of these things. In this paper, we present such a generative model of reference resolution. We explain our model and show empirically through a series of experiments that the model can work incrementally (i.e., word for word) as referring expressions unfold, can incorporate multimodal information such as gaze and pointing gestures in two ways, can learn a grounded meaning of words in the referring expression, can incorporate contextual (i.e., saliency) information, and is robust to noisy input such as automatic speech recognition transcriptions, as well as uncertainty in the representation of the candidate objects.","['Casey Kennington', 'David Schlangen']",January 2017,Computer Speech & Language,"['Dialogue', 'Situated', 'Incremental', 'Stochastic', 'Reference resolution']",A simple generative model of incremental reference resolution for situated dialogue☆
239,"Present work explores the excitation source information for the language identification (LID) task. In this work, excitation source information is captured by implicit processing of linear prediction (LP) residual signal for discriminating the languages. Raw samples of LP residual signal, its magnitude, and phase components are processed independently at sub-segmental, segmental and suprasegmental levels for extracting the language-specific excitation source information. The LID studies are carried out using 27 Indian languages from Indian Institute of Technology Kharagpur-Multi Lingual Indian Language Speech Corpus (IITKGP-MLILSC) and 11 international languages from OGI-MLTS corpus. The Gaussian mixture models (GMMs) are used in this work to model the language-specific excitation source information for LID task. From the experimental results, it can be observed that, features extracted from segmental level yields better identification accuracy (50.92%), compared to sub-segmental (47.77%) and suprasegmental levels (43.88%). Further, the evidence from all three levels is combined to obtain the complete excitation source information. Finally, we have investigated the existence of non-overlapping language-specific information present in excitation source and vocal tract features.","['Dipanjan Nandi', 'Debadatta Pati', 'K. Sreenivasa Rao']",January 2017,Computer Speech & Language,"['Hilbert envelope', 'Residual phase', 'LP residual', 'Language identification (LID)', 'Subsegmental', 'Segmental', 'Suprasegmental', 'Excitation source information', 'IITKGP-MLILSC', 'OGI-MLTS', 'Implicit processing of LP residual']",Implicit processing of LP residual for language identification
240,"In this work, the linear prediction (LP) residual signal has been parameterized to capture the excitation source information for language identification (LID) study. LP residual signal has been processed at three different levels: sub-segmental, segmental and supra-segmental levels to demonstrate different aspects of language-specific excitation source information. Proposed excitation source features have been evaluated on 27 Indian languages from Indian Institute of Technology Kharagpur-Multi Lingual Indian Language Speech Corpus (IITKGP-MLILSC), Oregon Graduate Institute Multi-Language Telephone-based Speech (OGI-MLTS) and National Institute of Standards and Technology Language Recognition Evaluation (NIST LRE) 2011 corpora. LID systems were developed using Gaussian mixture model (GMM) and i-vector based approaches. Experimental results have shown that segmental level parametric features provide better identification accuracy (62%), compared to sub-segmental (40%) and supra-segmental level (34%) features. Excitation source features obtained from three levels show distinct language-specific evidence. Therefore, the scores from all three levels are combined to obtain the complete excitation source information for the LID task. LID performances achieved from both the excitation source and vocal tract system are compared. Finally, the scores obtained by processing the vocal tract and excitation source features are combined to achieve better improvement in LID accuracy. The best recognition accuracies obtained from stage-IV integrated LID systems I, II and III are 69%, 70% and 72% respectively.","['Dipanjan Nandi', 'Debadatta Pati', 'K. Sreenivasa Rao']",January 2017,Computer Speech & Language,"['Excitation source information', 'Language identification (LID)', 'LP residual', 'GFD parameters', 'RMFCC', 'MPDSS', 'Pitch contour', 'Epoch strength contour', 'MFCC', 'i-Vector', 'IITKGP-MLILSC', 'OGI-MLTS', 'NIST LRE', 'Cavg']",Parametric representation of excitation source information for language identification
241,"Vocal emotions, as well as different speaking styles and speaker traits, are characterized by a complex interplay of multiple prosodic features. Natural sounding speech synthesis with the ability to control such paralinguistic aspects requires the manipulation of the corresponding prosodic features. With traditional concatenative speech synthesis it is easy to manipulate the “primary” prosodic features pitch, duration, and intensity, but it is very hard to individually control “secondary” prosodic features like phonation type, vocal tract length, articulatory precision and nasality. These secondary features can be controlled more directly with parametric synthesis methods. In the present study we analyze the ability of articulatory speech synthesis to control secondary prosodic features by rule. To this end, nine German words were re-synthesized with the software VocalTractLab 2.1 and then manipulated in different ways at the articulatory level to vary vocal tract length, articulatory precision and degree of nasality. Listening tests showed that most of the intended prosodic manipulations could be reliably identified with recognition rates between 77% and 96%. Only the manipulations to increase articulatory precision were hardly recognized. The results suggest that rule-based manipulations in articulatory synthesis are generally sufficient for the convincing synthesis of secondary prosodic features at the word level.","['Peter Birkholz', 'Lucia Martin', 'Yi Xu', 'Stefan Scherbaum', 'Christiane Neuschaefer-Rube']",January 2017,Computer Speech & Language,"['Prosody', 'Feature manipulation', 'Articulatory synthesis']","Manipulation of the prosodic features of vocal tract length, nasality and articulatory precision using articulatory synthesis"
242,"Word Sense Disambiguation (WSD) is a fundamental task useful for Information Retrieval, Information Extraction, web search, and indexing, among others. In the literature there exist several works dedicated to generic WSD task, but in recent years domain-specific WSD has attracted the attention of several researchers. In this sense, this paper describes an approach for domain-specific WSD by selecting the predominant sense (synset from WordNet) of ambiguous words. To achieve it the method uses two corpora: the domain-specific test corpus (containing target ambiguous words) and a domain-specific auxiliary corpus (obtained by using relevant words from the domain-specific test corpus). The approach has four main stages: (1) auxiliary corpus generation; (2) related features extraction (from the auxiliary corpus); (3) test features extraction (from the test corpus); and (4) features integration. The proposed approach has been tested on domain-specific corpora (Sports and Finance) and on one balanced corpus, BNC. Even though our WSD approach showed some limitations when dealing with the general-domain corpus, the obtained results for domain-specific corpora, which are our main interest, were better than those reported in previous works.","['Ivan Lopez-Arevalo', 'Victor J. Sosa-Sosa', 'Franco Rojas-Lopez', 'Edgar Tello-Leal']",January 2017,Computer Speech & Language,"['Domain-specific word sense disambiguation', 'WordNet', 'Synset', 'Context']",Improving selection of synsets from WordNet for domain-specific word sense disambiguation
243,"A widely used automatic translation approach, phrase-based statistical machine translation, learns a probabilistic translation model composed of phrases from a large parallel corpus with a large language model. The translation model is often enormous because of many combinations of source and target phrases, which leads to the restriction of applications to limited computing environments. Entropy-based pruning resolves this issue by reducing the model size while retaining the translation quality. To safely reduce the size, this method detects redundant components by evaluating a relative entropy of models before and after pruning the components. In the literature, this method is effective, but we have observed that it can be improved more by adjusting the divergence distribution determined by the relative entropy. In the results of preliminary experiments, we derive two factors responsible for limiting pruning efficiency of entropy-based pruning. The first factor is proportion of pairs composing translation models with respect to their translation probability and its estimate. The second factor is the exponential increase of the divergence for pairs with low translation probability and estimate. To control the factors, we propose a divergence-based fine pruning using a divergence metric to adapt the curvature change of the boundary conditions for pruning and Laplace smoothing. In practical translation tasks for English–Spanish and English–French language pairs, this method shows statistically significant improvement on the efficiency up to 50% and average 12% more pruning compared to entropy-based pruning to show the same translation quality.","['Kangil Kim', 'Eun-Jin Park', 'Jong-Hun Shin', 'Oh-Woog Kwon', 'Young-Kil Kim']",January 2017,Computer Speech & Language,"['Statistical machine translation', 'Model revision', 'Phrase table', 'Entropy-based pruning', 'Relative entropy']",Divergence-based fine pruning of phrase-based statistical translation model
244,"This paper presents a new domain compensation framework by using phonetically discriminative features which are extracted from domain-dependent deep neural networks (DNNs). The domain compensation can be applied in both unsupervised and supervised manner, depending on whether the domain information of the development data is provided or not in advance. In supervised manner, the DNNs are trained on the development speech recordings of each given domain separately. While in the unsupervised manner, the development datasets are first automatically clustered into different domains, by using the Gaussian Mixture Model mean supervectors which are generated from each of the speech recordings, DNNs are then trained on the resulting clusters. Finally, we compensate the domain variabilities during the target speaker modeling step using support vector machines, by feeding in statistical vectors which are derived from the discriminative features extracted from the domain-dependent DNNs. The main strength of our proposed framework is that it does not need any speaker labels in the development dataset, which makes the proposed framework of great advantage over the state-of-the-art techniques that need speaker labels to train inter-speaker and/or intra-speaker variability models or channel compensation. Three speaker verification systems are investigated to examine the effectiveness of this new framework. Experimental results on the NIST SRE 2010 task demonstrate competitive performances to the state-of-the-art techniques in an initial implementation of the proposed framework.","['Yanhua Long', 'Hong Ye', 'Jifeng Ni']",January 2017,Computer Speech & Language,"['Discriminative features', 'I-vector', 'Domain compensation', 'Deep neural network', 'Speaker verification']",Domain compensation based on phonetically discriminative features for speaker verification
245,"This paper proposes a framework for performing adaptation to complex and non-stationary background conditions in Automatic Speech Recognition (ASR) by means of asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transforms and asynchronous Noise Adaptive Training (aNAT). The proposed method aims to apply the feature transform that best compensates the background for every input frame. The implementation is done with a new Hidden Markov Model (HMM) topology that expands the usual left-to-right HMM into parallel branches adapted to different background conditions and permits transitions among them. Using this, the proposed adaptation does not require ground truth or previous knowledge about the background in each frame as it aims to maximise the overall log-likelihood of the decoded utterance. The proposed aCMLLR transforms can be further improved by retraining models in an aNAT fashion and by using speaker-based MLLR transforms in cascade for an efficient modelling of background effects and speaker. An initial evaluation in a modified version of the WSJCAM0 corpus incorporating 7 different background conditions provides a benchmark in which to evaluate the use of aCMLLR transforms. A relative reduction of 40.5% in Word Error Rate (WER) was achieved by the combined use of aCMLLR and MLLR in cascade. Finally, this selection of techniques was applied in the transcription of multi-genre media broadcasts, where the use of aNAT training, aCMLLR transforms and MLLR transforms provided a relative improvement of 2–3%.","['Oscar Saz', 'Thomas Hain']",January 2017,Computer Speech & Language,"['Speech recognition', 'Acoustic adaptation', 'Factorisation', 'Dynamic background', 'Media transcription']",Acoustic adaptation to dynamic background conditions with asynchronous transformations
246,"Understanding architectural choices for deep neural networks (DNNs) is crucial to improving state-of-the-art speech recognition systems. We investigate which aspects of DNN acoustic model design are most important for speech recognition system performance, focusing on feed-forward networks. We study the effects of parameters like model size (number of layers, total parameters), architecture (convolutional networks), and training details (loss function, regularization methods) on DNN classifier performance and speech recognizer word error rates. On the Switchboard benchmark corpus we compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. Using a much larger 2100-hour training corpus (combining Switchboard and Fisher) we examine the performance of very large DNN models – with up to ten times more parameters than those typically used in speech recognition systems. The results suggest that a relatively simple DNN architecture and optimization technique give strong performance, and we offer intuitions about architectural choices like network depth over breadth. Our findings extend previous works to help establish a set of best practices for building DNN hybrid speech recognition systems and constitute an important first step toward analyzing more complex recurrent, sequence-discriminative, and HMM-free architectures.","['Andrew L. Maas', 'Peng Qi', 'Ziang Xie', 'Awni Y. Hannun', 'Christopher T. Lengerich', 'Daniel Jurafsky', 'Andrew Y. Ng']",January 2017,Computer Speech & Language,"['Hidden Markov model deep neural network (HMM-DNN)', 'Neural networks', 'Acoustic modeling', 'Speech recognition', 'Large vocabulary continuous speech recognition (LVCSR)']",Building DNN acoustic models for large vocabulary speech recognition
247,"Natural Language Processing (NLP) is a field of computer science and linguistics concerned with the unique conversation between computers and human languages. It processes data through Lexical analysis, Syntax analysis, Semantic analysis, Discourse processing and Pragmatic analysis. An intelligent text summarization is one of the most challenging tasks in Natural language processing. It can be further used for applications like storytelling and question answering. This paper presents an automatic text summarizer for text documents using soft computing approach, consisting of SVO (Subject, Verb, and Object) Rules and Tag based training. This approach processes data through POS Tagger, NLP Parser, ambiguity removal, Semantic Representation, Sentence Reduction and Sentence Combination. At first, this paper defines the theme (title) of the document. After this operation, it preprocesses text document to perform pronominal reference resolution and text clustering. After these preprocessing operations, it identifies and removes ambiguity from the language using parser. And then, it calculates the score for the sentences using the title of the document, Semantic Sentence Similarity utility and n-gram Co-Occurrence relations of the words in a particular sentence. At last, sentences are combined with the SVO Rules after providing tag based training for simple and complex sentences. The summarizer was tested on the standard DUC 2007 dataset as well as a corpus of hundred text documents of different domains created by us. DUC 2007 Update Task produced accuracy F-scores of 0.13523 (ROUGE-2) and 0.112561 (ROUGE-SU4) for DUC 2007 documents and 0.4036 (ROUGE-2) and 0.3129 (ROUGE-SU4) for our corpus. Subjective evaluation was carried out by five language experts and twenty random individuals for system generated sample summaries.","['Madhuri A. Tayal', 'Mukesh M. Raghuwanshi', 'Latesh G. Malik']",January 2017,Computer Speech & Language,"['Text document', 'Summarization', 'Semantic representation', 'Clustering', 'Reference resolution', 'Evaluation']",ATSSC: Development of an approach based on soft computing for text summarization
248,"In this paper we explore the use of semantics in training language models for automatic speech recognition and spoken language understanding. Traditional language models (LMs) do not consider the semantic constraints and train models based on fixed-sized word histories. The theory of frame semantics analyzes word meanings and their constructs by using “semantic frames”. Semantic frames represent a linguistic scene with its relevant participants and their relations. They are triggered by target words and include slots which are filled by frame elements. We present semantic LMs (SELMs), which use recurrent neural network architectures and the linguistic scene of frame semantics as context. SELMs incorporate semantic features which are extracted from semantic frames and target words. In this way, long-range and “latent” dependencies, i.e. the implicit semantic dependencies between words, are incorporated into LMs. This is crucial especially when the main aim of spoken language systems is understanding what the user means. Semantic features consist of low-level features, where frame and target information is directly used; and deep semantic encodings, where deep autoencoders are used to extract semantic features. We evaluate the performance of SELMs on publicly available corpora: the Wall Street Journal read-speech corpus and the LUNA human–human conversational corpus. The encoding of semantic frames into SELMs improves the word recognition performance and especially the recognition performance of the target words, the meaning bearing elements of semantic frames. We assess the performance of SELMs for the understanding tasks and we show that SELMs yield better semantic frame identification performance compared to recurrent neural network LMs.","['Ali Orkan Bayer', 'Giuseppe Riccardi']",November 2016,Computer Speech & Language,"['Language modeling', 'Recurrent neural networks', 'Frame semantics', 'Semantic language models', 'Deep autoencoders']",Semantic language models with deep neural networks
249,"This paper proposes the task of speaker attribution as speaker diarization followed by speaker linking. The aim of attribution is to identify and label common speakers across multiple recordings. To do this, it is necessary to first carry out diarization to obtain speaker-homogeneous segments from each recording. Speaker linking can then be conducted to link common speaker identities across multiple inter-session recordings. This process can be extremely inefficient using the traditional agglomerative cluster merging and retraining commonly employed in diarization. We thus propose an attribution system using complete-linkage clustering (CLC) without model retraining. We show that on top of the efficiency gained through elimination of the retraining phase, greater accuracy is achieved by utilizing the farthest-neighbor criterion inherent to CLC for both diarization and linking. We first evaluate the use of CLC against an agglomerative clustering (AC) without retraining approach, traditional agglomerative clustering with retraining (ACR) and single-linkage clustering (SLC) for speaker linking. We show that CLC provides a relative improvement of 20%, 29% and 39% in attribution error rate (AER) over the three said approaches, respectively. We then propose a diarization system using CLC and show that it outperforms AC, ACR and SLC with relative improvements of 32%, 50% and 70% in diarization error rate (DER), respectively. In our work, we employ the cross-likelihood ratio (CLR) as the model comparison metric for clustering and investigate its robustness as a stopping criterion for attribution.","['Houman Ghaemmaghami', 'David Dean', 'Sridha Sridharan', 'David A. van Leeuwen']",November 2016,Computer Speech & Language,"['Speaker attribution', 'Linking', 'Diarization', 'Complete-linkage clustering', 'Joint factor analysis', 'Cross-likelihood ratio']",A study of speaker clustering for speaker attribution in large telephone conversation datasets
250,"In this work, we present a comprehensive study on the use of deep neural networks (DNNs) for automatic language identification (LID). Motivated by the recent success of using DNNs in acoustic modeling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from its short-term acoustic features. We propose two different DNN-based approaches. In the first one, the DNN acts as an end-to-end LID classifier, receiving as input the speech features and providing as output the estimated probabilities of the target languages. In the second approach, the DNN is used to extract bottleneck features that are then used as inputs for a state-of-the-art i-vector system. Experiments are conducted in two different scenarios: the complete NIST Language Recognition Evaluation dataset 2009 (LRE'09) and a subset of the Voice of America (VOA) data from LRE'09, in which all languages have the same amount of training data. Results for both datasets demonstrate that the DNN-based systems significantly outperform a state-of-art i-vector system when dealing with short-duration utterances. Furthermore, the combination of the DNN-based and the classical i-vector system leads to additional performance improvements (up to 45% of relative improvement in both EER and Cavg on 3s and 10s conditions, respectively).","['Ignacio Lopez-Moreno', 'Javier Gonzalez-Dominguez', 'David Martinez', 'Oldřich Plchot', 'Joaquin Gonzalez-Rodriguez', 'Pedro J. Moreno']",November 2016,Computer Speech & Language,"['LID', 'DNN', 'Bottleneck', 'i-vectors']",On the use of deep feedforward neural networks for automatic language identification
251,"The topic model is one of best known hierarchical Bayesian models for language modeling and document analysis. It has achieved a great success in text classification, in which a text is represented as a big of its words, disregarding grammar and even word order, that is referred to as the bag-of-words assumption. In this paper, we investigate topic modeling of the Chinese language, which has different morphology from alphabetical western languages like English. The Chinese characters, but not the Chinese words, are the basic structural units in Chinese. In previous empirical studies, it shows that the character-based topic model performs better than the word-based topic model. In this research, we propose the character–word topic model (CWTM) to consider the character–word relation in topic modeling. Two types of experiments are designed to test the performance of the new proposed model: topic extraction and text classification. By empirical studies, we demonstrate the superiority of the new proposed model comparing to both word and character based topic models.","['Zengchang Qin', 'Yonghui Cong', 'Tao Wan']",November 2016,Computer Speech & Language,"['Topic models', 'Chinese language modeling', 'Text classification', 'Language model', 'Character–word topic model', 'Latent Dirichlet allocation']",Topic modeling of Chinese language beyond a bag-of-words☆
252,"Traditional concept retrieval is based on usual word definition dictionaries with simple performance: they just map words to their definitions. This approach is mostly helpful for readers and language students, but writers sometimes need to find a word that encompasses a set of ideas that they have in mind. For this task, inverse dictionaries are ready to help; however, in some cases a sought word does not correspond to a single definition but to a composite meaning of several concepts. A language producer then tends to require a concept search that starts with a group of words or a series of related terms, looking for a target word. This paper aims to assist on this task by presenting a new approach for concept blending through the development of a search-by-concept method based on vector space representation using semantic analysis and statistical natural language processing techniques. Words are represented as numeric vectors based on different semantic similarity measures and probabilistic measures; the semantic properties of a word are captured in the vector elements determined by a given linguistic context. Three different sources are used as context for word vector construction: WordNet, a distributional thesaurus, and the Latent Dirichlet Allocation algorithm; each source is used for building a different semantic vector space.The concept-blender input is then conformed by a set of n-nouns. All input members are read and substituted by their corresponding vectors. Then, a semantic space analysis including a filtering and ranking process is carried out to deploy a list of target words. A test set of 50 concepts was created in order to evaluate the system's performance. A group of 30 evaluators found our integrated concept blending model to provide better results for finding an adequate word for the provided set of concepts.","['Hiram Calvo', 'Oscar Méndez', 'Marco A. Moreno-Armendáriz']",November 2016,Computer Speech & Language,"['Computational linguistics', 'Natural language processing', 'Lexicography', 'Vector space models', 'Reverse lookup dictionaries', 'Concept-blending']",Integrated concept blending with vector space models☆
253,"The degree of similarity between sentences is assessed by sentence similarity methods. Sentence similarity methods play an important role in areas such as summarization, search, and categorization of texts, machine translation, etc. The current methods for assessing sentence similarity are based only on the similarity between the words in the sentences. Such methods either represent sentences as bag of words vectors or are restricted to the syntactic information of the sentences. Two important problems in language understanding are not addressed by such strategies: the word order and the meaning of the sentence as a whole. The new sentence similarity assessment measure presented here largely improves and refines a recently published method that takes into account the lexical, syntactic and semantic components of sentences. The new method was benchmarked using Li–McLean, showing that it outperforms the state of the art systems and achieves results comparable to the evaluation made by humans. Besides that, the method proposed was extensively tested using the SemEval 2012 sentence similarity test set and in the evaluation of the degree of similarity between summaries using the CNN-corpus. In both cases, the measure proposed here was proved effective and useful.","['Rafael Ferreira', 'Rafael Dueire Lins', 'Steven J. Simske', 'Fred Freitas', 'Marcelo Riss']",September 2016,Computer Speech & Language,"['Graph-based model', 'Sentence simplification', 'Relation extraction', 'Inductive logic programming']","Assessing sentence similarity through lexical, syntactic and semantic analysis☆"
254,"Nowadays natural language processing plays an important and critical role in the domain of intelligent computing, pattern recognition, semantic analysis and machine intelligence. For Chinese information processing, to construct the predictive models of different semantic word-formation patterns with a large-scale corpus can significantly improve the efficiency and accuracy of the paraphrase of the unregistered or new word, ambiguities elimination, automatic lexicography, machine translation and other applications. Therefore it is required to find the relationship between word-formation patterns and different influential factors, which can be denoted as a classification problem. However, due to noise, anomalies, imprecision, polysemy, ambiguity, nonlinear structure, and class-imbalance in semantic word-formation data, multi-criteria optimization classifier (MCOC), support vector machines (SVM) and other traditional classification approaches will give the poor predictive performance. In this paper, according to the characteristic analysis of Chinese word-formations, we firstly proposed a novel layered semantic graph of each disyllabic word, the layer-weighted graph edit distance (GED) and its similarity kernel embedded into a new vector space, then on the normalized data MCOC with kernel, fuzzification and penalty factors (KFP-MCOC) and SVM are employed to predict Chinese semantic word-formation patterns. Our experimental results and comparison with SVM show that KFP-MCOC based on the layer-weighted semantic graphs can increase the separation of different patterns, the predictive accuracy of target patterns and the generalization of semantic pattern classification on new compound words.","['Guangxia Gao', 'Zhiwang Zhang']",September 2016,Computer Speech & Language,"['Semantic', 'Word-formation', 'Graph edit distance', 'Multi-criteria optimization', 'Pattern classification']",Prediction of Chinese word-formation patterns using the layer-weighted semantic graph-based KFP-MCO classifier☆
255,"To automatically build, from scratch, the language processing component for a speech synthesis system in a new language, a purified text corpora is needed where any words and phrases from other languages are clearly identified or excluded. When using found data and where there is no inherent linguistic knowledge of the language/languages contained in the data, identifying the pure data is a difficult problem. We propose an unsupervised language identification approach based on Latent Dirichlet Allocation where we take the raw n-gram count as features without any smoothing, pruning or interpolation. The Latent Dirichlet Allocation topic model is reformulated for the language identification task and Collapsed Gibbs Sampling is used to train an unsupervised language identification model. In order to find the number of languages present, we compared four kinds of measure and also the Hierarchical Dirichlet process on several configurations of the ECI/UCI benchmark. Experiments on the ECI/MCI data and a Wikipedia based Swahili corpus shows this LDA method, without any annotation, has comparable precisions, recalls and F-scores to state of the art supervised language identification techniques.","['Wei Zhang', 'Robert A.J. Clark', 'Yongyuan Wang', 'Wen Li']",September 2016,Computer Speech & Language,"['Language filtering', 'Language purifying', 'Language identification']",Unsupervised language identification based on Latent Dirichlet Allocation☆
256,"In this paper we present a silent speech interface (SSI) system aimed at restoring speech communication for individuals who have lost their voice due to laryngectomy or diseases affecting the vocal folds. In the proposed system, articulatory data captured from the lips and tongue using permanent magnet articulography (PMA) are converted into audible speech using a speaker-dependent transformation learned from simultaneous recordings of PMA and audio signals acquired before laryngectomy. The transformation is represented using a mixture of factor analysers, which is a generative model that allows us to efficiently model non-linear behaviour and perform dimensionality reduction at the same time. The learned transformation is then deployed during normal usage of the SSI to restore the acoustic speech signal associated with the captured PMA data. The proposed system is evaluated using objective quality measures and listening tests on two databases containing PMA and audio recordings for normal speakers. Results show that it is possible to reconstruct speech from articulator movements captured by an unobtrusive technique without an intermediate recognition step. The SSI is capable of producing speech of sufficient intelligibility and naturalness that the speaker is clearly identifiable, but problems remain in scaling up the process to function consistently for phonetically rich vocabularies.","['Jose A. Gonzalez', 'Lam A. Cheah', 'James M. Gilbert', 'Jie Bai', 'Stephen R. Ell', 'Phil D. Green', 'Roger K. Moore']",September 2016,Computer Speech & Language,"['Silent speech interfaces', 'Speech rehabilitation', 'Speech synthesis', 'Permanent magnet articulography', 'Augmentative and alternative communication']",A silent speech system based on permanent magnet articulography and direct synthesis☆
257,"Parallel corpora are essential resources for statistical machine translation (SMT) and cross language information retrieval (CLIR) systems. Creating parallel corpora is highly expensive in terms of both time and cost. In this paper, we propose a novel approach to automatically extract parallel sentences from aligned documents. To do so, we first train a Maximum Entropy binary classifier to compute the local similarity between each two sentences in different languages. To consider global information (e.g., the position of sentence pairs in the aligned documents), we define an objective function to penalize the cross alignments and then propose an integer linear programming approach to optimize the objective function. In our experiments, we focus on English and Persian Wikipedia articles. The experimental results on manually aligned test data indicate that the proposed method outperforms the baselines, significantly. Furthermore, the extrinsic evaluations of the corpus extracted from Wikipedia on both SMT and CLIR systems demonstrate the quality of the extracted parallel sentences. In addition, Experiments on the English–German language pair demonstrate that the proposed ILP method is a language-independent sentence alignment approach. The extracted English–Persian parallel corpus is freely available for research purposes.","['Hamed Zamani', 'Heshaam Faili', 'Azadeh Shakery']",September 2016,Computer Speech & Language,"['Parallel corpus', 'Sentence alignment', 'Bilingual resource', 'Global information', 'Integer linear programming']",Sentence alignment using local and global information☆
258,"We propose an information theoretic region selection algorithm from the real time magnetic resonance imaging (rtMRI) video frames for a broad phonetic class recognition task. Representations derived from these optimal regions are used as the articulatory features for recognition. A set of connected and arbitrary shaped regions are selected such that the articulatory features computed from such regions provide maximal information about the broad phonetic classes. We also propose a tree-structured greedy region splitting algorithm to further segment these regions so that articulatory features from these split regions enhance the information about the phonetic classes. We find that some of the proposed articulatory features correlate well with the articulatory gestures from the Articulatory Phonology theory of speech production. Broad phonetic class recognition experiment using four rtMRI subjects reveals that the recognition accuracy with optimal split regions is, on average, higher than that using only acoustic features. Combining acoustic and articulatory features further reduces the error-rate by ∼8.25% (relative).","['Abhay Prasad', 'Prasanta Kumar Ghosh']",September 2016,Computer Speech & Language,"['Mutual information', 'Phonetic recognition', 'Speech production', 'Region splitting']",Information theoretic optimal vocal tract region selection from real time magnetic resonance images for broad phonetic class recognition☆
259,"In this paper, we investigate the ensemble of deep neural networks (DNNs) by using an acoustic environment classification (AEC) technique for the statistical model-based voice activity detection (VAD). From an investigation of the statistical model-based VAD, it is known that the traditional decision rule is based on the geometric mean of the likelihood ratio or the support vector machine (SVM), which is a shallow model with zero or one hidden layer. Since the shallow models cannot take an advantage of the diversity of the space distribution of features, in the training step, we basically build the multiple DNNs according the different noise types by employing the parameters of the statistical model-based VAD algorithm. In addition, the separate DNN is designed for the AEC algorithm in order to choose the best DNN for each noise. In the on-line noise-aware VAD step, the AEC is first performed on a frame-by-frame basis using the separate DNN so the a posteriori probabilities to identify noise are obtained. Once the probabilities are achieved for each noise, the environmental knowledge is contributed to allow us to combine the speech presence probabilities which are derived from the ensemble of the DNNs trained for the individual noise. Our approach for VAD was evaluated in terms of objective measures and showed significant improvement compared to the conventional algorithm.","['Inyoung Hwang', 'Hyung-Min Park', 'Joon-Hyuk Chang']",July 2016,Computer Speech & Language,"['Voice activity detection', 'Statistical model', 'Acoustic environment classification', 'Deep neural network', 'Ensemble']",Ensemble of deep neural networks using acoustic environment classification for statistical model-based voice activity detection☆
260,"This paper proposes a new probabilistic synchronous context-free grammar model for statistical machine translation. The model labels nonterminals with classes of boundary words on the target side of aligned phrase pairs. Labeling of the rules is performed with coarse grained and fine grained nonterminals using POS tags and word clusters trained on the target language corpus. Considering the large size of the proposed model due to the diversity of nonterminals, we have also proposed a novel approach for filtered rule extraction based on the alignment pattern of phrase pairs. Using limited patterns of rules, the extraction of hierarchical rules gets restricted from phrase pairs that are decomposable to two aligned subphrases. The proposed filtered rule extraction decreases the model size and the decoding time considerably with no significant impact on the translation quality. Using BLEU as a metric in our experiments, the proposed model achieved a notable improvement rate over the state-of-the-art hierarchical phrase-based model in the translation from Persian, French and Spanish to English language. This is applicable for all languages, even under-resourced ones having no linguistic tools.","['Shahram Salami', 'Mehrnoush Shamsfard', 'Shahram Khadivi']",July 2016,Computer Speech & Language,"['Statistical machine translation', 'Hierarchical models', 'Rules filtering']",Phrase-boundary model for statistical machine translation
261,"Most state-of-the-art phone classifiers use the same features and decision criteria for all phones, despite the fact that different broad classes are characterized by different manners and place of articulation that result in different acoustic features. This paper uses manifold learning to address structure in the acoustic space. Previous approaches to dimensionality reduction based on manifold learning assumed that the acoustic space can be characterized by a uniform manifold structure. In this paper we relax this assumption by learning different manifold structures for broad phonetic classes. Because all known classifiers make confusions between broad classes, we designed a two-level classifier in which the top level consists of a number of partially overlapping broad classes. Since the resulting classifiers are not statistically independent, we propose a new method for fusing the classifiers. Experimental results show that our two-level classifier obtained slightly better results when broad-class specific manifolds were learned, compared to a uniform manifold. However, the accuracy is still considerably lower than what could be obtained with oracle knowledge about broad class membership. From this we infer that phones do not form compact clusters in acoustic space.","['Heyun Huang', 'Yang Liu', 'Louis ten Bosch', 'Bert Cranen', 'Lou Boves']",July 2016,Computer Speech & Language,"['Manifold learning', 'Dimensionality reduction', 'Partial classification', 'Classifier fusion', 'Phone classification', 'TIMIT']",Locally learning heterogeneous manifolds for phonetic classification☆
262,"A Concept-to-Speech (CTS) system converts the conceptual representation of a sentence-to-be-spoken into speech. While some CTS systems consist of independently built text generation and Text-to-Speech (TTS) modules, the majority of the existing CTS systems enhance the connection between these two modules with a prosodic prediction module that utilizes linguistic knowledge from the text generator to predict prosodic features for TTS generation. However, knowledge embodied within the individual modules has the potential to be shared in more ways. This paper describes knowledge sharing for acoustic modelling and utterance filtering in a Mandarin CTS system. First, syntactic information generated by the text generator is propagated to a hidden Markov model (HMM) based acoustic model within the TTS module and replaces the symbolic prosodic phrasing features therein. Our experimental results show that this approach alleviates the local hard-decision problem in automatic prosodic phrasing for Mandarin CTS systems and achieves a comparable performance to the traditional approach without explicit prosodic phrasing. Second, the acoustic features of multiple synthetic utterances expressing the same input concept are utilized to evaluate the utterance candidates. With this ‘post-processing’ mechanism, our CTS system is able to filter out inferior synthetic utterances and find an acceptable candidate to express the input concept.","['Xin Wang', 'Zhen-Hua Ling', 'Li-Rong Dai']",July 2016,Computer Speech & Language,"['Concept-to-Speech', 'Speech synthesis', 'Hidden Markov model', 'Natural language generation']",Concept-to-Speech generation with knowledge sharing for acoustic modelling and utterance filtering☆
263,"A review is proposed of the impact of word representations and classification methods in the task of theme identification of telephone conversation services having highly imperfect automatic transcriptions. We firstly compare two word-based representations using the classical Term Frequency-Inverse Document Frequency with Gini purity criteria (TF-IDF-Gini) method and the latent Dirichlet allocation (LDA) approach. We then introduce a classification method that takes advantage of the LDA topic space representation, highlighted as the best word representation. To do so, two assumptions about topic representation led us to choose a Gaussian Process (GP) based method. Its performance is compared with a classical Support Vector Machine (SVM) classification method. Experiments showed that the GP approach is a better solution to deal with the multiple theme complexity of a dialogue, no matter the conditions studied (manual or automatic transcriptions) (Morchid et al., 2014). In order to better understand results obtained using different word representation methods and classification approaches, we then discuss the impact of discriminative and non-discriminative words extracted by both word representations methods in terms of transcription accuracy (Morchid et al., 2014). Finally, we propose a novel study that evaluates the impact of the Word Error Rate (WER) in the LDA topic space learning process as well as during the theme identification task. This original qualitative study points out that selecting a small subset of words having the lowest WER (instead of using all the words) allows the system to better classify automatic transcriptions with an absolute gain of 0.9 point, in comparison to the best performance achieved on this dialogue classification task (precision of 83.3%).","['Mohamed Morchid', 'Richard Dufour', 'Georges Linarès']",July 2016,Computer Speech & Language,"['Speech analytics', 'Human–human dialogue', 'Latent Dirichlet allocation', 'Topic representation', 'Principal component analysis', 'Classification performance study']",Impact of Word Error Rate on theme identification task of highly imperfect human–human conversations☆
264,"In this paper, we present an approach to multilingual Spoken Language Understanding based on a process of generalization of multiple translations, followed by a specific methodology to perform a semantic parsing of these combined translations. A statistical semantic model, which is learned from a segmented and labeled corpus, is used to represent the semantics of the task in a language. Our goal is to allow the users to interact with the system using other languages different from the one used to train the semantic models, avoiding the cost of segmenting and labeling a training corpus for each language. In order to reduce the effect of translation errors and to increase the coverage, we propose an algorithm to generate graphs of words from different translations. We also propose an algorithm to parse graphs of words with the statistical semantic model. The experimental results confirm the good behavior of this approach using French and English as input languages in a spoken language understanding task that was developed for Spanish.","['Marcos Calvo', 'Lluís-Felip Hurtado', 'Fernando Garcia', 'Emilio Sanchis', 'Encarna Segarra']",July 2016,Computer Speech & Language,"['Multilingual Language Understanding', 'Graph of words', 'Graph of concepts', 'Statistical semantic models']",Multilingual Spoken Language Understanding using graphs and multiple translations☆
265,"Previous studies have demonstrated the benefits of PLDA–SVM scoring with empirical kernel maps for i-vector/PLDA speaker verification. The method not only performs significantly better than the conventional PLDA scoring and utilizes the multiple enrollment utterances of target speakers effectively, but also opens up opportunity for adopting sparse kernel machines in PLDA-based speaker verification systems. This paper proposes taking the advantages of empirical kernel maps by incorporating them into a more advanced kernel machine called relevance vector machines (RVMs). The paper reports extensive analyses on the behaviors of RVMs and provides insight into the properties of RVMs and their applications in i-vector/PLDA speaker verification. Results on NIST 2012 SRE demonstrate that PLDA–RVM outperforms the conventional PLDA and that it achieves a comparable performance as PLDA–SVM. Results also show that PLDA–RVM is much sparser than PLDA–SVM.","['Wei Rao', 'Man-Wai Mak']",July 2016,Computer Speech & Language,"['Relevance vector machines', 'Empirical kernel maps', 'Probabilistic linear discriminant analysis', 'I-vectors', 'NIST SRE']",Sparse kernel machines with empirical kernel maps for PLDA speaker verification☆
266,"In this paper, a systematic review of relevant published studies on computer-based speech therapy systems or virtual speech therapists (VSTs) for people with speech disorders is presented. We structured this work based on the PRISMA framework. The advancements in speech technology and the increased number of successful real-world projects in this area point to a thriving market for VSTs in the near future; however, there is no standard roadmap to pinpoint how these systems should be designed, implemented, customized, and evaluated with respect to the various speech disorders. The focus of this systematic review is on articulation and phonological impairments. This systematic review addresses three research questions: what types of articulation and phonological disorders do VSTs address, how effective are virtual speech therapists, and what technological elements have been utilized in VST projects. The reviewed papers were sourced from comprehensive digital libraries, and were published in English between 2004 and 2014. All the selected studies involve computer-based intervention in the form of a VST regarding articulation or phonological impairments, followed by qualitative and/or quantitative assessments. To generate this review, we encountered several challenges. Studies were heterogeneous in terms of disorders, type and frequency of therapy, sample size, level of functionality, etc. Thus, overall conclusions were difficult to draw. Commonly, publications with rigorous study designs did not describe the technical elements used in their VST, and publications that did describe technical elements had poor study designs. Despite this heterogeneity, the selected studies reported the effectiveness of computers as a more engaging type of intervention with more tools to enrich the intervention programs, particularly when it comes to children; however, it was emphasized that virtual therapists should not drive the intervention but must be used as a medium to deliver the intervention planned by speech-language pathologists. Based on the reviewed papers, VSTs are significantly effective in training people with a variety of speech disorders; however, it cannot be claimed that a consensus exists in the superiority of VSTs over speech-language pathologists regarding rehabilitation outcomes. Our review shows that hearing-impaired cases were the most frequently addressed disorder in the reviewed studies. Automatic speech recognition, speech corpus, and speech synthesizers were the most popular technologies used in the VSTs.","['Yi-Ping Phoebe Chen', 'Caddi Johnson', 'Pooia Lalbakhsh', 'Terry Caelli', 'Guang Deng', 'David Tay', 'Shane Erickson', 'Philip Broadbridge', 'Amr El Refaie', 'Wendy Doube', 'Meg E. Morris']",May 2016,Computer Speech & Language,"['Virtual speech therapist', 'Computer-based speech therapy', 'Speech and language disorders', 'Computer-based intervention']",ReviewSystematic review of virtual speech therapists for speech disorders☆
267,"In this paper, automatic assessment models are developed for two perceptual variables: speech intelligibility and voice quality. The models are developed and tested on a corpus of Dutch tracheoesophageal (TE) speakers. In this corpus, each speaker read a text passage of approximately 300 syllables and two speech therapists provided consensus scores for the two perceptual variables. Model accuracy and stability are investigated as a function of the amount of speech that is made available for speaker assessment (clinical setting). Five sets of automatically generated acoustic-phonetic speaker features are employed as model inputs. In Part I, models taking complete feature sets as inputs are compared to models taking only the features which are expected to have sufficient support in the speech available for assessment. In Part II, the impact of phonetic content and stimulus length on the computer-generated scores is investigated. Our general finding is that a text encompassing circa 100 syllables is long enough to achieve close to asymptotic accuracy.","['Renee P. Clapham', 'Jean-Pierre Martens', 'Rob J.J.H. van Son', 'Frans J.M. Hilgers', 'Michiel M.W. van den Brekel', 'Catherine Middag']",May 2016,Computer Speech & Language,"['Laryngectomy', 'Tracheoesophageal speech', 'Automatic speech recognition', 'Speech intelligibility', 'Voice quality', 'AMPEX']",Computing scores of voice quality and speech intelligibility in tracheoesophageal speech for speech stimuli of varying lengths☆
268,"Automatic speech recognition applications can benefit from a confidence measure (CM) to predict the reliability of the output. Previous works showed that a word-dependent naïve Bayes (NB) classifier outperforms the conventional word posterior probability as a CM. However, a discriminative formulation usually renders improved performance due to the available training techniques.Taking this into account, we propose a logistic regression (LR) classifier defined with simple input functions to approximate to the NB behaviour. Additionally, as a main contribution, we propose to adapt the CM to the speaker in cases in which it is possible to identify the speakers, such as online lecture repositories.The experiments have shown that speaker-adapted models outperform their non-adapted counterparts on two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) educational lectures. They have also shown that the NB model is clearly superseded by the proposed LR classifier.","['Isaias Sanchez-Cortina', 'Jesús Andrés-Ferrer', 'Alberto Sanchis', 'Alfons Juan']",May 2016,Computer Speech & Language,"['Confidence measures', 'Speech recognition', 'Speaker adaptation', 'Log-linear models', 'Online video lectures']",Speaker-adapted confidence measures for speech recognition of video lectures☆
269,"Multi-document summarization (MDS) is becoming a crucial task in natural language processing. MDS targets to condense the most important information from a set of documents to produce a brief summary. Most existing extractive multi-document summarization methods employ different sentence selection approaches to obtain the summary as a subset of sentences from the given document set. The ability of the weighted hierarchical archetypal analysis to select “the best of the best” summary sentences motivates us to use this method in our solution to multi-document summarization tasks. In this paper, we propose a new framework for various multi-document summarization tasks based on weighted hierarchical archetypal analysis. The paper demonstrates how four variant summarization tasks, including general, query-focused, update, and comparative summarization, can be modeled as different versions acquired from the proposed framework. Experiments on summarization data sets (DUC04-07, TAC08) are conducted to demonstrate the efficiency and effectiveness of our framework for all four kinds of the multi-document summarization tasks.","['Ercan Canhasi', 'Igor Kononenko']",May 2016,Computer Speech & Language,"['Multi-document summarization framework', 'Weighted hierarchical archetypal analysis', 'General', 'Query-focused', 'Update', 'Comparative summarization']",Weighted hierarchical archetypal analysis for multi-document summarization☆
270,"Child engagement is defined as the interaction of a child with his/her environment in a contextually appropriate manner. Engagement behavior in children is linked to socio-emotional and cognitive state assessment with enhanced engagement identified with improved skills. A vast majority of studies however rely solely, and often implicitly, on subjective perceptual measures of engagement. Access to automatic quantification could assist researchers/clinicians to objectively interpret engagement with respect to a target behavior or condition, and furthermore inform mechanisms for improving engagement in various settings. In this paper, we present an engagement prediction system based exclusively on vocal cues observed during structured interaction between a child and a psychologist involving several tasks. Specifically, we derive prosodic cues that capture engagement levels across the various tasks. Our experiments suggest that a child's engagement is reflected not only in the vocalizations, but also in the speech of the interacting psychologist. Moreover, we show that prosodic cues are informative of the engagement phenomena not only as characterized over the entire task (i.e., global cues), but also in short term patterns (i.e., local cues). We perform a classification experiment assigning the engagement of a child into three discrete levels achieving an unweighted average recall of 55.8% (chance is 33.3%). While the systems using global cues and local level cues are each statistically significant in predicting engagement, we obtain the best results after fusing these two components. We perform further analysis of the cues at local and global levels to achieve insights linking specific prosodic patterns to the engagement phenomenon. We observe that while the performance of our model varies with task setting and interacting psychologist, there exist universal prosodic patterns reflective of engagement.","['Rahul Gupta', 'Daniel Bone', 'Sungbok Lee', 'Shrikanth Narayanan']",May 2016,Computer Speech & Language,"['Engagement', 'Prosody', 'Global level cues', 'Local level cues', 'Classifier decision fusion']",Analysis of engagement behavior in children during dyadic interactions using prosodic cues☆
271,"Language transfer creates a challenge for Chinese (L1) speakers in acquiring English (L2) rhythm. This appears to be a widely encountered difficulty among foreign learners of English, and is a major obstacle in acquiring a near-native oral proficiency. This paper presents a system named MusicSpeak, which strives to capitalize on musical rhythm for prosodic training in second language acquisition. This is one of the first efforts that develop an automatic procedure which can be applied to arbitrary English sentences, to cast rhythmic patterns in speech into rhythmic patterns in music. Learners can practice by speaking in synchrony with the musical rhythm. Evaluation results suggest that after practice, the learners’ speech generally achieves higher durational variability and better approximates stress-timed rhythm.","['Hao Wang', 'Peggy Mok', 'Helen Meng']",May 2016,Computer Speech & Language,"['Musical rhythm generation', 'Prosodic training', 'CALL']",Capitalizing on musical rhythm for prosodic training in computer-aided language learning☆
272,"Incremental dialogue systems are often perceived as more responsive and natural because they are able to address phenomena of turn-taking and overlapping speech, such as backchannels or barge-ins. Previous work in this area has often identified distinctive prosodic features, or features relating to syntactic or semantic completeness, as marking appropriate places of turn-taking. In a separate strand of work, psycholinguistic studies have established a connection between information density and prominence in language—the less expected a linguistic unit is in a particular context, the more likely it is to be linguistically marked. This has been observed across linguistic levels, including the prosodic, which plays an important role in predicting overlapping speech.In this article, we explore the hypothesis that information density (ID) also plays a role in turn-taking. Specifically, we aim to show that humans are sensitive to the peaks and troughs of information density in speech, and that overlapping speech at ID troughs is perceived as more acceptable than overlaps at ID peaks. To test our hypothesis, we collect human ratings for three models of generating overlapping speech based on features of: (1) prosody and semantic or syntactic completeness, (2) information density, and (3) both types of information. Results show that over 50% of users preferred the version using both types of features, followed by a preference for information density features alone. This indicates a clear human sensitivity to the effects of information density in spoken language and provides a strong motivation to adopt this metric for the design, development and evaluation of turn-taking modules in spoken and incremental dialogue systems.","['Nina Dethlefs', 'Helen Hastie', 'Heriberto Cuayáhuitl', 'Yanchao Yu', 'Verena Rieser', 'Oliver Lemon']",May 2016,Computer Speech & Language,"['Overlap', 'Turn-taking', 'Information density', 'Incremental processing', 'Spoken dialogue systems']",Information density and overlap in spoken dialogue☆
273,"Aspects of speech production have provided inspiration for ideas in speech technologies throughout the history of speech processing research. This special issue was inspired by the 2013Workshop on Speech Production in Automatic Speech Recognition in Lyon, France, and this introduction provides an overview of the included papers in the context of the current research landscape.","['Karen Livescu', 'Frank Rudzicz', 'Eric Fosler-Lussier', 'Mark Hasegawa-Johnson', 'Jeff Bilmes']",March 2016,Computer Speech & Language,"['Speech production', 'Speech recognition', 'Speech synthesis', 'Articulatory inversion', 'Articulatory features', 'Pronunciation modeling', 'Acoustic modeling', 'Acoustic features']",Speech Production in Speech Technologies: Introduction to the CSL Special Issue
274,"Hybrid deep neural network–hidden Markov model (DNN-HMM) systems have become the state-of-the-art in automatic speech recognition. In this paper we experiment with DNN-HMM phone recognition systems that use measured articulatory information. Deep neural networks are both used to compute phone posterior probabilities and to perform acoustic-to-articulatory mapping (AAM). The AAM processes we propose are based on deep representations of the acoustic and the articulatory domains. Such representations allow to: (i) create different pre-training configurations of the DNNs that perform AAM; (ii) perform AAM on a transformed (through DNN autoencoders) articulatory feature (AF) space that captures strong statistical dependencies between articulators. Traditionally, neural networks that approximate the AAM are used to generate AFs that are appended to the observation vector of the speech recognition system. Here we also study a novel approach (AAM-based pretraining) where a DNN performing the AAM is instead used to pretrain the DNN that computes the phone posteriors. Evaluations on both the MOCHA-TIMIT msak0 and the mngu0 datasets show that: (i) the recovered AFs reduce phone error rate (PER) in both clean and noisy speech conditions, with a maximum 10.1% relative phone error reduction in clean speech conditions obtained when autoencoder-transformed AFs are used; (ii) AAM-based pretraining could be a viable strategy to exploit the available small articulatory datasets to improve acoustic models trained on large acoustic-only datasets.","['Leonardo Badino', 'Claudia Canevari', 'Luciano Fadiga', 'Giorgio Metta']",March 2016,Computer Speech & Language,"['DNN-HMM', 'Acoustic-to-articulatory mapping', 'Deep neural networks', 'Acoustic modeling', 'Electromagnetic articulography', 'Autoencoders']",Integrating articulatory data in deep neural network-based acoustic modeling☆
275,"We propose a practical, feature-level and score-level fusion approach by combining acoustic and estimated articulatory information for both text independent and text dependent speaker verification. From a practical point of view, we study how to improve speaker verification performance by combining dynamic articulatory information with the conventional acoustic features. On text independent speaker verification, we find that concatenating articulatory features obtained from measured speech production data with conventional Mel-frequency cepstral coefficients (MFCCs) improves the performance dramatically. However, since directly measuring articulatory data is not feasible in many real world applications, we also experiment with estimated articulatory features obtained through acoustic-to-articulatory inversion. We explore both feature level and score level fusion methods and find that the overall system performance is significantly enhanced even with estimated articulatory features. Such a performance boost could be due to the inter-speaker variation information embedded in the estimated articulatory features. Since the dynamics of articulation contain important information, we included inverted articulatory trajectories in text dependent speaker verification. We demonstrate that the articulatory constraints introduced by inverted articulatory features help to reject wrong password trials and improve the performance after score level fusion. We evaluate the proposed methods on the X-ray Microbeam database and the RSR 2015 database, respectively, for the aforementioned two tasks. Experimental results show that we achieve more than 15% relative equal error rate reduction for both speaker verification tasks.","['Ming Li', 'Jangwon Kim', 'Adam Lammert', 'Prasanta Kumar Ghosh', 'Vikram Ramanarayanan', 'Shrikanth Narayanan']",March 2016,Computer Speech & Language,"['Text independent speaker verification', 'Text dependent speaker verification', 'Speech production', 'Articulatory features', 'Acoustic-to-articulatory inversion']",Speaker verification based on the fusion of speech acoustics and inverted articulatory signals☆
276,"Spoken language, especially conversational speech, is characterized by great variability in word pronunciation, including many variants that differ grossly from dictionary prototypes. This is one factor in the poor performance of automatic speech recognizers on conversational speech, and it has been very difficult to mitigate in traditional phone-based approaches to speech recognition. An alternative approach, which has been studied by ourselves and others, is one based on sub-phonetic features rather than phones. In such an approach, a word's pronunciation is represented as multiple streams of phonological features rather than a single stream of phones. Features may correspond to the positions of the speech articulators, such as the lips and tongue, or may be more abstract categories such as manner and place.This article reviews our work on a particular type of articulatory feature-based pronunciation model. The model allows for asynchrony between features, as well as per-feature substitutions, making it more natural to account for many pronunciation changes that are difficult to handle with phone-based models. Such models can be efficiently represented as dynamic Bayesian networks. The feature-based models improve significantly over phone-based counterparts in terms of frame perplexity and lexical access accuracy. The remainder of the article discusses related work and future directions.","['Karen Livescu', 'Preethi Jyothi', 'Eric Fosler-Lussier']",March 2016,Computer Speech & Language,"['Speech recognition', 'Articulatory features', 'Pronunciation modeling', 'Dynamic Bayesian networks']",Articulatory feature-based pronunciation modeling☆
277,"Phonological studies suggest that the typical subword units such as phones or phonemes used in automatic speech recognition systems can be decomposed into a set of features based on the articulators used to produce the sound. Most of the current approaches to integrate articulatory feature (AF) representations into an automatic speech recognition (ASR) system are based on a deterministic knowledge-based phoneme-to-AF relationship. In this paper, we propose a novel two stage approach in the framework of probabilistic lexical modeling to integrate AF representations into an ASR system. In the first stage, the relationship between acoustic feature observations and various AFs is modeled. In the second stage, a probabilistic relationship between subword units and AFs is learned using transcribed speech data. Our studies on a continuous speech recognition task show that the proposed approach effectively integrates AFs into an ASR system. Furthermore, the studies show that either phonemes or graphemes can be used as subword units. Analysis of the probabilistic relationship captured by the parameters has shown that the approach is capable of adapting the knowledge-based phoneme-to-AF representations using speech data; and allows different AFs to evolve asynchronously.","['Ramya Rasipuram', 'Mathew Magimai.-Doss']",March 2016,Computer Speech & Language,"['Automatic speech recognition', 'Articulatory features', 'Probabilistic lexical modeling', 'Kullback–Leibler divergence based hidden Markov model', 'Phoneme subword units', 'Grapheme subword units']",Articulatory feature based continuous speech recognition using probabilistic lexical modeling☆
278,"The conventional approach for data-driven articulatory synthesis consists of modeling the joint acoustic-articulatory distribution with a Gaussian mixture model (GMM), followed by a post-processing step that optimizes the resulting acoustic trajectories. This final step can significantly improve the accuracy of the GMM frame-by-frame mapping but is computationally intensive and requires that the entire utterance be synthesized beforehand, making it unsuited for real-time synthesis. To address this issue, we present a deep neural network (DNN) articulatory synthesizer that uses a tapped-delay input line, allowing the model to capture context information in the articulatory trajectory without the need for post-processing. We characterize the DNN as a function of the context size and number of hidden layers, and compare it against two GMM articulatory synthesizers, a baseline model that performs a simple frame-by-frame mapping, and a second model that also performs trajectory optimization. Our results show that a DNN with a 60-ms context window and two 512-neuron hidden layers can synthesize speech at four times the frame rate – comparable to frame-by-frame mappings, while improving the accuracy of trajectory optimization (a 9.8% reduction in Mel Cepstral distortion). Subjective evaluation through pairwise listening tests also shows a strong preference toward the DNN articulatory synthesizer when compared to GMM trajectory optimization.","['Sandesh Aryal', 'Ricardo Gutierrez-Osuna']",March 2016,Computer Speech & Language,"['Articulatory synthesis', 'Electromagnetic articulography', 'Deep learning', 'Gaussian mixture models']",Data driven articulatory synthesis with deep neural networks☆
279,"This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a ‘phonetically-informed’ version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants.","['Thomas Hueber', 'Gérard Bailly']",March 2016,Computer Speech & Language,"['Silent speech interface', 'GMM', 'HMM', 'Ultrasound', 'Articulatory–acoustic mapping']",Statistical conversion of silent articulation into audible speech using full-covariance HMM☆
280,"In this paper, a new statistical method for detecting bilabial closure gestures is proposed based on articulatory data. This can be surprisingly challenging, since mere proximity of the lips does not imply their involvement in a directed phonological goal. This segment-based bilabial closure detection scheme uses principal differential analysis (PDA) to extract articulatory gestures. The dynamic patterns of the tract variables (TVs) lip aperture, lip protrusion, and their derivatives, are captured with PDA and used to detect and quantify bilabial closure gestures. The proposed feature sets, which are optimized using sequential forward floating selection (SFFS), are combined and used in binary classification. Experimental results using the articulatory database MOCHA-TIMIT show the effectiveness of the proposed method demonstrating promising performance in terms of high classification accuracy (95%), sensitivity (95%), and specificity (95%).","['Farook Sattar', 'Frank Rudzicz']",March 2016,Computer Speech & Language,"['Bilabial closure gesture', 'Principal differential analysis (PDA)', 'Articulatory data', 'Tract variables', 'Classification']",Principal differential analysis for detection of bilabial closure gestures from articulatory data☆
281,"Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data.One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach.To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change.The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation.Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production.","['Samuel Silva', 'António Teixeira']",March 2016,Computer Speech & Language,"['Vocal tract analysis', 'Quantitative comparison', 'RT-MRI']",Quantitative systematic analysis of vocal tract data☆
282,"How the speech production and perception systems evolved in humans still remains a mystery today. Previous research suggests that human auditory systems are able, and have possibly evolved, to preserve maximal information about the speaker's articulatory gestures. This paper attempts an initial step toward answering the complementary question of whether speakers’ articulatory mechanisms have also evolved to produce sounds that can be optimally discriminated by the listener's auditory system. To this end we explicitly model, using computational methods, the extent to which derived representations of “primitive movements” of speech articulation can be used to discriminate between broad phone categories. We extract interpretable spatio-temporal primitive movements as recurring patterns in a data matrix of human speech articulation, i.e., representing the trajectories of vocal tract articulators over time. To this end, we propose a weakly-supervised learning method that attempts to find a part-based representation of the data in terms of recurring basis trajectory units (or primitives) and their corresponding activations over time. For each phone interval, we then derive a feature representation that captures the co-occurrences between the activations of the various bases over different time-lags. We show that this feature, derived entirely from activations of these primitive movements, is able to achieve a greater discrimination relative to using conventional features on an interval-based phone classification task. We discuss the implications of these findings in furthering our understanding of speech signal representations and the links between speech production and perception systems.","['Vikram Ramanarayanan', 'Maarten Van Segbroeck', 'Shrikanth S. Narayanan']",March 2016,Computer Speech & Language,"['Speech communication', 'Movement primitives', 'Phone classification', 'Motor theory', 'Information transfer']",Directly data-derived articulatory gesture-like representations retain discriminatory information about phone categories☆
283,"This paper describes an optimal algorithm using continuous state Hidden Markov Models for solving the HMS decoding problem, which is the problem of recovering an underlying sequence of phonetic units from measurements of smoothly varying acoustic features, thus inverting the speech generation process described by Holmes, Mattingly and Shearme in a well known paper (Speech synthesis by rule. Lang. Speech 7 (1964)).","['Colin Champion', 'S.M. Houghton']",March 2016,Computer Speech & Language,"['Speech recognition', 'Hidden Markov Model', 'Recognition by synthesis']",Application of continuous state Hidden Markov Models to a classical problem in speech recognition☆
284,"The linear source-filter model of speech production assumes that the source of the speech sounds is independent of the filter. However, acoustic simulations based on the physical speech production models show that when the fundamental frequency of the source harmonics approaches the first formant of the vocal tract filter, the filter has significant effects on the source due to the nonlinear coupling between them. In this study, two interactive system models are proposed under the quasi steady Bernoulli flow and linear vocal tract assumptions. An algorithm is developed to estimate the model parameters. Glottal flow and the linear vocal tract parameters are found by conventional methods. Rosenberg model is used to synthesize the glottal waveform. A recursive optimization method is proposed to find the parameters of the interactive model. Finally, glottal flow produced by the nonlinear interactive system is computed. The experimental results show that the interactive system model produces fine details of glottal flow source accurately.","['Turgay Koc', 'Tolga Ciloglu']",March 2016,Computer Speech & Language,"['Speech production', 'Source-filter theory', 'Source-filter interaction', 'Speech modeling']",Nonlinear interactive source-filter models for speech☆
285,"Spectral imputation and classifier modification can be counted as the two main missing data approaches for robust automatic speech recognition (ASR). Despite their potentials, little attention has been paid to the classifier modification techniques. In this paper, we show that transferring bounded marginalization, which is a classifier modification method, from spectral to cepstral domain would be beneficial for robust ASR. We also propose improved solutions on this transfer toward a better performance. Two such techniques are presented. The first approach still does not need training of any extra model. It benefits from an observed characteristic of cepstral features and raises accuracy of previously proposed method to a comparable level with that of a classic imputation method. The second technique combines our originally proposed method with an imputation technique but replaces spectral reconstruction with a simpler and faster possible range estimation of missing components. We show that the resulting method improves the accuracies of either of the two combined methods. The proposed techniques also show good robustness when implemented with an inaccurate spectrographic mask.","['Kian Ebrahim Kafoori', 'Seyed Mohammad Ahadi']",March 2016,Computer Speech & Language,"['Automatic speech recognition', 'Missing data theory', 'Noise robustness', 'Cepstral analysis']",Bounded cepstral marginalization of missing data for robust speech recognition☆
286,"Discriminative criteria have been widely used for training acoustic models for automatic speech recognition (ASR). Many discriminative criteria have been proposed including maximum mutual information (MMI), minimum phone error (MPE), and boosted MMI (BMMI). Discriminative training is known to provide significant performance gains over conventional maximum-likelihood (ML) training. However, as discriminative criteria aim at direct minimization of the classification error, they strongly rely on having accurate reference labels. Errors in the reference labels directly affect the performance. Recently, the differenced MMI (dMMI) criterion has been proposed for generalizing conventional criteria such as BMMI and MPE. dMMI can approach BMMI or MPE if its hyper-parameters are properly set. Moreover, dMMI introduces intermediate criteria that can be interpreted as smoothed versions of BMMI or MPE. These smoothed criteria are robust to errors in the reference labels. In this paper, we demonstrate the effect of dMMI on unsupervised speaker adaptation where the reference labels are estimated from a first recognition pass and thus inevitably contain errors. In particular, we introduce dMMI-based linear regression (dMMI-LR) adaptation and demonstrate significant gains in performance compared with MLLR and BMMI-LR in two large vocabulary lecture recognition tasks.","['Marc Delcroix', 'Atsunori Ogawa', 'Seong-Jun Hahm', 'Tomohiro Nakatani', 'Atsushi Nakamura']",March 2016,Computer Speech & Language,"['Discriminative criterion', 'Differenced maximum mutual information', 'Speech recognition', 'Acoustic model adaptation', 'Unsupervised adaptation']",Differenced maximum mutual information criterion for robust unsupervised acoustic model adaptation☆
287,"A multi-component emotion model is proposed to describe the affective states comprehensively and provide more details about emotion for the application of expressive speech synthesis. Four types of components from different perspectives – cognitive appraisal, psychological feeling, physical response and utterance manner are involved. And the interactions among them are also considered, by which the four components constitute a multi-layered structure. Based on the describing model, a detecting method is proposed to extract the affective states from text, as it is the requisite first step for an automatic generation of expressive synthetic speech. The deep stacking network is adopted and integrated with the hypothetic producing process of the four components, by which the intermediate layers of the network become visible and explicable. In addition, the affective states at document level and paragraph level are regarded as contextual features to extend available information for the emotion detection at sentence level. The effectiveness of the proposed method is validated through experiments. At sentence level, a 0.59 F-value of the predictions of utterance manner is achieved.","['Yingying Gao', 'Weibin Zhu']",March 2016,Computer Speech & Language,"['Multi-component emotion model', 'Deep stacking network', 'Visible intermediate layers', 'Contextual impact', 'Expressive speech synthesis']",Detecting affective states from text based on a multi-component emotion model☆
288,"In speech enhancement, Gaussian Mixture Models (GMMs) can be used to model the Probability Density Function (PDF) of the Periodograms of speech and different noise types. These GMMs are created by applying the Estimate Maximization (EM) algorithm on large datasets of speech and different noise type Periodograms and hence classify them into a small number of clusters whose centroid Periodograms are the mean vectors of the GMMs. These GMMs are used to realize the Maximum A-Posteriori (MAP) estimation of the speech and noise Periodograms present in a noisy speech observation. To realize the MAP estimation, use of a constrained optimization algorithm is proposed in which relatively good enhancement results with high processing times are attained. Due to the use of constraints in the optimization algorithm, incorrect estimation results may arise due to possible local maxima. A simple analytic MAP algorithm is proposed to attain global maximums in lower calculation times. With the new method the complicated MAP formula is simplified as much as possible to find the maxima, through solving a set of equations and not through conventional numerical methods used in optimization. This method results in excellent speech enhancement with a relatively short processing time.","['Sarang Chehrehsa', 'Tom James Moir']",March 2016,Computer Speech & Language,"['Gaussian Mixture Model (GMM)', 'Maximum A-Posteriori (MAP)', 'Wiener filter', 'Speech enhancement']",Speech enhancement using Maximum A-Posteriori and Gaussian Mixture Models for speech and noise Periodogram estimation☆
289,"Non-verbal communication involves encoding, transmission and decoding of non-lexical cues and is realized using vocal (e.g. prosody) or visual (e.g. gaze, body language) channels during conversation. These cues perform the function of maintaining conversational flow, expressing emotions, and marking personality and interpersonal attitude. In particular, non-verbal cues in speech such as paralanguage and non-verbal vocal events (e.g. laughters, sighs, cries) are used to nuance meaning and convey emotions, mood and attitude. For instance, laughters are associated with affective expressions while fillers (e.g. um, ah, um) are used to hold floor during a conversation. In this paper we present an automatic non-verbal vocal events detection system focusing on the detect of laughter and fillers. We extend our system presented during Interspeech 2013 Social Signals Sub-challenge (that was the winning entry in the challenge) for frame-wise event detection and test several schemes for incorporating local context during detection. Specifically, we incorporate context at two separate levels in our system: (i) the raw frame-wise features and, (ii) the output decisions. Furthermore, our system processes the output probabilities based on a few heuristic rules in order to reduce erroneous frame-based predictions. Our overall system achieves an Area Under the Receiver Operating Characteristics curve of 95.3% for detecting laughters and 90.4% for fillers on the test set drawn from the data specifications of the Interspeech 2013 Social Signals Sub-challenge. We perform further analysis to understand the interrelation between the features and obtained results. Specifically, we conduct a feature sensitivity analysis and correlate it with each feature's stand alone performance. The observations suggest that the trained system is more sensitive to a feature carrying higher discriminability with implications towards a better system design.","['Rahul Gupta', 'Kartik Audhkhasi', 'Sungbok Lee', 'Shrikanth Narayanan']",March 2016,Computer Speech & Language,"['Paralinguistic event', 'Laughter', 'Filler', 'Probability smoothing', 'Probability masking']",Detecting paralinguistic events in audio stream using context in features and probabilistic decisions☆
290,"Many under-resourced languages such as Arabic diglossia or Hindi sub-dialects do not have sufficient in-domain text to build strong language models for use with automatic speech recognition (ASR). Semi-supervised language modeling uses a speech-to-text system to produce automatic transcripts from a large amount of in-domain audio typically to augment a small amount of manual transcripts. In contrast to the success of semi-supervised acoustic modeling, conventional language modeling techniques have provided only modest gains. This paper first explains the limitations of back-off language models due to their dependence on long-span n-grams, which are difficult to accurately estimate from automatic transcripts. From this analysis, we motivate a more robust use of the automatic counts as a prior over the estimated parameters of a log-linear language model. We demonstrate consistent gains for semi-supervised language models across a range of low-resource conditions.","['Scott Novotney', 'Richard Schwartz', 'Sanjeev Khudanpur']",March 2016,Computer Speech & Language,"['Language modeling', 'Automatic speech recognition', 'LVCSR', 'Low-resource']",Getting more from automatic transcripts for semi-supervised language modeling☆
291,"Due to the increasing aging population in modern society and to the proliferation of smart devices, there is a need to enhance speech recognition among smart devices in order to make information easily accessible to the elderly as it is to the younger population. In general, speech recognition systems are optimized to an average adult's voice and tend to exhibit a lower accuracy rate when recognizing an elderly person's voice, due to the effects of speech articulation and speaking style. Additional costs are bound to be incurred when adding modifications to current speech recognitions systems for better speech recognition among elderly users. Thus, using a preprocessing application on a smart device can not only deliver better speech recognition but also substantially reduce any added costs. Audio samples of 50 words uttered by 80 elderly and young adults were collected and comparatively analyzed. The speech patterns of the elderly have a slower speech rate with longer inter-syllabic silence length and slightly lower speech intelligibility. The speech recognition rate for elderly adults could be improved by means of increasing the speech rate, adding a 1.5% increase in accuracy, eliminating silence periods, adding another 4.2% increase in accuracy, and boosting the energy of the formant frequency bands for a 6% boost in accuracy. After all the preprocessing, a 12% increase in the accuracy of elderly speech recognition was achieved. Through this study, we show that speech recognition of elderly voices can be improved through modifying specific aspects of differences in speech articulation and speaking style. In the future, we will conduct studies on methods that can precisely measure and adjust speech rate and find additional factors that impact intelligibility.","['Soonil Kwon', 'Sung-Jae Kim', 'Joon Yeon Choeh']",March 2016,Computer Speech & Language,"['Elderly voice interface', 'Speech recognition', 'Aging society']",Preprocessing for elderly speech recognition of smart devices☆
292,"Stridence as a form of speech disorder in Serbian language is manifested by the appearance of an intense and sharp whistling. Its acoustic characteristics significantly affect the quality of verbal communication. Although various forms of stridence manifestation are successfully diagnosed by speech therapists, there is a need for the automatic detection and evaluation of stridence. In this paper, an algorithm for stridence detection using Patterson's auditory model is presented. The algorithm consists of three processing stages. In the first stage spectral analysis and masking effects are applied using Paterson's auditory model. In the second stage a contour of spectral peaks that best fits characteristic features of the stridence is selected in the time-frequency (TF) representation of the signal obtained by Patterson's auditory model. In the third stage hypothesis testing is performed with three decisions: D0 – no stridence, D1 – stridence, and D2 – unable to decide. The reliability of stridence detection is tested on the speech corpus of 16 speakers without stridence (with correct speech), 16 speakers without stridence but with some other speech sound disorders, and 16 speakers with stridence. Test results show high correspondence of subjective measures and automatic detection.","['Ružica Bilibajkić', 'Zoran Šarić', 'Slobodan T. Jovičić', 'Silvana Punišić', 'Miško Subotić']",March 2016,Computer Speech & Language,"['Speech pathology', 'Stridence', 'Pathology detection', 'Auditory model']",Automatic detection of stridence in speech using the auditory model☆
293,"The field of Cross-Language Information Retrieval relates techniques close to both the Machine Translation and Information Retrieval fields, although in a context involving characteristics of its own. The present study looks to widen our knowledge about the effectiveness and applicability to that field of non-classical translation mechanisms that work at character n-gram level. For the purpose of this study, an n-gram based system of this type has been developed. This system requires only a bilingual machine-readable dictionary of n-grams, automatically generated from parallel corpora, which serves to translate queries previously n-grammed in the source language. n-Gramming is then used as an approximate string matching technique to perform monolingual text retrieval on the set of n-grammed documents in the target language.The tests for this work have been performed on CLEF collections for seven European languages, taking English as the target language. After an initial tuning phase in order to analyze the most effective way for its application, the results obtained, close to the upper baseline, not only confirm the consistency across languages of this kind of character n-gram based approaches, but also constitute a further proof of their validity and applicability, these not being tied to a given implementation.","['Jesús Vilares', 'Manuel Vilares', 'Miguel A. Alonso', 'Michael P. Oakes']",March 2016,Computer Speech & Language,"['Cross-Language Information Retrieval', 'Character n-grams', 'Alignment algorithms for Machine Translation']",On the feasibility of character n-grams pseudo-translation for Cross-Language Information Retrieval tasks☆
294,,"['Adrian-Horia Dediu', 'Carlos Martín-Vide', 'Ruslan Mitkov']",January 2016,Computer Speech & Language,[],"PrefaceFirst International Conference on Statistical Language and Speech Processing, SLSP 2013"
295,"Recent years have seen rapid growth in the deployment of statistical methods for computational language and speech processing. The current popularity of such methods can be traced to the convergence of several factors, including the increasing amount of data now accessible, sustained advances in computing power and storage capabilities, and ongoing improvements in machine learning algorithms. The purpose of this contribution is to review the state of the art in both areas, point out the top trends in statistical modelling across a wide range of problems, and identify their most salient characteristics. The paper concludes with some prognostications regarding the likely impact on the field going forward.","['Jerome R. Bellegarda', 'Christof Monz']",January 2016,Computer Speech & Language,"['Statistical methods for language processing', 'Speech processing']",State of the art in statistical methods for language and speech processing☆
296,"Probabilistic approaches are now widespread in most natural language processing applications and selection of a particular approach usually depends on the task at hand. Targeting speech semantic interpretation in a multilingual context, this paper presents a comparison between the state-of-the-art methods used for machine translation and speech understanding. This comparison justifies our proposition of a unified framework for both tasks based on a discriminative approach. We demonstrate that this framework can be used to perform a joint translation-understanding decoding which allows to combine, in the same process, translation and semantic tagging scores of a sentence. A cascade of finite-state transducers is used to compose the translation and understanding hypothesis graphs (1-bests, word graphs or confusion networks). Not only this proposition is competitive with the state-of-the-art techniques but also its framework is even more attractive as it can be generalized to other components of human–machine vocal interfaces (e.g. speech recognizer) so as to allow a richer transmission of information between them.","['Bassam Jabaian', 'Fabrice Lefèvre', 'Laurent Besacier']",January 2016,Computer Speech & Language,"['Multilingual speech understanding', 'Conditional random fields', 'Hypothesis graphs', 'Statistical machine translation', 'Dialogue systems']",A unified framework for translation and understanding allowing discriminative joint decoding for multilingual speech semantic interpretation☆
297,"In this paper we study the effect of different lexical resources for selecting synonyms and strategies for word sense disambiguation in a lexical simplification system for the Spanish language. The resources used for the experiments are the Spanish EuroWordNet, the Spanish Open Thesaurus and a combination of both. As for the synonym selection strategies, we have used both local and global contexts for word sense disambiguation. We present a novel evaluation framework in lexical simplification that takes into account the level of ambiguity of the word to be simplified. The evaluation compares various instances of the lexical simplification system, a gold standard, and a baseline. The paper presents an in-depth qualitative error analysis of the results.","['Horacio Saggion', 'Stefan Bott', 'Luz Rello']",January 2016,Computer Speech & Language,"['Lexical simplification', 'Spanish', 'Text simplification', 'Evaluation']",Simplifying words in context. Experiments with two lexical resources in Spanish☆
298,"Owing to the growing need of acquiring medical data from clinical records, processing such documents is an important topic in natural language processing (NLP). However, for general NLP methods to work, a proper, normalized input is required. Otherwise the system is overwhelmed by the unusually high amount of noise generally characteristic of this kind of text. The different types of this noise originate from non-standard language use: short fragments instead of proper sentences, usage of Latin words, many acronyms and very frequent misspellings.In this paper, a method is described for the automated correction of spelling errors in Hungarian clinical records. First, a word-based algorithm was implemented to generate a ranked list of correction candidates for word forms regarded as incorrect. Second, the problem of spelling correction was modelled as a translation task, where the source language is the erroneous text and the target language is the corrected one. A Statistical Machine Translation (SMT) decoder performed the task of error correction. Since no orthographically correct proofread text from this domain is available, we could not use such a corpus for training the system. Instead, the word-based system was used to create translation models. In addition, a 3-gram token-based language model was used to model lexical context. Due to the high number of abbreviations and acronyms in the texts, the behaviour of these abbreviated forms was further examined both in the case of the context-unaware word-based and the SMT-decoder-based implementations.The results show that the SMT-based method outperforms the first candidate accuracy of the word-based ranking system. However, the normalization of abbreviations should be handled as a separate task.","['Borbála Siklósi', 'Attila Novák', 'Gábor Prószéky']",January 2016,Computer Speech & Language,"['Spelling correction', 'Medical text processing', 'Agglutinating languages']",Context-aware correction of spelling errors in Hungarian medical documents☆
299,"In this paper, we study methods to discover words and extract their pronunciations from audio data for non-written and under-resourced languages. We examine the potential and the challenges of pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment. In our scenario a human translator produces utterances in the (non-written) target language from prompts in a resource-rich source language. We add the resource-rich source language prompts to help the word discovery and pronunciation extraction process. By aligning the source language words to the target language phonemes, we segment the phoneme sequences into word-like chunks. The resulting chunks are interpreted as putative word pronunciations but are very prone to alignment and phoneme recognition errors. Thus we suggest our alignment model Model 3P that is particularly designed for cross-lingual word-to-phoneme alignment. We present two different methods (source word dependent and independent clustering) that extract word pronunciations from word-to-phoneme alignments and compare them. We show that both methods compensate for phoneme recognition and alignment errors. We also extract a parallel corpus consisting of 15 different translations in 10 languages from the Christian Bible to evaluate our alignment model and error recovery methods. For example, based on noisy target language phoneme sequences with 45.1% errors, we build a dictionary for an English Bible with a Spanish Bible translation with 4.5% OOV rate, where 64% of the extracted pronunciations contain no more than one wrong phoneme. Finally, we use the extracted pronunciations in an automatic speech recognition system for the target language and report promising word error rates – given that pronunciation dictionary and language model are learned completely unsupervised and no written form for the target language is required for our approach.","['Felix Stahlberg', 'Tim Schlippe', 'Stephan Vogel', 'Tanja Schultz']",January 2016,Computer Speech & Language,"['Pronunciation dictionary', 'Non-written languages', 'Lexical language discovery', 'Under-resourced languages', 'Speech-to-speech translation', 'Word segmentation']",Word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment☆
300,"The steered response power phase transform (SRP-PHAT) is one of the widely used algorithms for sound source localization. Since it must examine a large number of candidate sound source locations, conventional SRP-PHAT approaches may not be used in real time. To overcome this problem, an effort was made previously to parallelize the SRP-PHAT on graphics processing units (GPUs). However, the full capacities of the GPU were not exploited since on-chip memory usage was not addressed. In this paper, we propose GPU-based parallel algorithms of the SRP-PHAT both in the frequency domain and time domain. The proposed methods optimize the memory access patterns of the SRP-PHAT and efficiently use the on-chip memory. As a result, the proposed methods demonstrate a speedup of 1276 times in the frequency domain and 80 times in the time domain compared to CPU-based algorithms, and 1.5 times in the frequency domain and 6 times in the time domain compared to conventional GPU-based methods.","['Taewoo Lee', 'Sukmoon Chang', 'Dongsuk Yook']",January 2016,Computer Speech & Language,"['Sound source localization', 'SRP-PHAT', 'GPUs']",Parallel SRP-PHAT for GPUs☆
301,"Speech that has been distorted by introducing spectral or temporal gaps is still perceived as continuous and complete by human listeners, so long as the gaps are filled with additive noise of sufficient intensity. When such perceptual restoration occurs, the speech is also more intelligible compared to the case in which noise has not been added in the gaps. This observation has motivated so-called ‘missing data’ systems for automatic speech recognition (ASR), but there have been few attempts to determine whether such systems are a good model of perceptual restoration in human listeners. Accordingly, the current paper evaluates missing data ASR in a perceptual restoration task. We evaluated two systems that use a new approach to bounded marginalisation in the cepstral domain, and a bounded conditional mean imputation method. Both methods model available speech information as a clean-speech posterior distribution that is subsequently passed to an ASR system. The proposed missing data ASR systems were evaluated using distorted speech, in which spectro-temporal gaps were optionally filled with additive noise. Speech recognition performance of the proposed systems was compared against a baseline ASR system, and with human speech recognition performance on the same task. We conclude that missing data methods improve speech recognition performance in a manner that is consistent with perceptual restoration in human listeners.","['Ulpu Remes', 'Ana Ramírez López', 'Lauri Juvela', 'Kalle Palomäki', 'Guy J. Brown', 'Paavo Alku', 'Mikko Kurimo']",January 2016,Computer Speech & Language,"['Automatic speech recognition', 'Missing data', 'Observation uncertainties', 'Perceptual restoration', 'Uncertainty propagation']",Comparing human and automatic speech recognition in a perceptual restoration experiment☆
302,"Probabilistic linear discriminant analysis (PLDA) with i-vectors as features has become one of the state-of-the-art methods in speaker verification. Discriminative training (DT) has proven to be effective for improving PLDA's performance but suffers more from data insufficiency than generative training (GT). In this paper, we achieve robustness against data insufficiency in DT in two ways. First, we compensate for statistical dependencies in the training data by adjusting the weights of the training trials in order for the training loss to be an accurate estimate of the expected loss. Second, we propose three constrained DT schemes, among which the best was a discriminatively trained transformation of the PLDA score function having four parameters. Experiments on the male telephone part of the NIST SRE 2010 confirmed the effectiveness of our proposed techniques. For various number of training speakers, the combination of weight-adjustment and the constrained DT scheme gave between 7% and 19% relative improvements in Cˆllr over GT followed by score calibration. Compared to another baseline, DT of all the parameters of the PLDA score function, the improvements were larger.","['Johan Rohdin', 'Sangeeta Biswas', 'Koichi Shinoda']",January 2016,Computer Speech & Language,"['Speaker verification', 'PLDA', 'Discriminative training', 'Statistically dependent training data', 'Overfitting']",Robust discriminative training against data insufficiency in PLDA-based speaker verification☆
303,"In this paper, we propose a single-channel speech enhancement method, based on the combination of the wavelet packet transform and an improved version of the principal component analysis (PCA). Our method integrates ability of PCA to de-correlate the coefficients by extracting a linear relationship with what of wavelet packet analysis to derive feature vectors used for speech enhancement. This allows us to operate with a convenient shrinkage function on these new coefficients, removing the noise without degrading the speech. Then, the enhanced speech obtained by the inverse wavelet packet transform is decomposed into three subspaces: low rank, sparse, and the remainder noise components. Finally, we calculate the components as a segregation problem. The performance evaluation shows that our method provides a higher noise reduction and a lower signal distortion even in highly noisy conditions without introducing artifacts.","['Mohamed anouar Ben messaoud', 'Aïcha Bouzid', 'Noureddine Ellouze']",January 2016,Computer Speech & Language,"['Speech enhancement', 'Wavelet packet analysis', 'Principal components analysis', 'Sparse matrix']",Speech enhancement based on wavelet packet of an improved principal component analysis☆
304,"Several modification algorithms that alter natural or synthetic speech with the goal of improving intelligibility in noise have been proposed recently. A key requirement of many modification techniques is the ability to predict intelligibility, both offline during algorithm development, and online, in order to determine the optimal modification for the current noise context. While existing objective intelligibility metrics (OIMs) have good predictive power for unmodified natural speech in stationary and fluctuating noise, little is known about their effectiveness for other forms of speech. The current study evaluated how well seven OIMs predict listener responses in three large datasets of modified and synthetic speech which together represent 396 combinations of speech modification, masker type and signal-to-noise ratio. The chief finding is a clear reduction in predictive power for most OIMs when faced with modified and synthetic speech. Modifications introducing durational changes are particularly harmful to intelligibility predictors. OIMs that measure masked audibility tend to over-estimate intelligibility in the presence of fluctuating maskers relative to stationary maskers, while OIMs that estimate the distortion caused by the masker to a clean speech prototype exhibit the reverse pattern.","['Yan Tang', 'Martin Cooke', 'Cassia Valentini-Botinhao']",January 2016,Computer Speech & Language,"['Objective intelligibility metric', 'Noise', 'Speech modifications', 'Synthetic speech']",Evaluating the predictions of objective intelligibility metrics for modified and synthetic speech☆
305,"In most of the wavelet based speech enhancement methods, it is assumed that the wavelet coefficients are independent of each other. However, investigating the joint histogram of the wavelet coefficients reveals some dependencies among them. In this regard, Sendur proposed a probability density function (pdf) that models the relation between a wavelet coefficient of image signal and its parent. Then, this pdf is utilized to propose a bivariate shrinkage function which uses the dependencies between the child–parent wavelet coefficients of Image signals to enhance the noisy images. In this paper, we intend to find wavelet structures which are more suitable for speech enhancement based on bivariate shrinkage. We show that the dependencies between the child–parent wavelet coefficients can only be modeled rather easily up to two stages of two-channel discrete wavelet transform using the Sendur's pdf. However, the bivariate shrinkage function works better in three-channel redundant wavelet filter-bank with dilation 2, since it has a joint distribution which is similar to the Sendur's pdf up to the fourth stage of decomposition for speech signals. Furthermore, we show that three-channel higher density wavelet obtained by eliminating the downsampling part of the third channel is more suitable for the bivariate shrinkage function when it is utilized for speech enhancement. Then, appropriate filter values for three-channel higher density wavelet filter-bank are found. Moreover, we propose four-channel double density discrete wavelet filter-bank which leads to some improvement in speech enhancement results. Since the probability of speech presence is higher in lower frequencies, we suggest level-dependent bivariate shrinkage. Finally, Sendur bivariate shrinkage is optimized for speech enhancement and new methods are proposed by combining former successful methods with the bivariate shrinkage function.","['Hamid Reza Tohidypour', 'Seyed Mohammad Ahadi']",January 2016,Computer Speech & Language,"['Wavelet transform', 'Speech enhancement', 'Redundant filter-banks', 'Four channel double density discrete wavelet', 'Three-channel higher density discrete wavelet', 'Bivariate wavelet shrinkage', 'Zero moments']",New features for speech enhancement using bivariate shrinkage based on redundant wavelet filter-banks
306,"This paper describes the ALISA tool, which implements a lightly supervised method for sentence-level alignment of speech with imperfect transcripts. Its intended use is to enable the creation of new speech corpora from a multitude of resources in a language-independent fashion, thus avoiding the need to record or transcribe speech data. The method is designed so that it requires minimum user intervention and expert knowledge, and it is able to align data in languages which employ alphabetic scripts. It comprises a GMM-based voice activity detector and a highly constrained grapheme-based speech aligner. The method is evaluated objectively against a gold standard segmentation and transcription, as well as subjectively through building and testing speech synthesis systems from the retrieved data. Results show that on average, 70% of the original data is correctly aligned, with a word error rate of less than 0.5%. In one case, subjective listening tests show a statistically significant preference for voices built on the gold transcript, but this is small and in other tests, no statistically significant differences between the systems built from the fully supervised training data and the one which uses the proposed method are found.","['A. Stan', 'Y. Mamiya', 'J. Yamagishi', 'P. Bell', 'O. Watts', 'R.A.J. Clark', 'S. King']",January 2016,Computer Speech & Language,"['Speech segmentation', 'Speech and text alignment', 'Grapheme acoustic models', 'Lightly supervised system', 'Imperfect transcripts']",ALISA: An automatic lightly supervised speech segmentation and alignment tool☆☆☆
307,"For summary readers, coherence is no less important than informativeness and is ultimately measured in human terms. Taking a human cognitive perspective, this paper is aimed to generate coherent summaries of narrative text by developing a cognitive model. To model coherence with a cognitive background, we simulate the long-term human memory by building a semantic network from a large corpus like Wiki and design algorithms to account for the information flow among different compartments of human memory. Proposition is the basic processing unit for the model. After processing a whole narrative in a cyclic way, our model supplies information to be used for extractive summarization on the proposition level. Experimental results on two kinds of narrative text, newswire articles and fairy tales, show the superiority of our proposed model to several representative and popular methods.","['Renxian Zhang', 'Wenjie Li', 'Naishi Liu', 'Dehong Gao']",January 2016,Computer Speech & Language,"['Cognitive modeling', 'Summarization', 'Coherence', 'Proposition extraction']",Coherent narrative summarization with a cognitive model
308,,[],January 2016,Computer Speech & Language,[],Reviewer Acknowledgement
309,"This special issue includes research articles which apply spoken language processing to robots that interact with human users through speech, possibly combined with other modalities. Robots that can listen to human speech, understand it, interact according to the conveyed meaning, and respond represent major research and technological challenges. Their common aim is to equip robots with natural interaction abilities. However, robotics and spoken language processing are areas that are typically studied within their respective communities with limited communication across disciplinary boundaries. The articles in this special issue represent examples that address the need for an increased multidisciplinary exchange of ideas.","['Heriberto Cuayáhuitl', 'Kazunori Komatani', 'Gabriel Skantze']",November 2015,Computer Speech & Language,"['Interactive robots', 'Speech localisation', 'Dialogue management', 'Multimodal interaction', 'Speech synthesis', 'Human–robot interaction']",Introduction for Speech and language for interactive robots
310,"This paper attempts to provide a state-of-the-art of sound source localization in robotics. Noticeably, this context raises original constraints—e.g. embeddability, real time, broadband environments, noise and reverberation—which are seldom simultaneously taken into account in acoustics or signal processing. A comprehensive review is proposed of recent robotics achievements, be they binaural or rooted in array processing techniques. The connections are highlighted with the underlying theory as well as with elements of physiology and neurology of human hearing.","['S. Argentieri', 'P. Danès', 'P. Souères']",November 2015,Computer Speech & Language,"['Robot audition', 'Source localization', 'Binaural audition', 'Array processing']",A survey on sound source localization in robotics: From binaural to array processing methods☆☆☆
311,"This work develops a method of estimating subspace-based direction of arrival (DOA) that uses two proposed preprocesses. The method can be used in applications that involve interactive robots to calculate the direction to a noise-contaminated signal in noisy environments. The proposed method can be divided into two parts, which are linear phase approximation and frequency bin selection. Linear phase approximation rectifies the phases of the two-channel signals that are affected by noise, and reconstructs the covariance matrix of the received signals according to the compensative phases using phase line regression. To increase the accuracy of DOA result, a method of frequency bin selection that is based on eigenvalue decomposition (EVD) is utilized to detect and filter out the noisy frequency bins of the microphone signals. The proposed techniques are adopted in a method of subspace-based DOA estimation that is called multiple signal classification (MUSIC). Experimental results reveal that the mean estimation error obtained using proposed method can be reduced by 7.61° from the conventional MUSIC method. The proposed method is compared with the covariance-based DOA method that is called the minimum variance distortionless response (MVDR). The DOA improves the mean estimation accuracy by 4.98° relative to the conventional MVDR method. The experimental results demonstrate that both subspace-based and covariance-based DOA algorithms with the proposed preprocessing method outperform the DOA estimation in detecting the direction of signal in a noisy environment.","['Sheng-Chieh Lee', 'Bo-Wei Chen', 'Jhing-Fa Wang', 'Min-Jian Liao', 'Wen Ji']",November 2015,Computer Speech & Language,"['Sound source location', 'Direction of arrival (DOA)', 'Angular correction', 'Linear phase approximation', 'Covariance matrix reconstruction', 'Frequency bin selection', 'Interactive robot', 'Human–robot interaction']",Subspace-based DOA with linear phase approximation and frequency bin selection preprocessing for interactive robots in noisy environments
312,"Autonomous human–robot interaction ultimately requires an artificial audition module that allows the robot to process and interpret a combination of verbal and non-verbal auditory inputs. A key component of such a module is the acoustic localization. The acoustic localization not only enables the robot to simultaneously localize multiple persons and auditory events of interest in the environment, but also provides input to auditory tasks such as speech enhancement and speech recognition. The use of microphone arrays in robots is an efficient and commonly applied approach to the localization problem. In this paper, moving away from simulated environments, we look at the acoustic localization under real-world conditions and limitations. Our approach proposes a series of enhancements, taking into account the imperfect frequency response of the array microphones and addressing the influence of the robot's shape and surface material. Motivated by the importance of the signal's phase information, we introduce a novel pre-processing step for enhancing the acoustic localization. Results show that the proposed approach improves the localization performance in joint noisy and reverberant conditions and allows a humanoid robot to locate multiple speakers in a real-world environment.","['Georgios Athanasopoulos', 'Werner Verhelst', 'Hichem Sahli']",November 2015,Computer Speech & Language,"['Microphone arrays', 'Acoustic localization', 'Time delay estimation', 'Steered response power', 'Phase spectrum enhancement']",Robust speaker localization for real-world robots☆
313,"In this paper, we present Scusi?, an anytime numerical mechanism for the interpretation of spoken referring expressions. Our contributions are: (1) an anytime interpretation process that considers multiple alternatives at different interpretation stages (speech, syntax, semantics and pragmatics), which enables Scusi? to defer decisions to the end of the interpretation process; (2) a mechanism that combines scores associated with the output of the different interpretation stages, taking into account the uncertainty arising from a variety of sources, such as ambiguity or inaccuracy in a description, speech recognition errors and out-of-vocabulary terms; and (3) distance-based functions with probabilistic semantics that represent lexical similarity between objects’ names and similarity between stated requirements and physical properties of objects (viz colour, size and positional relations). We considered two approaches for combining these descriptive attributes, viz multiplicative and additive, and determined whether prioritizing certain interpretation stages and descriptive attributes affects interpretation performance. We conducted two experiments to evaluate different aspects of Scusi?'s performance: Interpretive, where we compared Scusi?'s understanding of descriptions that are mainly ambiguous or inaccurate with people's understanding of these descriptions, and Generative, where we assessed Scusi?'s understanding of naturally occurring spoken descriptions. Our results show that Scusi?'s understanding of the descriptions in the Interpretive trial is comparable to that of people; and that its performance is encouraging when given arbitrary spoken descriptions in diverse scenarios, and excellent for the corresponding written descriptions. In both experiments, Scusi? significantly outperformed a baseline system that maintains only top same-score interpretations.","['Ingrid Zukerman', 'Su Nam Kim', 'Thomas Kleinbauer', 'Masud Moshtaghi']",November 2015,Computer Speech & Language,"['Spoken language understanding', 'Numerical approach', 'Semantic interpretation', 'Distance-based semantics', 'Performance evaluation']",Employing distance-based semantics to interpret spoken referring expressions☆☆☆
314,"In this paper, we address issues in situated language understanding in a moving car, which has the additional challenge of being a rapidly changing environment. More specifically, we propose methods for understanding user queries regarding specific target buildings in their surroundings. Unlike previous studies on physically situated interactions, such as interactions with mobile robots, the task at hand is very time sensitive because the spatial relationship between the car and target changes while the user is speaking. We collected situated utterances from drivers using our research system called Townsurfer, which was embedded in a real vehicle. Based on this data, we analyzed the timing of user queries, the spatial relationships between the car and the targets, the head pose of the user, and linguistic cues. Based on this analysis, we further propose methods to optimize timing and spatial distances and to make use of linguistic cues. Finally, we demonstrate that our algorithms improved the target identification rate by 24.1% absolute.","['Teruhisa Misu', 'Antoine Raux', 'Rakesh Gupta', 'Ian Lane']",November 2015,Computer Speech & Language,"['Spoken dialog systems', 'Situated dialog', 'Language understanding', 'Reference resolution']",Situated language understanding for a spoken dialog system within vehicles☆
315,"The RoboHelper project has the goal of developing assistive robots for the elderly. One crucial component of such a robot is a multimodal dialogue architecture, since collaborative task-oriented human–human dialogue is inherently multimodal. In this paper, we focus on a specific type of interaction, Haptic-Ostensive (H-O) actions, that are pervasive in collaborative dialogue. H-O actions manipulate objects, but they also often perform a referring function.We collected 20 collaborative task-oriented human–human dialogues between a helper and an elderly person in a realistic setting. To collect the haptic signals, we developed an unobtrusive sensory glove with pressure sensors. Multiple annotations were then conducted to build the Find corpus. Supervised machine learning was applied to these annotations in order to develop reference resolution and dialogue act classification modules. Both corpus analysis, and these two modules show that H-O actions play a crucial role in interaction: models that include H-O actions, and other extra-linguistic information such as pointing gestures, perform better.For true human–robot interaction, all communicative intentions must of course be recognized in real time, not on the basis of annotated categories. To demonstrate that our corpus analysis is not an end in itself, but can inform actual human–robot interaction, the last part of our paper presents additional experiments on recognizing H-O actions from the haptic signals measured through the sensory glove. We show that even though pressure sensors are relatively imprecise and the data provided by the glove is noisy, the classification algorithms can successfully identify actions of interest within subjects.","['Lin Chen', 'Maria Javaid', 'Barbara Di Eugenio', 'Miloš Žefran']",November 2015,Computer Speech & Language,"['Haptic-Ostensive actions', 'Multimodal dialogues', 'Reference resolution', 'Dialogue act classification']",The roles and recognition of Haptic-Ostensive actions in collaborative multimodal human–human dialogues☆
316,"We present a new modelling framework for dialogue management based on the concept of probabilistic rules. Probabilistic rules are defined as structured mappings between logical conditions and probabilistic effects. They function as high-level templates for probabilistic graphical models and may include unknown parameters whose values are estimated from data using Bayesian inference. Thanks to their use of logical abstractions, probabilistic rules are able to encode the probability and utility models employed in dialogue management in a compact and human-readable form. As a consequence, they can reduce the amount of dialogue data required for parameter estimation and allow system designers to directly incorporate their expert domain knowledge into the dialogue models.Empirical results of a user evaluation in a human–robot interaction task with 37 participants show that a dialogue manager structured with probabilistic rules outperforms both purely hand-crafted and purely statistical methods on a range of subjective and objective quality metrics. The framework is implemented in a software toolkit called OpenDial, which can be used to develop various types of dialogue systems based on probabilistic rules.",['Pierre Lison'],November 2015,Computer Speech & Language,"['Spoken dialogue systems', 'Dialogue management', 'Probabilistic graphical models', 'Bayesian inference', 'Human–robot interaction']",A hybrid approach to dialogue management based on probabilistic rules☆
317,"This paper investigates some conditions under which polarized user appraisals gathered throughout the course of a vocal interaction between a machine and a human can be integrated in a reinforcement learning-based dialogue manager. More specifically, we discuss how this information can be cast into socially-inspired rewards for speeding up the policy optimisation for both efficient task completion and user adaptation in an online learning setting. For this purpose a potential-based reward shaping method is combined with a sample efficient reinforcement learning algorithm to offer a principled framework to cope with these potentially noisy interim rewards. The proposed scheme will greatly facilitate the system's development by allowing the designer to teach his system through explicit positive/negative feedbacks given as hints about task progress, in the early stage of training. At a later stage, the approach will be used as a way to ease the adaptation of the dialogue policy to specific user profiles. Experiments carried out using a state-of-the-art goal-oriented dialogue management framework, the Hidden Information State (HIS), support our claims in two configurations: firstly, with a user simulator in the tourist information domain (and thus simulated appraisals), and secondly, in the context of man–robot dialogue with real user trials.","['Emmanuel Ferreira', 'Fabrice Lefèvre']",November 2015,Computer Speech & Language,"['Human–robot interaction', 'POMDP-based dialogue management', 'Reinforcement learning', 'Reward shaping']",Reinforcement-learning based dialogue system for human–robot interactions with socially-inspired rewards☆
318,"We address a spoken dialogue system which conducts information navigation in a style of small talk. The system uses Web news articles as an information source, and the user can receive information about the news of the day through interaction. The goal and procedure of this kind of dialogue are not well defined. An empirical approach based on a partially observable Markov decision process (POMDP) has recently been widely used for dialogue management, but it assumes a definite task goal and information slots, which does not hold in our application system. In this work, we formulate the problem of dialogue management as a selection of modules and optimize it with POMDP by tracking the dialogue state and focus of attention. The POMDP-based dialogue manager receives a user intention that is classified by a spoken language understanding (SLU) component based on logistic regression (LR). The manager also receives a user focus that is detected by the SLU component based on conditional random fields (CRFs). These dialogue states are used for selecting appropriate modules by policy function, which is optimized by reinforcement learning. The reward function is defined by the quality of interaction to encourage long interaction of information navigation with users. The module which responds to user queries is based on a similarity of predicate-argument (P-A) structures that are automatically defined from a domain corpus. It allows for flexible response generation even if the system cannot find exact matching information to the user query. The system also proactively presents information by following the user focus and retrieving a news article based on the similarity measure even if the user does not make any utterance. Experimental evaluations with real dialogue sessions demonstrate that the proposed system outperformed the conventional rule-based system in terms of dialogue state tracking and action selection. Effect of focus detection in the POMDP framework is also confirmed.","['Koichiro Yoshino', 'Tatsuya Kawahara']",November 2015,Computer Speech & Language,"['00-01', '99-00', 'Spoken dialogue system', 'Dialogue management', 'Partially observable Markov decision process (POMDP)', 'Focus in dialogue']",Conversational system for information navigation based on POMDP with user focus tracking☆
319,"This paper proposes an emotion transplantation method capable of modifying a synthetic speech model through the use of CSMAPLR adaptation in order to incorporate emotional information learned from a different speaker model while maintaining the identity of the original speaker as much as possible. The proposed method relies on learning both emotional and speaker identity information by means of their adaptation function from an average voice model, and combining them into a single cascade transform capable of imbuing the desired emotion into the target speaker. This method is then applied to the task of transplanting four emotions (anger, happiness, sadness and surprise) into 3 male speakers and 3 female speakers and evaluated in a number of perceptual tests. The results of the evaluations show how the perceived naturalness for emotional text significantly favors the use of the proposed transplanted emotional speech synthesis when compared to traditional neutral speech synthesis, evidenced by a big increase in the perceived emotional strength of the synthesized utterances at a slight cost in speech quality. A final evaluation with a robotic laboratory assistant application shows how by using emotional speech we can significantly increase the students’ satisfaction with the dialog system, proving how the proposed emotion transplantation system provides benefits in real applications.","['Jaime Lorenzo-Trueba', 'Roberto Barra-Chicote', 'Rubén San-Segundo', 'Javier Ferreiros', 'Junichi Yamagishi', 'Juan M. Montero']",November 2015,Computer Speech & Language,"['Statistical parametric speech synthesis', 'Expressive speech synthesis', 'Cascade adaptation', 'Emotion transplantation']",Emotion transplantation through adaptation in HMM-based speech synthesis☆
320,"This paper proposes a singing style control technique based on multiple regression hidden semi-Markov models (MRHSMMs) for changing singing styles and their intensities appearing in synthetic singing voices. In the proposed technique, singing styles and their intensities are represented by low-dimensional vectors called style vectors and are modeled in accordance with the assumption that mean parameters of acoustic models are given as multiple regressions of the style vectors. In the synthesis process, we can weaken or emphasize the intensities of singing styles by setting a desired style vector. In addition, the idea of pitch adaptive training is extended to the case of the MRHSMM to improve the modeling accuracy of pitch associated with musical notes. A novel vibrato modeling technique is also presented to extract vibrato parameters from singing voices that sometimes have unclear vibrato expressions. Subjective evaluations show that we can intuitively control singing styles and their intensities while maintaining the naturalness of synthetic singing voices comparable to the conventional HSMM-based singing voice synthesis.","['Takashi Nose', 'Misa Kanemoto', 'Tomoki Koriyama', 'Takao Kobayashi']",November 2015,Computer Speech & Language,"['HMM-based singing voice synthesis', 'Singing style control', 'Multiple-regression HSMM', 'Pitch adaptive training', 'Vibrato modeling']",HMM-based expressive singing voice synthesis with singing style control and robust pitch modeling☆
321,"We introduce a Bayesian approach for the adaptation of the log-linear weights present in state-of-the-art statistical machine translation systems. Typically, these weights are estimated by optimising a given translation quality criterion, taking only into account a certain set of development data (e.g., the adaptation data). In this article, we show that the Bayesian framework provides appropriate estimates of such weights in conditions where adaptation data is scarce. The theoretical framework is presented, alongside with a thorough experimentation and comparison with other weight estimation methods. We provide a comparison of different sampling strategies, including an effective heuristic strategy and a theoretically sound Markov chain Monte-Carlo algorithm. Experimental results show that Bayesian predictive adaptation (BPA) outperforms the re-estimation from scratch in conditions where adaptation data is scarce. Further analysis reveals that the improvements obtained are due to the greater stability of the estimation procedure. In addition, the proposed BPA framework has a much lower computational cost than raw re-estimation.","['Germán Sanchis-Trilles', 'Francisco Casacuberta']",November 2015,Computer Speech & Language,"['Bayesian methods', 'Adaptation', 'Natural language processing', 'Machine translation']",Improving translation quality stability using Bayesian predictive adaptation☆
322,"The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction.Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation.Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered.Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies.As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy.Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected.The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems.Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors.","['X. Bost', 'G. Senay', 'M. El-Bèze', 'R. De Mori']",November 2015,Computer Speech & Language,"['Human/human conversation analysis', 'Multi-topic identification', 'Spoken language understanding', 'Interpretation strategies']",Multiple topic identification in human/human conversations☆
323,"Text-to-speech synthesis system has been widely studied for many languages. However, speech synthesis for Arabic language has not sufficient progresses and it is still in its first stage. Statistical parametric synthesis based on hidden Markov models was the most commonly applied approach for Arabic language. Recently, synthesized speech quality based on deep neural networks was found as intelligible as human voice. This paper describes a Text-To-Speech (TTS) synthesis system for modern standard Arabic language based on statistical parametric approach and Mel-cepstral coefficients. Deep neural networks achieved state-of-the-art performance in a wide range of tasks, including speech synthesis. Our TTS system includes a diacritization system which is very important for Arabic TTS application. Our diacritization system is also based on deep neural networks. In addition to the use deep techniques, different methods were also proposed to model the acoustic parameters in order to address the problem of acoustic models accuracy. They are based on linguistic and acoustic characteristics (e.g. letter position based diacritization system, unit types based synthesis system, diacritic marks based synthesis system) and based on deep learning techniques (stacked generalization techniques). Experimental results show that our diacritization system can generate a diacritized text with high accuracy. As regards the speech synthesis system, the experimental results and subjective evaluation show that our proposed method for synthesis system can generate intelligible and natural speech.","['Ilyes Rebai', 'Yassine BenAyed']",November 2015,Computer Speech & Language,"['Text-to-speech synthesis', 'Statistical parametric', 'Deep neural networks', 'Natural language processing', 'Diacritization system']",Text-to-speech synthesis system with Arabic diacritic recognition system☆
324,"In this study we explore opinion summarization on spontaneous conversations using unsupervised and supervised approaches. We annotate a phone conversation corpus with reference extractive and abstractive summaries for a speaker's opinion on a given topic. We investigate two methods: the first is an unsupervised graph-based method, which incorporates topic and sentiment information, as well as sentence-to-sentence relations extracted based on dialogue structure; the second is a supervised method that casts the summarization problem as a classification problem. Furthermore, we investigate the use of pronoun resolution in this summarization task. We develop various features based on pronoun coreference and incorporate them in the supervised opinion summarization system. Our experimental results show that both the graph-based method and the supervised method outperform the baseline approach, and the pronoun related features can help to generate better summaries.","['Dong Wang', 'Yang Liu']",November 2015,Computer Speech & Language,"['Opinion summarization', 'Switchboard corpus', 'Sentiment analysis', 'Graph-based summarization', 'Pronoun resolution']",Opinion summarization on spontaneous conversations☆
325,"In this paper, we present a framework for facilitation robots that regulate imbalanced engagement density in a four-participant conversation as the forth participant with proper procedures for obtaining initiatives. Four is the special number in multiparty conversations. In three-participant conversations, the minimum unit for multiparty conversations, social imbalance, in which a participant is left behind in the current conversation, sometimes occurs. In such scenarios, a conversational robot has the potential to objectively observe and control situations as the fourth participant. Consequently, we present model procedures for obtaining conversational initiatives in incremental steps to harmonize such four-participant conversations. During the procedures, a facilitator must be aware of both the presence of dominant participants leading the current conversation and the status of any participant that is left behind. We model and optimize these situations and procedures as a partially observable Markov decision process (POMDP), which is suitable for real-world sequential decision processes. The results of experiments conducted to evaluate the proposed procedures show evidence of their acceptability and feeling of groupness.","['Yoichi Matsuyama', 'Iwao Akiba', 'Shinya Fujie', 'Tetsunori Kobayashi']",September 2015,Computer Speech & Language,"['Multiparty conversation', 'Facilitation robot', 'Conversational strategy', 'Engagement density control', 'Multimodal processing', 'Partially observable Markov decision process']",Four-participant group conversation: A facilitation robot controlling engagement density as the fourth participant☆
326,"Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement.","['Samuel Silva', 'António Teixeira']",September 2015,Computer Speech & Language,"['Vocal tract', 'Segmentation', 'Real-time MRI']",Unsupervised segmentation of the vocal tract from real-time MRI sequences☆
327,"A vocal tract model based on a digital waveguide is presented in which the vocal tract has been decomposed into a number of convergent and divergent ducts. The divergent duct is modeled by a 2D-featured 1D digital waveguide and the convergent duct by a one dimensional waveguide. The modeling of the divergent duct is based on splitting the volume velocity into axial and radial components. The combination of separate modeling of the divergent and convergent ducts forms the foundation of the current approach. The advantage of this approach is the ability to get a transfer function in zero-pole form that eliminates the need to perform numerical calculations on a discrete 2D mesh. In this way the present model named as a 2D-featured 1D digital waveguide model has been found to be more efficient than the standard 2D waveguide model and in very good comparison with it in the formant frequency patterns of the vowels /a/, /e/, /i/, /o/ and /u/. The model has two control parameters, the wall and glottal reflection coefficients that can be effectively employed for bandwidth tuning. The model also shows its ability to generate smooth dynamic changes in the vocal tract during the transition of vowels.","['Tahir Mushtaq Qureshi', 'Khalid Saifullah Syed']",September 2015,Computer Speech & Language,"['43.70.Bk', '43.70.Aj', '43.70.Ep', 'Waveguide', 'Vocal tract', 'Fractional delay', 'Transfer function']",Two dimensional featured one dimensional digital waveguide model for the vocal tract☆
328,"In this paper, we experiment a discriminative possibilistic classifier with a reweighting model for morphological disambiguation of Arabic texts. The main idea is to provide a possibilistic classifier that acquires automatically disambiguation knowledge from vocalized corpora and tests on non-vocalized texts. Initially, we determine all the possible analyses of vocalized words using a morphological analyzer. The values of their morphological features are exploited to train the classifier. The testing phase consists in identifying the accurate class value (i.e., a morphological feature) using the features of the preceding and the following words. The appropriate class is the one having the greatest value of a possibilistic measure computed over the training set. To discriminate the effect of each feature, we add the weights of the training attributes to this measure. To assess this approach, we carry out experiments on a corpus of Arabic stories and on the Arabic Treebank. We present results concerning all the morphological features and we discern to which degree the discriminative approach improves disambiguation rates and extract the dependency relationships among the features. The results reveal the contribution of possibility theory for resolving ambiguities in real applications. We also compare the success rates in modern versus classical Arabic texts. Finally, we try to evaluate the impact of the lexical likelihood in morphological disambiguation.","['Ibrahim Bounhas', 'Raja Ayed', 'Bilel Elayeb', 'Fabrice Evrard', 'Narjès Bellamine Ben Saoud']",September 2015,Computer Speech & Language,"['Morphological analysis', 'Morphological disambiguation', 'Discriminative possibilistic classifier', 'Reweighting model']",Experimenting a discriminative possibilistic classifier with reweighting model for Arabic morphological disambiguation☆
329,"This paper investigates three different sources of information and their integration into language modelling. Global semantics is modelled by Latent Dirichlet allocation and brings long range dependencies into language models. Word clusters given by semantic spaces enrich these language models with short range semantics. Finally, our own stemming algorithm is used to further enhance the performance of language modelling for inflectional languages.Our research shows that these three sources of information enrich each other and their combination dramatically improves language modelling. All investigated models are acquired in a fully unsupervised manner.We show the efficiency of our methods for several languages such as Czech, Slovenian, Slovak, Polish, Hungarian, and English, proving their multilingualism. The perplexity tests are accompanied by machine translation tests that prove the ability of the proposed models to improve the performance of a real-world application.","['Tomáš Brychcín', 'Miloslav Konopík']",September 2015,Computer Speech & Language,"['Language models', 'Latent Dirichlet allocation', 'Semantic spaces', 'Stemming', 'HAL', 'COALS', 'Random indexing', 'HPS', 'LDA', 'Machine translation', 'Moses']",Latent semantics in language models☆
330,"Due to the mobile Internet revolution, people tend to browse the Web while driving their car which puts the driver's safety at risk. Therefore, an intuitive and non-distractive in-car speech interface to the Web needs to be developed. Before developing a new speech dialog system (SDS) in a new domain developers have to examine the user's preferred interaction style and its influence on driving safety.This paper reports a driving simulation study, which was conducted to compare different speech-based in-car human–machine interface concepts concerning usability and driver distraction. The applied SDS prototypes were developed to perform an online hotel booking by speech while driving. The speech dialog prototypes were based on different speech dialog strategies: a command-based and a conversational dialog. Different graphical user interface (GUI) concepts (one including a human-like avatar) were designed in order to support the respective dialog strategy the most and to evaluate the effect of the GUI on usability and driver distraction.The results show that only few differences concerning speech dialog quality were found when comparing the speech dialog strategies. The command-based dialog was slightly better accepted than the conversational dialog, which seems to be due to the high concept error rate of the conversational dialog. A SDS without GUI also seems to be feasible for the driving environment and was accepted by the users. The comparison of speech dialog strategies did not reveal differences in driver distraction. However, the use of a GUI impaired the driving performance and increased gaze-based distraction. The presence of an avatar was not appreciated by participants and did not affect the dialog performance. Concerning driver distraction, the virtual agent did neither negatively affect the driving performance nor increase visual distraction.The results implicate that in-car SDS developers should take both speaking styles into consideration when designing an SDS for information exchange tasks. Furthermore, developers have to consider reducing the content presented on the screen in order to reduce driver distraction. A human-like avatar was not appreciated by users while driving. Research should further investigate if other kinds of avatars might achieve different results.","['Hansjörg Hofmann', 'Vanessa Tobisch', 'Ute Ehrlich', 'André Berton']",September 2015,Computer Speech & Language,"['Speech dialog system', 'Driving simulation study', 'Usability', 'Driver distraction']",Evaluation of speech-based HMI concepts for information exchange tasks: A driving simulator study☆
331,"This paper addresses the issue of language model adaptation for Recurrent Neural Network Language Models (rnnlms), which have recently emerged as a state-of-the-art method for language modeling in the area of speech recognition. Curriculum learning is an established machine learning approach that achieves better models by applying a curriculum, i.e., a well-planned ordering of the training data, during the learning process. Our contribution is to demonstrate the importance of curriculum learning methods for adapting rnnlms and to provide key insights on how they should be applied. rnnlms model language in a continuous space and can theoretically exploit word-dependency information over arbitrarily long distances. These characteristics give rnnlms the ability to learn patterns robustly with relatively little training data, implying that they are well suited for adaptation. In this paper, we focus on two related challenges facing language models: within-domain adaptation and limited-data within-domain adaptation. We propose three types of curricula that start with general data, i.e., characterizing the domain as a whole, and move towards specific data, i.e., characterizing the sub-domain targeted for adaptation. Effectively, these curricula result in a model that can be considered to represent an implicit interpolation between general data and sub-domain-specific data. We carry out an extensive set of experiments that investigates how adapting rnnlms using curriculum learning can improve their performance.Our first set of experiments addresses the within-domain adaptation challenge, i.e., creating models that are adapted to specific sub-domains that are part of a larger, heterogeneous domain of speech data. Under this challenge, all training data is available to the system at the time when the language model is trained. First, we demonstrate that curriculum learning can be used to create effective sub-domain-adapted rnnlms. Second, we show that a combination of sub-domain-adapted rnnlms can be used if the sub-domain of the target data is unknown at test time. Third, we explore the potential of applying combinations of sub-domain-adapted rnnlms to data for which sub-domain information is unknown at training time and must be inferred.Our second set of experiments addresses limited-data within-domain adaptation, i.e., adapting an existing model trained on a large set of data using a smaller amount of data from the target sub-domain. Under this challenge, data from the target sub-domain is not available at the time when the language model is trained, but rather becomes available little by little over time. We demonstrate that the implicit interpolation carried out by applying curriculum learning methods to rnnlms outperforms conventional interpolation and has the potential to make more of less adaptation data.","['Yangyang Shi', 'Martha Larson', 'Catholijn M. Jonker']",September 2015,Computer Speech & Language,"['Recurrent neural networks', 'Language models', 'Curriculum learning', 'Latent Dirichlet allocation', 'Topics', 'Socio-situational setting']",Recurrent neural network language model adaptation with curriculum learning☆
332,,"['José A.R. Fonollosa', 'Marta R. Costa-jussà']",July 2015,Computer Speech & Language,"['00-01', '99-00', 'Hybridization', 'Machine Translation', 'Corpus', 'Rules', 'Applications']",Editorial☆
333,"This survey on hybrid machine translation (MT) is motivated by the fact that hybridization techniques have become popular as they attempt to combine the best characteristics of highly advanced pure rule or corpus-based MT approaches. Existing research typically covers either simple or more complex architectures guided by either rule or corpus-based approaches. The goal is to combine the best properties of each type.This survey provides a detailed overview of the modification of the standard rule-based architecture to include statistical knowledge, the introduction of rules in corpus-based approaches, and the hybridization of approaches within this last single category. The principal aim here is to cover the leading research and progress in this field of MT and in several related applications.","['Marta R. Costa-jussà', 'José A.R. Fonollosa']",July 2015,Computer Speech & Language,"['00-01', '99-00', 'Hybridization', 'Machine translation', 'Corpus', 'Rules', 'Applications']",Latest trends in hybrid machine translation and its applications☆
334,"This paper explores the use of linguistic information for the selection of data to train language models. We depart from the state-of-the-art method in perplexity-based data selection and extend it in order to use word-level linguistic units (i.e. lemmas, named entity categories and part-of-speech tags) instead of surface forms. We then present two methods that combine the different types of linguistic knowledge as well as the surface forms (1, naïve selection of the top ranked sentences selected by each method; 2, linear interpolation of the datasets selected by the different methods). The paper presents detailed results and analysis for four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). The interpolation-based combination outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72–13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the English side of the same parallel dataset as for Czech and 61.90 on the same parallel dataset as for Spanish).","['Antonio Toral', 'Pavel Pecina', 'Longyue Wang', 'Josef van Genabith']",July 2015,Computer Speech & Language,"['Data selection', 'Language modelling', 'Computational linguistics']",Linguistically-augmented perplexity-based data selection for language models☆
335,"Synchronous context-free grammars (SCFGs) can be learned from parallel texts that are annotated with target-side syntax, and can produce translations by building target-side syntactic trees from source strings. Ideally, producing syntactic trees would entail that the translation is grammatically well-formed, but in reality, this is often not the case. Focusing on translation into German, we discuss various ways in which string-to-tree translation models over- or undergeneralise. We show how these problems can be addressed by choosing a suitable parser and modifying its output, by introducing linguistic constraints that enforce morphological agreement and constrain subcategorisation, and by modelling the productive generation of German compounds.","['Rico Sennrich', 'Philip Williams', 'Matthias Huck']",July 2015,Computer Speech & Language,"['Statistical machine translation', 'Syntactic translation models', 'String-to-tree models', 'Morphology']",A tree does not make a well-formed sentence: Improving syntactic string-to-tree statistical machine translation with more linguistic knowledge☆
336,"Statistical and rule-based methods are complementary approaches to machine translation (MT) that have different strengths and weaknesses. This complementarity has, over the last few years, resulted in the consolidation of a growing interest in hybrid systems that combine both data-driven and linguistic approaches. In this paper, we address the situation in which the amount of bilingual resources that is available for a particular language pair is not sufficiently large to train a competitive statistical MT system, but the cost and slow development cycles of rule-based MT systems cannot be afforded either. In this context, we formalise a new method that uses scarce parallel corpora to automatically infer a set of shallow-transfer rules to be integrated into a rule-based MT system, thus avoiding the need for human experts to handcraft these rules.Our work is based on the alignment template approach to phrase-based statistical MT, but the definition of the alignment template is extended to encompass different generalisation levels. It is also greatly inspired by the work of Sánchez-Martínez and Forcada (2009) in which alignment templates were also considered for shallow-transfer rule inference. However, our approach overcomes many relevant limitations of that work, principally those related to the inability to find the correct generalisation level for the alignment templates, and to select the subset of alignment templates that ensures an adequate segmentation of the input sentences by the rules eventually obtained. Unlike previous approaches in literature, our formalism does not require linguistic knowledge about the languages involved in the translation. Moreover, it is the first time that conflicts between rules are resolved by choosing the most appropriate ones according to a global minimisation function rather than proceeding in a pairwise greedy fashion.Experiments conducted using five different language pairs with the free/open-source rule-based MT platform Apertium show that translation quality significantly improves when compared to the method proposed by Sánchez-Martínez and Forcada (2009), and is close to that obtained using handcrafted rules. For some language pairs, our approach is even able to outperform them. Moreover, the resulting number of rules is considerably smaller, which eases human revision and maintenance.","['Víctor M. Sánchez-Cartagena', 'Juan Antonio Pérez-Ortiz', 'Felipe Sánchez-Martínez']",July 2015,Computer Speech & Language,"['Machine translation', 'Transfer rule inference', 'Hybrid machine translation']",A generalised alignment template formalism and its application to the inference of shallow-transfer machine translation rules from scarce bilingual corpora☆
337,"Despite having a research history of more than 20 years, English to Hindi machine translation often suffers badly from incorrect translations of noun compounds. The problems envisaged can be of various types, such as, the absence of proper postpositions, inappropriate word order, incorrect semantics. Different existing English to Hindi machine translation systems show their vulnerability, irrespective of the underlying technique. A potential solution to this problem lies in understanding the semantics of the noun compounds. The present paper proposes a scheme based on semantic relations to address this issue. The scheme works in three steps: identification of the noun compounds in a given text, determination of the semantic relationship(s) between them, and finally, selecting the right translation pattern. The scheme provides translation patterns for different semantic relations for 2-word noun compounds first. These patterns are used recursively to find the semantic relations and the translation patterns for 3-word and 4-word noun compounds. Frequency and probability based adjacency and dependency models are used for bracketing (grouping) the constituent words of 3-word and 4-word noun compounds into 2-word noun compounds. The semantic relations and the translation patterns generated for 2-word, 3-word and 4-word noun compounds are evaluated. The proposed scheme is compared with some well-known English to Hindi translators, viz. AnglaMT, Anuvadaksh, Bing, Google, and also with the Moses baseline system. The results obtained, show significant improvement over the Moses baseline system. Also, it performs better than the other online MT systems in terms of recall and precision.","['Renu Balyan', 'Niladri Chatterjee']",July 2015,Computer Speech & Language,"['Noun compounds', 'Semantic relation', 'Translation pattern', 'Bracketing', 'Machine translation']",Translating noun compounds using semantic relations☆
338,"Globalization has dramatically increased the need of translating information from one language to another. Frequently, such translation needs should be satisfied under very tight time constraints. Machine translation (MT) techniques can constitute a solution to this overly complex problem. However, the documents to be translated in real scenarios are often limited to a specific domain, such as a particular type of medical or legal text. This situation seriously hinders the applicability of MT, since it is usually expensive to build a reliable translation system, no matter what technology is used, due to the linguistic resources that are required to build them, such as dictionaries, translation memories or parallel texts. In order to solve this problem, we propose the application of automatic post-editing in an online learning framework. Our proposed technique allows the human expert to translate in a specific domain by using a base translation system designed to work in a general domain whose output is corrected (or adapted to the specific domain) by means of an automatic post-editing module. This automatic post-editing module learns to make its corrections from user feedback in real time by means of online learning techniques. We have validated our system using different translation technologies to implement the base translation system, as well as several texts involving different domains and languages. In most cases, our results show significant improvements in terms of BLEU (up to 16 points) with respect to the baseline systems. The proposed technique works effectively when the n-grams of the document to be translated presents a certain rate of repetition, situation which is common according to the document-internal repetition property.","['Antonio L. Lagarda', 'Daniel Ortiz-Martínez', 'Vicent Alabau', 'Francisco Casacuberta']",July 2015,Computer Speech & Language,"['Machine translation', 'Statistical machine translation', 'Interactive machine translation', 'Automatic post-editing', 'Online learning']",Translating without in-domain corpus: Machine translation post-editing with online learning techniques☆
339,"Arabic is a highly inflected language and a morpho-syntactically complex language with many differences compared to several languages that are heavily studied. It may thus require good pre-processing as it presents significant challenges for Natural Language Processing (NLP), specifically for Machine Translation (MT). This paper aims to examine how Statistical Machine Translation (SMT) can be improved using rule-based pre-processing and language analysis. We describe a hybrid translation approach coupling an Arabic–French statistical machine translation system using the Moses decoder with additional morphological rules that reduce the morphology of the source language (Arabic) to a level that makes it closer to that of the target language (French). Moreover, we introduce additional swapping rules for a structural matching between the source language and the target language. Two structural changes involving the positions of the pronouns and verbs in both the source and target languages have been attempted. The results show an improvement in the quality of translation and a gain in terms of BLEU score after introducing a pre-processing scheme for Arabic and applying these rules based on morphological variations and verb re-ordering (VS into SV constructions) in the source language (Arabic) according to their positions in the target language (French). Furthermore, a learning curve shows the improvement in terms on BLEU score under scarce- and large-resources conditions. The proposed approach is completed without increasing the amount of training data or radically changing the algorithms that can affect the translation or training engines.","['Emad Mohamed', 'Fatiha Sadat']",July 2015,Computer Speech & Language,"['Machine translation', 'Linguistic analysis', 'Arabic morphology', 'BLEU', 'Moses', 'Arabic–French Statistical Machine Translation']",Hybrid Arabic–French machine translation using syntactic re-ordering and morphological pre-processing☆
340,"Languages such as English need to be morphologically analyzed in translation into morphologically rich languages such as Persian. Analyzing the output of English to Persian machine translation systems illustrates that Persian morphology comes with many challenges especially in the verb conjugation. In this paper, we investigate three ways to deal with the morphology of Persian verb in machine translation (MT): no morphology generation in statistical MT, rule-based morphology generation in rule-based MT and a hybrid-model-independent morphology generation. By model-independent we mean that it is not based on statistical or rule-based MT and could be applied to any English to Persian MT as a post-processor. We select Google translator (translate.google.com) to show the performance of a statistical MT without any morphology generation component for the verb conjugation. Rule-based morphology generation is implemented as a part of a rule-based MT. Finally, we enrich the rule-based approach by statistical methods and information to present a hybrid model. A set of linguistically motivated features are defined using both English and Persian linguistic knowledge obtained from a parallel corpus. Then we make a model to predict six morphological features of the verb in Persian using decision tree classifier and generate an inflected verb form. In a real translation process, by applying our model to the output of Google translator and a rule-based MT as a post-processor, we achieve an improvement of about 0.7% absolute BLEU score in the best case. When we are given the gold lemma in our reference experiments, using the most common feature values as a baseline shows an improvement of almost 2.8% absolute BLEU score on a test set containing 15K sentences.","['Alireza Mahmoudi', 'Heshaam Faili']",July 2015,Computer Speech & Language,"['Persian verb morphology', 'Morphological analysis', 'Machine translation', 'SMT', 'Rule-based MT', 'Decision tree']",Using decision tree to hybrid morphology generation of Persian verb for English–Persian translation
341,"This paper proposes a new set of speech features called Locally-Normalized Cepstral Coefficients (LNCC) that are based on Seneff's Generalized Synchrony Detector (GSD). First, an analysis of the GSD frequency response is provided to show that it generates spurious peaks at harmonics of the detected frequency. Then, the GSD frequency response is modeled as a quotient of two filters centered at the detected frequency. The numerator is a triangular band pass filter centered around a particular frequency similar to the ordinary Mel filters. The denominator term is a filter that responds maximally to frequency components on either side of the numerator filter. As a result, a local normalization is performed without the spurious peaks of the original GSD. Speaker verification results demonstrate that the proposed LNCC features are of low computational complexity and far more effectively compensate for spectral tilt than ordinary MFCC coefficients. LNCC features do not require the computation and storage of a moving average of the feature values, and they provide relative reductions in Equal Error Rate (EER) as high as 47.7%, 34.0% or 25.8% when compared with MFCC, MFCC + CMN, or MFCC + RASTA in one case of variable spectral tilt, respectively.","['Victor Poblete', 'Felipe Espic', 'Simon King', 'Richard M. Stern', 'Fernando Huenupán', 'Josué Fredes', 'Nestor Becerra Yoma']",May 2015,Computer Speech & Language,"['Channel robust feature extraction', 'Auditorymodels', 'Spectral local normalization', 'Synchrony detection']",A perceptually-motivated low-complexity instantaneous linear channel normalization technique applied to speaker verification
342,"In recent years, the use of rhythm-based features in speech processing systems has received growing interest. This approach uses a wide array of rhythm metrics that have been developed to capture speech timing differences between and within languages. However, the reliability of rhythm metrics is being increasingly called into question. In this paper, we propose two modifications to this approach. First, we describe a model that is based on auditory cues that simulate the external, middle and inner parts of the ear. We evaluate this model by performing experiments to discriminate between native and non-native Arabic speech. Data are from the West Point Arabic Speech Corpus; testing is done on standard classifiers based on Gaussian Mixture Models (GMMs), Support Vector Machines (SVMs) and a hybrid GMM/SVM. Results show that the auditory-based model consistently outperforms a traditional rhythm-metric approach that includes both duration- and intensity-based metrics. Second, we propose a framework that combines the rhythm metrics and the auditory-based cues in the context of a Logistic Regression (LR) method that can optimize feature combination. Further results show that the proposed LR-based method improves performance over the standard classifiers in the discrimination between the native and non-native Arabic speech.","['S.-A. Selouani', 'Y. Alotaibi', 'W. Cichocki', 'S. Gharsellaoui', 'K. Kadi']",May 2015,Computer Speech & Language,"['Speech rhythm', 'Auditory cues', 'Duration', 'Intensity', 'SVMs', 'GMMs', 'Hybrid GMM/SVM', 'Logistic regression']",Native and non-native class discrimination using speech rhythm- and auditory-based cues☆
343,"This article assesses the impact of translation on the acquisition of vocabulary for higher-intermediate level students of English for Speakers of Other Languages (ESOL). The use of translation is a relevant issue in the research of Second Language (L2) acquisition and different authors provide arguments on both sides of the issue. Language technologies serve this issue in both the usability of automatic translation and the automatic detection of lexical phenomena. This paper will explore translation as it affects the acquisition of new words in context when students are given real texts retrieved from the Internet in a web-based interface. The students can instantly obtain the dictionary definition of a word and its translation to their native language. This platform can accurately measure how much each student relies on translation and compare this to their accuracy and fluency on a lexical retrieval task using words seen in the texts. Results show that abundant use of translation may increase accuracy in the short term, but in the longer term, it negatively affects accuracy and possibly fluency. However, students who use translation in moderation seem to benefit the most in this lexical task. This paper provides a focused and precise way to measure the relevant variables for each individual student, and new findings that contribute to our understanding of the impact of the use of translation in language learning.","['Oscar Saz', 'Yibin Lin', 'Maxine Eskenazi']",May 2015,Computer Speech & Language,"['Reading tutor', 'Vocabulary acquisition', 'Translation', 'ESOL']",Measuring the impact of translation on the accuracy and fluency of vocabulary acquisition of English☆
344,"This paper examines the individual and combined impacts of various front-end approaches on the performance of deep neural network (DNN) based speech recognition systems in distant talking situations, where acoustic environmental distortion degrades the recognition performance. Training of a DNN-based acoustic model consists of generation of state alignments followed by learning the network parameters. This paper first shows that the network parameters are more sensitive to the speech quality than the alignments and thus this stage requires improvement. Then, various front-end robustness approaches to addressing this problem are categorised based on functionality. The degree to which each class of approaches impacts the performance of DNN-based acoustic models is examined experimentally. Based on the results, a front-end processing pipeline is proposed for efficiently combining different classes of approaches. Using this front-end, the combined effects of different classes of approaches are further evaluated in a single distant microphone-based meeting transcription task with both speaker independent (SI) and speaker adaptive training (SAT) set-ups. By combining multiple speech enhancement results, multiple types of features, and feature transformation, the front-end shows relative performance gains of 7.24% and 9.83% in the SI and SAT scenarios, respectively, over competitive DNN-based systems using log mel-filter bank features.","['T. Yoshioka', 'M.J.F. Gales']",May 2015,Computer Speech & Language,"['Environmental robustness', 'Deep neural network', 'Front-end', 'Meeting transcription']",Environmentally robust ASR front-end for deep neural network acoustic models☆
345,"This paper presents uses a data-driven approach to improve Spoken Dialog System (SDS) performance by automatically finding the most appropriate terms to be used in system prompts. The literature shows that speakers use one another's terms (entrain) when trying to create common ground during a spoken dialog. Those terms are commonly called “primes”, since they influence the interlocutors’ linguistic decision-making. This approach emulates human interaction, with a system built to propose primes to the user and accept the primes that the user proposes. These primes are chosen on the fly during the interaction, based on a set of features that indicate good candidate primes. A good candidate is one that we know is easily recognized by the speech recognizer, and is also a normal word choice given the context. The system is trained to follow the user's choice of prime if system performance is not negatively affected. When system performance is affected, the system proposes a new prime. In our previous work we have shown how we can identify the prime candidates and how the system can select primes using rules. In this paper we go further, presenting a data-driven method to perform the same task. Live tests with this method show that use of on-the-fly entrainment reduces out-of-vocabulary and word error rate, and also increases the number of correctly transferred concepts.","['José Lopes', 'Maxine Eskenazi', 'Isabel Trancoso']",May 2015,Computer Speech & Language,"['Lexical entrainment', 'Spoken dialog systems', 'Data-driven model', 'Rule-based model']",From rule-based to data-driven lexical entrainment models in spoken dialog systems☆
346,,['Thomas Drugman'],March 2015,Computer Speech & Language,[],PrefaceNon-Linear Speech Processing (NOLISP 2013)
347,"Voice conversion functions based on Gaussian mixture models and parametric speech signal representations are opaque in the sense that it is not straightforward to interpret the physical meaning of the conversion parameters. Following the line of recent works based on the frequency warping plus amplitude scaling paradigm, in this article we show that voice conversion functions can be designed according to physically meaningful constraints in such manner that they become highly informative. The resulting voice conversion method can be used to visualize the differences between source and target voices or styles in terms of formant location in frequency, spectral tilt and amplitude in a number of spectral bands.","['Daniel Erro', 'Agustin Alonso', 'Luis Serrano', 'Eva Navas', 'Inma Hernaez']",March 2015,Computer Speech & Language,"['Voice conversion', 'Gaussian mixture models', 'Frequency warping', 'Amplitude scaling', 'Spectral tilt']",Interpretable parametric voice conversion functions based on Gaussian mixture models and constrained transformations
348,"After total laryngectomy, the placement of a tracheoesophageal (TE) prosthesis offers the possibility to recover a new voice. However, the quality of the resulting TE speech is known to be degraded. To assess a patient's voice, current approaches rely either on quality-of-life questionnaires or on a perceptual evaluation carried out by speech therapists. These two methods exhibit the disadvantage of being both subjective and time-consuming. In this paper, we propose a dedicated scale, called A4S, for the objective Automatic Acoustic Assessment of Alaryngeal Speech. For this purpose, we first identify the artefacts existing in TE speech. These are linked to the periodicity, regularity, high-frequency noise and gargling noise/creakiness of the signal, as well as to the speaking rate. Specific acoustic features are proposed for the characterization of each artefact. A statistical study shows that TE speakers have a significantly worse voice compared to the control group, except for the speaking rate. Based on these advances, the A4S scale is proposed. This scale is made of five normalized dimensions, related to the five identified artefacts. A given patient's phonation can then be represented by a pentagon in a radar chart, which allows a fast and intuitive visualization of the strengths and flaws of the voice. A4S can then be seen as a useful tool for speech therapists to design tailored exercises specific to the patient's voice. In addition, we show the applicability of A4S for the follow-up of patients, as well as to study the impact of the type of surgery (open neck, robot and flap reconstruction) used for total laryngectomy and of a pre-surgical radiotherapy on various aspects of the TE voice.","['Thomas Drugman', 'Myriam Rijckaert', 'Claire Janssens', 'Marc Remacle']",March 2015,Computer Speech & Language,"['Total laryngectomy', 'Tracheoesophageal speech', 'Voice rehabilitation', 'Voice assessment', 'Speech analysis']",Tracheoesophageal speech: A dedicated objective acoustic assessment☆
349,"In this paper, we propose a new front-end for Acoustic Event Classification tasks (AEC). First, we study the spectral characteristics of different acoustic events in comparison with the structure of speech spectra. Second, from the findings of this study, we propose a new parameterization for AEC, which is an extension of the conventional Mel-Frequency Cepstral Coefficients (MFCC) and is based on the high pass filtering of the acoustic event signal. The proposed front-end have been tested in clean and noisy conditions and compared to the conventional MFCC in an AEC task. Results support the fact that the high pass filtering of the audio signal is, in general terms, beneficial for the system, showing that the removal of frequencies below 100–275 Hz in the feature extraction process in clean conditions and below 400–500 Hz in noisy conditions, improves significantly the performance of the system with respect to the baseline.","['Jimmy Ludeña-Choez', 'Ascensión Gallardo-Antolín']",March 2015,Computer Speech & Language,"['Acoustic Event Classification', 'High-pass filtering', 'Auditory filterbank']",Feature extraction based on the high-pass filtering of audio signals for Acoustic Event Classification☆
350,"Alzheimer's disease (AD) is the most prevalent form of degenerative dementia; it has a high socio-economic impact in Western countries. The purpose of our project is to contribute to earlier diagnosis of AD and allow better estimates of its severity by using automatic analysis performed through new biomarkers extracted through non-invasive intelligent methods. The method selected is based on speech biomarkers derived from the analysis of spontaneous speech (SS). Thus the main goal of the present work is feature search in SS, aiming at pre-clinical evaluation whose results can be used to select appropriate tests for AD diagnosis. The feature set employed in our earlier work offered some hopeful conclusions but failed to capture the nonlinear dynamics of speech that are present in the speech waveforms. The extra information provided by the nonlinear features could be especially useful when training data is limited. In this work, the fractal dimension (FD) of the observed time series is combined with linear parameters in the feature vector in order to enhance the performance of the original system while controlling the computational cost.","['Karmele López-de-Ipiña', 'Jordi Solé-Casals', 'Harkaitz Eguiraun', 'J.B. Alonso', 'C.M. Travieso', 'Aitzol Ezeiza', 'Nora Barroso', 'Miriam Ecay-Torres', 'Pablo Martinez-Lage', 'Blanca Beitia']",March 2015,Computer Speech & Language,"['Nonlinear speech processing', ""Alzheimer's disease diagnosis"", 'Spontaneous speech', 'Fractal dimensions']",Feature selection for spontaneous speech analysis to aid in Alzheimer's disease diagnosis: A fractal dimension approach
351,"In this paper, we present a survey on the application of recurrent neural networks to the task of statistical language modeling. Although it has been shown that these models obtain good performance on this task, often superior to other state-of-the-art techniques, they suffer from some important drawbacks, including a very long training time and limitations on the number of context words that can be taken into account in practice. Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks. This paper gives an overview of the most important extensions. Each technique is described and its performance on statistical language modeling, as described in the existing literature, is discussed. Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks, applied to language modeling, but it also highlights the techniques for which further research is required.","['Wim De Mulder', 'Steven Bethard', 'Marie-Francine Moens']",March 2015,Computer Speech & Language,"['68T05', '68T10', '68T50', 'Recurrent neural networks', 'Natural language processing', 'Language modeling', 'Speech recognition', 'Machine translation']",ReviewA survey on the application of recurrent neural networks to statistical language modeling☆
352,"In this paper, the production characteristics of laughter are analysed at call and bout levels. Data of natural laughter is examined using electroglottograph (EGG) and acoustic signals. Nonspeech-laugh and laughed-speech are analysed in comparison with normal speech using features derived from the EGG and acoustic signals. Analysis of EGG signal is used to derive the average closed phase quotient in glottal cycles and the average instantaneous fundamental frequency (F0). Excitation source characteristics are analysed from the acoustic signal using a modified zero-frequency filtering (mZFF) method. Excitation impulse density and the strength of impulse-like excitation are extracted from the mZFF signal. Changes in the vocal tract system characteristics are examined in terms of the first two dominant frequencies derived using linear prediction (LP) analysis. Additional excitation information present in the acoustic signal is examined using a measure of sharpness of peaks in the Hilbert envelope of the LP residual at the glottal closure instants. Parameters representing degree of change and temporal changes in the production features are also derived to study the discriminating characteristics of laughter from normal speech. Changes are larger for nonspeech-laugh than laughed-speech, with reference to normal speech.","['Vinay Kumar Mittal', 'Bayya Yegnanarayana']",March 2015,Computer Speech & Language,"['Laughter analysis', 'Nonspeech-laugh', 'Laughed-speech', 'Modified zero-frequency filtering', 'Dominant frequency', 'Closed phase quotient', 'EGG', 'Differenced EGG']",Analysis of production characteristics of laughter☆
353,"This paper studies the relevance factor in maximum a posteriori (MAP) adaptation of Gaussian mixture model (GMM) for speaker and language recognition. Knowing that relevance factor determines how much the observed training data influence the model adaptation, thus the resulting GMM model, it is believed that more effective modeling can be achieved if the relevance factor is adaptive to the corresponding data. We therefore provide a mathematic derivation for the estimation of relevance factor. GMM supervector support vector machine (SVM) with nuisance attribute projection (NAP) (GMM–NAP–SVM) has been reported to be effective and reliable for speaker and language recognition. Being a discriminative classifier in nature, a GMM–NAP–SVM system is sensitive to the magnitude and direction of a supervector in the high dimensional space. However, when characterizing a speech utterance with GMM supervector estimated through MAP, we observe that the resulting supervector is undesirably affected by the varying duration of the utterance. We propose an adaptive relevance factor that adapts to the duration to mitigate the variability effect due to the length of utterance. We give a systematic investigation on different types of relevance factor of MAP in different applicatively platforms. We show the efficacy of the data-dependent as well as adaptive relevance factors on the National Institute of Standards and Technology (NIST) speaker recognition evaluation (SRE) 2008 and language recognition evaluation (LRE) 2009 and 2011 tasks respectively.","['Chang Huai You', 'Haizhou Li', 'Kong Aik Lee']",March 2015,Computer Speech & Language,"['Maximum a posteriori', 'Supervector', 'Gaussian mixture model', 'Support vector machine']",Relevance factor of maximum a posteriori adaptation for GMM–NAP–SVM in speaker and language recognition☆
354,"It is acknowledged that Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMMs) as the observation density functions achieve excellent digit recognition performance at high signal to noise ratios (SNRs). Moreover, many years of research have led to good techniques to reduce the impact of noise, distortion and mismatch between training and test conditions on the recognition accuracy. Nevertheless, we still await systems that are truly robust against these confounding factors. The present paper extends recent work on acoustic modeling based on Reservoir Computing (RC), a concept that has its roots in Machine Learning. By introducing a novel analysis of reservoirs as non-linear dynamical systems, new insights are gained and translated into a new reservoir design recipe that is extremely simple and highly comprehensible in terms of the dynamics of the acoustic features and the modeled acoustic units. By tuning the reservoir to these dynamics, one can create RC-based systems that not only compete well with conventional systems in clean conditions, but also degrade more gracefully in noisy conditions. Control experiments show that noise-robustness follows from the random fixation of the reservoir neurons whereas, tuning the reservoir dynamics increases the accuracy without compromising the noise-robustness.","['Azarakhsh Jalalvand', 'Fabian Triefenbach', 'Kris Demuynck', 'Jean-Pierre Martens']",March 2015,Computer Speech & Language,"['Reservoir Computing', 'Recurrent Neural Networks', 'Acoustic modeling', 'Automatic Speech Recognition', 'Noise robust spoken digit recognition']",Robust continuous digit recognition using Reservoir Computing☆
355,,"['Björn Schuller', 'Stefan Steidl', 'Anton Batliner', 'Alessandro Vinciarelli', 'Felix Burkhardt', 'Rob van Son']",January 2015,Computer Speech & Language,"['Computational Paralinguistics', 'Speaker states', 'Speaker traits', 'Personality', 'Likability', 'Pathology', 'Speaker Trait Challenge']",Introduction
356,"The INTERSPEECH 2012 Speaker Trait Challenge aimed at a unified test-bed for perceived speaker traits – the first challenge of this kind: personality in the five OCEAN personality dimensions, likability of speakers, and intelligibility of pathologic speakers. In the present article, we give a brief overview of the state-of-the-art in these three fields of research and describe the three sub-challenges in terms of the challenge conditions, the baseline results provided by the organisers, and a new openSMILE feature set, which has been used for computing the baselines and which has been provided to the participants. Furthermore, we summarise the approaches and the results presented by the participants to show the various techniques that are currently applied to solve these classification tasks.","['Björn Schuller', 'Stefan Steidl', 'Anton Batliner', 'Elmar Nöth', 'Alessandro Vinciarelli', 'Felix Burkhardt', 'Rob van Son', 'Felix Weninger', 'Florian Eyben', 'Tobias Bocklet', 'Gelareh Mohammadi', 'Benjamin Weiss']",January 2015,Computer Speech & Language,"['Computational paralinguistics', 'Personality', 'Likability', 'Pathology', 'Survey', 'Challenge']","A Survey on perceived speaker traits: Personality, likability, pathology, and the first challenge☆"
357,"Pathological speech usually refers to the condition of speech distortion resulting from atypicalities in voice and/or in the articulatory mechanisms owing to disease, illness or other physical or biological insult to the production system. Although automatic evaluation of speech intelligibility and quality could come in handy in these scenarios to assist experts in diagnosis and treatment design, the many sources and types of variability often make it a very challenging computational processing problem. In this work we propose novel sentence-level features to capture abnormal variation in the prosodic, voice quality and pronunciation aspects in pathological speech. In addition, we propose a post-classification posterior smoothing scheme which refines the posterior of a test sample based on the posteriors of other test samples. Finally, we perform feature-level fusions and subsystem decision fusion for arriving at a final intelligibility decision. The performances are tested on two pathological speech datasets, the NKI CCRT Speech Corpus (advanced head and neck cancer) and the TORGO database (cerebral palsy or amyotrophic lateral sclerosis), by evaluating classification accuracy without overlapping subjects’ data among training and test partitions. Results show that the feature sets of each of the voice quality subsystem, prosodic subsystem, and pronunciation subsystem, offer significant discriminating power for binary intelligibility classification. We observe that the proposed posterior smoothing in the acoustic space can further reduce classification errors. The smoothed posterior score fusion of subsystems shows the best classification performance (73.5% for unweighted, and 72.8% for weighted, average recalls of the binary classes).","['Jangwon Kim', 'Naveen Kumar', 'Andreas Tsiartas', 'Ming Li', 'Shrikanth S. Narayanan']",January 2015,Computer Speech & Language,"['Pathological speech', 'Automatic intelligibility assessment', 'Dysarthric speech', 'Head and neck cancer']",Automatic intelligibility classification of sentence-level pathological speech☆
358,"This study focuses on feature selection in paralinguistic analysis and presents recently developed supervised and unsupervised methods for feature subset selection and feature ranking. Using the standard k-nearest-neighbors (kNN) rule as the classification algorithm, the feature selection methods are evaluated individually and in different combinations in seven paralinguistic speaker trait classification tasks. In each analyzed data set, the overall number of features highly exceeds the number of data points available for training and evaluation, making a well-generalizing feature selection process extremely difficult. The performance of feature sets on the feature selection data is observed to be a poor indicator of their performance on unseen data. The studied feature selection methods clearly outperform a standard greedy hill-climbing selection algorithm by being more robust against overfitting. When the selection methods are suitably combined with each other, the performance in the classification task can be further improved. In general, it is shown that the use of automatic feature selection in paralinguistic analysis can be used to reduce the overall number of features to a fraction of the original feature set size while still achieving a comparable or even better performance than baseline support vector machine or random forest classifiers using the full feature set. The most typically selected features for recognition of speaker likability, intelligibility and five personality traits are also reported.","['Jouni Pohjalainen', 'Okko Räsänen', 'Serdar Kadioglu']",January 2015,Computer Speech & Language,"['Feature selection', 'Pattern recognition', 'Machine learning', 'Computational paralinguistics']","Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits☆"
359,"For several decades now, there has been sporadic interest in automatically characterizing the speech impairment due to Parkinson's disease (PD). Most early studies were confined to quantifying a few speech features that were easy to compute. More recent studies have adopted a machine learning approach where a large number of potential features are extracted and the models are learned automatically from the data. In the same vein, here we characterize the disease using a relatively large cohort of 168 subjects, collected from multiple (three) clinics. We elicited speech using three tasks – the sustained phonation task, the diadochokinetic task and a reading task, all within a time budget of 4 min, prompted by a portable device. From these recordings, we extracted 1582 features for each subject using openSMILE, a standard feature extraction tool. We compared the effectiveness of three strategies for learning a regularized regression and find that ridge regression performs better than lasso and support vector regression for our task. We refine the feature extraction to capture pitch-related cues, including jitter and shimmer, more accurately using a time-varying harmonic model of speech. Our results show that the severity of the disease can be inferred from speech with a mean absolute error of about 5.5, explaining 61% of the variance and consistently well-above chance across all clinics. Of the three speech elicitation tasks, we find that the reading task is significantly better at capturing cues than diadochokinetic or sustained phonation task. In all, we have demonstrated that the data collection and inference can be fully automated, and the results show that speech-based assessment has promising practical application in PD. The techniques reported here are more widely applicable to other paralinguistic tasks in clinical domain.","['Alireza Bayestehtashk', 'Meysam Asgari', 'Izhak Shafran', 'James McNames']",January 2015,Computer Speech & Language,"[""Parkinson's disease"", 'Pitch estimation', 'Jitter', 'Shimmer']",Fully automated assessment of the severity of Parkinson's disease from speech☆
360,"We introduce a ranking approach for emotion recognition which naturally incorporates information about the general expressivity of speakers. We demonstrate that our approach leads to substantial gains in accuracy compared to conventional approaches. We train ranking SVMs for individual emotions, treating the data from each speaker as a separate query, and combine the predictions from all rankers to perform multi-class prediction. The ranking method provides two natural benefits. It captures speaker specific information even in speaker-independent training/testing conditions. It also incorporates the intuition that each utterance can express a mix of possible emotion and that considering the degree to which each emotion is expressed can be productively exploited to identify the dominant emotion. We compare the performance of the rankers and their combination to standard SVM classification approaches on two publicly available datasets of acted emotional speech, Berlin and LDC, as well as on spontaneous emotional data from the FAU Aibo dataset. On acted data, ranking approaches exhibit significantly better performance compared to SVM classification both in distinguishing a specific emotion from all others and in multi-class prediction. On the spontaneous data, which contains mostly neutral utterances with a relatively small portion of less intense emotional utterances, ranking-based classifiers again achieve much higher precision in identifying emotional utterances than conventional SVM classifiers. In addition, we discuss the complementarity of conventional SVM and ranking-based classifiers. On all three datasets we find dramatically higher accuracy for the test items on whose prediction the two methods agree compared to the accuracy of individual methods. Furthermore on the spontaneous data the ranking and standard classification are complementary and we obtain marked improvement when we combine the two classifiers by late-stage fusion.","['Houwei Cao', 'Ragini Verma', 'Ani Nenkova']",January 2015,Computer Speech & Language,"['Emotion classification', 'Ranking models', 'Spontaneous speech', 'Acted speech', 'Speaker-sensitive']",Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech☆
361,"In this article we investigate what representations of acoustics and word usage are most suitable for predicting dimensions of affect—arousal, valance, power and expectancy—in spontaneous interactions. Our experiments are based on the AVEC 2012 challenge dataset. For lexical representations, we compare corpus-independent features based on psychological word norms of emotional dimensions, as well as corpus-dependent representations. We find that corpus-dependent bag of words approach with mutual information between word and emotion dimensions is by far the best representation. For the analysis of acoustics, we zero in on the question of granularity. We confirm on our corpus that utterance-level features are more predictive than word-level features. Further, we study more detailed representations in which the utterance is divided into regions of interest (ROI), each with separate representation. We introduce two ROI representations, which significantly outperform less informed approaches. In addition we show that acoustic models of emotion can be improved considerably by taking into account annotator agreement and training the model on smaller but reliable dataset. Finally we discuss the potential for improving prediction by combining the lexical and acoustic modalities. Simple fusion methods do not lead to consistent improvements over lexical classifiers alone but improve over acoustic models.","['Houwei Cao', 'Arman Savran', 'Ragini Verma', 'Ani Nenkova']",January 2015,Computer Speech & Language,"['Emotion', 'Affect', 'Spontaneous speech', 'Lexical features', 'Acoustics']",Acoustic and lexical representations for affect prediction in spontaneous conversations☆
362,"We examine the similarities and differences in the expression of emotion in the singing and the speaking voice. Three internationally renowned opera singers produced “vocalises” (using a schwa vowel) and short nonsense phrases in different interpretations for 10 emotions. Acoustic analyses of emotional expression in the singing samples show significant differences between the emotions. In addition to the obvious effects of loudness and tempo, spectral balance and perturbation make significant contributions (high effect sizes) to this differentiation. A comparison of the emotion-specific patterns produced by the singers in this study with published data for professional actors portraying different emotions in speech generally show a very high degree of similarity. However, singers tend to rely more than actors on the use of voice perturbation, specifically vibrato, in particular in the case of high arousal emotions. It is suggested that this may be due to by the restrictions and constraints imposed by the musical structure.","['Klaus R. Scherer', 'Johan Sundberg', 'Lucas Tamarit', 'Gláucia L. Salomão']",January 2015,Computer Speech & Language,"['Vocal expression', 'Emotional interpretation in singing', 'Comparison between emotion expression in speech and singing', 'Acoustic analyses of emotion']",Comparing the acoustic expression of emotion in the speaking and the singing voice☆
363,"This paper regards social question-and-answer (Q&A) collections such as Yahoo! Answers as knowledge repositories and investigates techniques to mine knowledge from them to improve sentence-based complex question answering (QA) systems. Specifically, we present a question-type-specific method (QTSM) that extracts question-type-dependent cue expressions from social Q&A pairs in which the question types are the same as the submitted questions. We compare our approach with the question-specific and monolingual translation-based methods presented in previous works. The question-specific method (QSM) extracts question-dependent answer words from social Q&A pairs in which the questions resemble the submitted question. The monolingual translation-based method (MTM) learns word-to-word translation probabilities from all of the social Q&A pairs without considering the question or its type. Experiments on the extension of the NTCIR 2008 Chinese test data set demonstrate that our models that exploit social Q&A collections are significantly more effective than baseline methods such as LexRank. The performance ranking of these methods is QTSM > {QSM, MTM}. The largest F3 improvements in our proposed QTSM over QSM and MTM reach 6.0% and 5.8%, respectively.","['Youzheng Wu', 'Chiori Hori', 'Hideki Kashioka', 'Hisashi Kawai']",January 2015,Computer Speech & Language,"['Complex question answering', 'Web mining', 'Summarization']",Leveraging social Q&A collections for improving complex question answering☆
364,"In this paper, we present unsupervised language model (LM) adaptation approaches using latent Dirichlet allocation (LDA) and latent semantic marginals (LSM). The LSM is the unigram probability distribution over words that are calculated using LDA-adapted unigram models. The LDA model is used to extract topic information from a training corpus in an unsupervised manner. The LDA model yields a document–topic matrix that describes the number of words assigned to topics for the documents. A hard-clustering method is applied on the document–topic matrix of the LDA model to form topics. An adapted model is created by using a weighted combination of the n-gram topic models. The stand-alone adapted model outperforms the background model. The interpolation of the background model and the adapted model gives further improvement. We modify the above models using the LSM. The LSM is used to form a new adapted model by using the minimum discriminant information (MDI) adaptation approach called unigram scaling, which minimizes the distance between the new adapted model and the other model. The unigram scaling of the adapted model using LSM yields better results over a conventional unigram scaling approach. The unigram scaling of the interpolation of the background and the adapted model using the LSM outperform the background model, the unigram scaling of the background model, the unigram scaling of the adapted model, and the interpolation of the background and the adapted models respectively. We perform experiments using the '87–89 Wall Street Journal (WSJ) corpus incorporating a multi-pass continuous speech recognition (CSR) system. In the first pass, we used the background n-gram language model for lattice generation and then we apply the LM adaptation approaches for lattice rescoring in the second pass.","['Md. Akmal Haidar', ""Douglas O'Shaughnessy""]",January 2015,Computer Speech & Language,"['Language model', 'Topic model', 'Mixture model', 'Speech recognition', 'Minimum discriminant information']",Unsupervised language model adaptation using LDA-based mixture models and latent semantic marginals☆
365,"Random Indexing based extractive text summarization has already been proposed in literature. This paper looks at the above technique in detail, and proposes several improvements. The improvements are both in terms of formation of index (word) vectors of the document, and construction of context vectors by using convolution instead of addition operation on the index vectors. Experiments have been conducted using both angular and linear distances as metrics for proximity. As a consequence, three improved versions of the algorithm, viz. RISUM, RISUM+ and MRISUM were obtained. These algorithms have been applied on DUC 2002 documents, and their comparative performance has been studied. Different ROUGE metrics have been used for performance evaluation. While RISUM and RISUM+ perform almost at par, MRISUM is found to outperform both RISUM and RISUM+ significantly. MRISUM also outperforms LSA+TRM based summarization approach. The study reveals that all the three Random Indexing based techniques proposed in this study produce consistent results when linear distance is used for measuring proximity.","['Niladri Chatterjee', 'Pramod Kumar Sahoo']",January 2015,Computer Speech & Language,"['Word Space Model', 'Random Indexing', 'PageRank', 'Convolution', 'Modified Random Indexing']",Random Indexing and Modified Random Indexing based approach for extractive text summarization☆
366,"Recently, re-ranking algorithms have been successfully applied on statistical machine translation systems. Due to the errors in the hypothesis alignment and varying word order between the source and target sentences and also the lack of sufficient resources such as parallel corpora, decoding may result in ungrammatical or non-fluent outputs. This paper proposes a re-ranking system based on swarm algorithms, which makes the use of sophisticated non-syntactical features to re-rank the n-best translation candidates. We introduce plenty of easy-computed non-syntactical features to deal with SMT system errors plus the quantum-behaved particle swarm optimization (QPSO) algorithm to adjust the weights of features. We have evaluated the proposed approach on 2 translation tasks in different language pairs (Persian → English and German → English) and genres (news and novel books). In comparison with PSO-, GA-, Perceptron- and averaged Perceptron-style re-ranking systems, the experimental study demonstrates the superiority of the proposed system in terms of translation quality on both translation tasks. In addition, the impacts of the proposed features on the translation quality have been analyzed, and the most positive ones have been recognized. At the end, the impact of the n-best list size on the proposed system is investigated.","['Saeed Farzi', 'Heshaam Faili']",January 2015,Computer Speech & Language,"['Re-ranking system', 'Quantum-behaved particle swarm optimization', 'Perceptron', 'BLEU']",A swarm-inspired re-ranker system for statistical machine translation☆
367,"A fine-grained segmentation of radio or TV broadcasts is an essential step for most multimedia processing tasks. Applying segmentation algorithms to the speech transcripts seems straightforward. Yet, most of these algorithms are not suited when dealing with short segments or noisy data. In this paper, we present a new segmentation technique inspired from the image analysis field and relying on a new way to compute similarities between candidate segments called vectorization. Vectorization makes it possible to match text segments that do not share common words; this property is shown to be particularly useful when dealing with transcripts in which transcription errors and short segments makes the segmentation difficult. This new topic segmentation technique is evaluated on two corpora of transcripts from French TV broadcasts on which it largely outperforms other existing approaches from the state-of-the-art.","['Vincent Claveau', 'Sébastien Lefèvre']",January 2015,Computer Speech & Language,"['Watershed transform', 'Image segmentation', 'Vectorization', 'Topic segmentation']",Topic segmentation of TV-streams by watershed transform and vectorization☆
368,"This paper proposes a hybrid refinement scheme for more accurate localization of phonetic boundaries by combining three different post-processing techniques, including statistical correction, fusion, and predictive models. A statistical method based on state-level correction is proposed to improve the segmentation results. Effects of search ranges on the statistical correction process are studied and a state selection scheme is used to enhance the correction results. This paper also examines the effects of time resolution, i.e., stepsize, of acoustic models on the accuracy of segmentation. A multi-resolution fusion process is proposed to further refine the statistically corrected results. Finally, predictive models are designed to improve the segmentation accuracy by incorporating various acoustic features and searching around the preliminary boundary with a smaller stepsize. By applying the hybrid refinement scheme on a well-known corpus, significant improvements of segmentation results in terms of segmentation accuracy with different tolerances, mean absolute error (MAE), and root-mean-square error (RMSE) can be observed. Furthermore, a scenario of cross-corpora segmentation is examined in generating the segmentation results for a new corpus with a small set of labeled data. Experimental results show that the proposed refinement procedure can generate segmentation results comparable to those given by well-trained acoustic models obtained from the new corpus.","['Sixuan Zhao', 'Ing Yann Soon', 'Soo Ngee Koh', 'Kang Kwong Luke']",January 2015,Computer Speech & Language,"['Phonetic segmentation', 'Statistical correction', 'Multi-resolution', 'Predictive model', 'Cross-corpora segmentation']",A hybrid refinement scheme for intra- and cross- corpora phonetic segmentation☆
369,,[],January 2015,Computer Speech & Language,[],Reviewer Acknowledgement
370,"South African English is currently considered an under-resourced variety of English. Extensive speech resources are, however, available for North American (US) English. In this paper we consider the use of these US resources in the development of a South African large vocabulary speech recognition system. Specifically we consider two research questions. Firstly, we determine the performance penalties that are incurred when using US instead of South African language models, pronunciation dictionaries and acoustic models. Secondly, we determine whether US acoustic and language modelling data can be used in addition to the much more limited South African resources to improve speech recognition performance. In the first case we find that using a US pronunciation dictionary or a US language model in a South African system results in fairly small penalties. However, a substantial penalty is incurred when using a US acoustic model. In the second investigation we find that small but consistent improvements over a baseline South African system can be obtained by the additional use of US acoustic data. Larger improvements are obtained when complementing the South African language modelling data with US and/or UK material. We conclude that, when developing resources for an under-resourced variety of English, the compilation of acoustic data should be prioritised, language modelling data has a weaker effect on performance and the pronunciation dictionary the smallest.","['Herman Kamper', 'Febe de Wet', 'Thomas Hain', 'Thomas Niesler']",November 2014,Computer Speech & Language,"['Under-resourced languages', 'Accented speech', 'South African English', 'Varieties of English']",Capitalising on North American speech resources for the development of a South African English large vocabulary speech recognition system☆
371,"This paper presents a new approach to speech enhancement from single-channel measurements involving both noise and channel distortion (i.e., convolutional noise), and demonstrates its applications for robust speech recognition and for improving noisy speech quality. The approach is based on finding longest matching segments (LMS) from a corpus of clean, wideband speech. The approach adds three novel developments to our previous LMS research. First, we address the problem of channel distortion as well as additive noise. Second, we present an improved method for modeling noise for speech estimation. Third, we present an iterative algorithm which updates the noise and channel estimates of the corpus data model. In experiments using speech recognition as a test with the Aurora 4 database, the use of our enhancement approach as a preprocessor for feature extraction significantly improved the performance of a baseline recognition system. In another comparison against conventional enhancement algorithms, both the PESQ and the segmental SNR ratings of the LMS algorithm were superior to the other methods for noisy speech enhancement.","['Ji Ming', 'Danny Crookes']",November 2014,Computer Speech & Language,"['Corpus-based speech modeling', 'Longest matching segment', 'Noisy speech', 'Channel distortion', 'Speech enhancement', 'Speech recognition']",An iterative longest matching segment approach to speech enhancement with additive noise and channel distortion☆
372,"This paper proposes an efficient speech data selection technique that can identify those data that will be well recognized. Conventional confidence measure techniques can also identify well-recognized speech data. However, those techniques require a lot of computation time for speech recognition processing to estimate confidence scores. Speech data with low confidence should not go through the time-consuming recognition process since they will yield erroneous spoken documents that will eventually be rejected. The proposed technique can select the speech data that will be acceptable for speech recognition applications. It rapidly selects speech data with high prior confidence based on acoustic likelihood values and using only speech and monophone models. Experiments show that the proposed confidence estimation technique is over 50 times faster than the conventional posterior confidence measure while providing equivalent data selection performance for speech recognition and spoken document retrieval.","['Satoshi Kobashikawa', 'Taichi Asami', 'Yoshikazu Yamaguchi', 'Hirokazu Masataki', 'Satoshi Takahashi']",November 2014,Computer Speech & Language,"['68T10', '43.72.Ne', 'Speech recognition', 'Spoken document retrieval', 'Data selection', 'Context independent model', 'Gaussian mixture model']",Efficient data selection for speech recognition based on prior confidence estimation using speech and monophone models☆
373,"Natural languages are known for their expressive richness. Many sentences can be used to represent the same underlying meaning. Only modelling the observed surface word sequence can result in poor context coverage and generalization, for example, when using n-gram language models (LMs). This paper proposes a novel form of language model, the paraphrastic LM, that addresses these issues. A phrase level paraphrase model statistically learned from standard text data with no semantic annotation is used to generate multiple paraphrase variants. LM probabilities are then estimated by maximizing their marginal probability. Multi-level language models estimated at both the word level and the phrase level are combined. An efficient weighted finite state transducer (WFST) based paraphrase generation approach is also presented. Significant error rate reductions of 0.5–0.6% absolute were obtained over the baseline n-gram LMs on two state-of-the-art recognition tasks for English conversational telephone speech and Mandarin Chinese broadcast speech using a paraphrastic multi-level LM modelling both word and phrase sequences. When it is further combined with word and phrase level feed-forward neural network LMs, a significant error rate reduction of 0.9% absolute (9% relative) and 0.5% absolute (5% relative) were obtained over the baseline n-gram and neural network LMs respectively.","['X. Liu', 'M.J.F. Gales', 'P.C. Woodland']",November 2014,Computer Speech & Language,"['Language modelling', 'Paraphrase', 'Speech recognition']",Paraphrastic language models☆
374,"Since Sag et al. (2002) highlighted a key problem that had been underappreciated in the past in natural language processing (NLP), namely idiosyncratic multiword expressions (MWEs) such as idioms, quasi-idioms, clichés, quasi-clichés, institutionalized phrases, proverbs and old sayings, and how to deal with them, many attempts have been made to extract these expressions from corpora and construct a lexicon of them. However, no extensive, reliable solution has yet been realized. This paper presents an overview of a comprehensive lexicon of Japanese multiword expressions (Japanese MWE Lexicon: JMWEL), which has been compiled in order to realize linguistically precise and wide-coverage natural Japanese processing systems. The JMWEL is characterized by significant notational, syntactic, and semantic diversity as well as a detailed description of the syntactic functions, structures, and flexibilities of MWEs. The lexicon contains about 111,000 header entries written in kana (phonetic characters) and their almost 820,000 variants written in kana and kanji (ideographic characters). The paper demonstrates the JMWEL's validity, supported mainly by comparing the lexicon with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08 generated by Google Inc. (Kudo and Kazawa, 2009). The present work is an attempt to provide a tentative answer for Japanese, from outside statistical empiricism, to the question posed by Church (2011): “How many multiword expressions do people know?”","['Toshifumi Tanabe', 'Masahito Takahashi', 'Kosho Shudo']",November 2014,Computer Speech & Language,"['Natural language processing', 'Multiword expression (MWE)', 'Lexicon', 'Linguistic idiosyncrasy', 'Non-compositionality', 'Dependency structure', 'Internal modification']","A lexicon of multiword expressions for linguistically precise, wide-coverage natural language processing☆"
375,"A new approach for intonation stylization that enables the extraction of an intonation representation from prosodically unlabeled data is introduced. This approach yields global and local intonation contour classes arising from a contour-based, parametric and superpositional intonation stylization. Based on findings about the linguistic interpretation of the contour classes derived from corpus statistics and perception experiments, we created simple prediction models for the partial generation of intonation contours from discourse structure defined by discourse segment boundaries and the information status of nouns within these segments. The predicted intonation contours were evaluated by human judgments of adequacy that yielded a high accordance.",['Uwe D. Reichel'],November 2014,Computer Speech & Language,"['Computational intonation stylization', 'Data-driven', 'Contour-based', 'Superposition', 'Discourse structure']",Linking bottom-up intonation stylization to discourse structure☆
376,"A typical spoken content retrieval solution integrates multiple technologies that belong to the areas of automatic speech recognition and information retrieval. Due to the rich set of challenges – many of them language specific – as well as widespread impact, numerous research sites in the world are actively engaged in this research area. This special issue highlights some of the recent advances in spoken content retrieval.","['M. Saraclar', 'Ciprian Chelba', 'Bhuvana Ramabhadran']",September 2014,Computer Speech & Language,"['Speech retrieval', 'Spoken document retrieval', 'Spoken term detection', 'Keyword search']",EditorialEditorial for the special issue on spoken content retrieval
377,"Increasing amounts of informal spoken content are being collected, e.g. recordings of meetings, lectures and personal data sources. The amount of this content being captured and the difficulties of manually searching audio data mean that efficient automated search tools are of increasing importance if its full potential is to be realized. Much existing work on speech search has focused on retrieval of clearly defined document units in ad hoc search tasks. We investigate search of informal speech content using an extended version of the AMI meeting collection. A retrieval collection was constructed by augmenting the AMI corpus with a set of ad hoc search requests and manually identified relevant regions of the recorded meetings. Unlike standard ad hoc information retrieval focussing primarily on precision, we assume a recall-focused search scenario of a user seeking to retrieve a particular incident occurring within meetings relevant to the query. We explore the relationship between automatic speech recognition (ASR) accuracy, automated segmentation of the meeting into retrieval units and retrieval behaviour with respect to both precision and recall. Experimental retrieval results show that while averaged retrieval effectiveness is generally comparable in terms of precision for automatically extracted segments for manual content transcripts and ASR transcripts with high recognition accuracy, segments with poor recognition quality become very hard to retrieve and may fall below the retrieval rank position to which a user is willing search. These changes impact on system effectiveness for recall-focused search tasks. Varied ASR quality across the relevant and non-relevant data means that the rank of some well-recognized relevant segments is actually promoted for ASR transcripts compared to manual ones. This effect is not revealed by the averaged precision based retrieval evaluation metrics typically used for evaluation of speech retrieval. However such variations in the ranks of relevant segments can impact considerably on the experience of the user in terms of the order in which retrieved content is presented. Analysis of our results reveals that while relevant longer segments are generally more robust to ASR errors, and consequentially retrieved at higher ranks, this is often at the expense of the user needing to engage in longer content playback to locate the relevant content in the audio recording. Our overall conclusion being that it is desirable to minimize the length of retrieval units containing relevant content while seeking to maintain high ranking of these items.","['Maria Eskevich', 'Gareth J.F. Jones']",September 2014,Computer Speech & Language,"['Speech retrieval', 'Recall-focused information retrieval', 'Informal spoken content search', 'Retrieval unit segmentation']",Exploring speech retrieval from meetings using the AMI corpus☆
378,"Spoken content retrieval will be very important for retrieving and browsing multimedia content over the Internet, and spoken term detection (STD) is one of the key technologies for spoken content retrieval. In this paper, we show acoustic feature similarity between spoken segments used with pseudo-relevance feedback and graph-based re-ranking can improve the performance of STD. This is based on the concept that spoken segments similar in acoustic feature vector sequences to those with higher/lower relevance scores should have higher/lower scores, while graph-based re-ranking further uses a graph to consider the similarity structure among all the segments retrieved in the first pass. These approaches are formulated on both word and subword lattices, and a complete framework of using them in open vocabulary retrieval of spoken content is presented. Significant improvements for these approaches with both in-vocabulary and out-of-vocabulary queries were observed in preliminary experiments.","['Hung-yi Lee', 'Po-wei Chou', 'Lin-shan Lee']",September 2014,Computer Speech & Language,"['Spoken content retrieval', 'Spoken term detection', 'Pseudo-relevance feedback', 'Random walk']",Improved open-vocabulary spoken content retrieval with word and subword lattices using acoustic feature similarity☆
379,"In this paper, we describe several approaches to language-independent spoken term detection and compare their performance on a common task, namely “Spoken Web Search”. The goal of this part of the MediaEval initiative is to perform low-resource language-independent audio search using audio as input. The data was taken from “spoken web” material collected over mobile phone connections by IBM India as well as from the LWAZI corpus of African languages. As part of the 2011 and 2012 MediaEval benchmark campaigns, a number of diverse systems were implemented by independent teams, and submitted to the “Spoken Web Search” task. This paper presents the 2011 and 2012 results, and compares the relative merits and weaknesses of approaches developed by participants, providing analysis and directions for future research, in order to improve voice access to spoken information in low resource settings.","['Florian Metze', 'Xavier Anguera', 'Etienne Barnard', 'Marelie Davel', 'Guillaume Gravier']",September 2014,Computer Speech & Language,"['Low-resource speech technology', 'Evaluation', 'Spoken web', 'Spoken term detection']",Language independent search in MediaEval's Spoken Web Search task
380,"Discriminative confidence based on multi-layer perceptrons (MLPs) and multiple features has shown significant advantage compared to the widely used lattice-based confidence in spoken term detection (STD). Although the MLP-based framework can handle any features derived from a multitude of sources, choosing all possible features may lead to over complex models and hence less generality. In this paper, we design an extensive set of features and analyze their contribution to STD individually and as a group. The main goal is to choose a small set of features that are sufficiently informative while keeping the model simple and generalizable. We employ two established models to conduct the analysis: one is linear regression which targets for the most relevant features and the other is logistic linear regression which targets for the most discriminative features. We find the most informative features are comprised of those derived from diverse sources (ASR decoding, duration and lexical properties) and the two models deliver highly consistent feature ranks. STD experiments on both English and Spanish data demonstrate significant performance gains with the proposed feature sets.","['Javier Tejedor', 'Doroteo T. Toledano', 'Dong Wang', 'Simon King', 'José Colás']",September 2014,Computer Speech & Language,"['Feature analysis', 'Discriminative confidence', 'Spoken term detection', 'Speech recognition']",Feature analysis for discriminative confidence estimation in spoken term detection☆
381,,[],September 2014,Computer Speech & Language,[],ForewordGlottal source processing: From analysis to applications
382,"The great majority of current voice technology applications rely on acoustic features, such as the widely used MFCC or LP parameters, which characterize the vocal tract response. Nonetheless, the major source of excitation, namely the glottal flow, is expected to convey useful complementary information. The glottal flow is the airflow passing through the vocal folds at the glottis. Unfortunately, glottal flow analysis from speech recordings requires specific and complex processing operations, which explains why it has been generally avoided. This paper gives a comprehensive overview of techniques for glottal source processing. Starting from analysis tools for pitch tracking, detection of glottal closure instant, estimation and modeling of glottal flow, this paper discusses how these tools and techniques might be properly integrated in various voice technology applications.","['Thomas Drugman', 'Paavo Alku', 'Abeer Alwan', 'Bayya Yegnanarayana']",September 2014,Computer Speech & Language,"['Speech processing', 'Speech analysis', 'Glottal source', 'Glottal flow', 'Excitation signal', 'Residual signal']",Glottal source processing: From analysis to applications
383,"This paper presents a new glottal inverse filtering (GIF) method that utilizes a Markov chain Monte Carlo (MCMC) algorithm. First, initial estimates of the vocal tract and glottal flow are evaluated by an existing GIF method, iterative adaptive inverse filtering (IAIF). Simultaneously, the initially estimated glottal flow is synthesized using the Rosenberg–Klatt (RK) model and filtered with the estimated vocal tract filter to create a synthetic speech frame. In the MCMC estimation process, the first few poles of the initial vocal tract model and the RK excitation parameter are refined in order to minimize the error between the synthetic and original speech signals in the time and frequency domain. MCMC approximates the posterior distribution of the parameters, and the final estimate of the vocal tract is found by averaging the parameter values of the Markov chain. Experiments with synthetic vowels produced by a physical modeling approach show that the MCMC-based GIF method gives more accurate results compared to two known reference methods.","['Harri Auvinen', 'Tuomo Raitio', 'Manu Airaksinen', 'Samuli Siltanen', 'Brad H. Story', 'Paavo Alku']",September 2014,Computer Speech & Language,"['Glottal inverse filtering', 'Markov chain Monte Carlo']",Automatic glottal inverse filtering with the Markov chain Monte Carlo method☆
384,"Laryngeal high-speed videoendoscopy is a state-of-the-art technique to examine physiological vibrational patterns of the vocal folds. With sampling rates of thousands of frames per second, high-speed videoendoscopy produces a large amount of data that is difficult to analyze subjectively. In order to visualize high-speed video in a straightforward and intuitive way, many methods have been proposed to condense the three-dimensional data into a few static images that preserve characteristics of the underlying vocal fold vibratory patterns. In this paper, we propose the “glottaltopogram,” which is based on principal component analysis of changes over time in the brightness of each pixel in consecutive video images. This method reveals the overall synchronization of the vibrational patterns of the vocal folds over the entire laryngeal area. Experimental results showed that this method is effective in visualizing pathological and normal vocal fold vibratory patterns.","['Gang Chen', 'Jody Kreiman', 'Abeer Alwan']",September 2014,Computer Speech & Language,"['High-speed videoendoscopy', 'Vocal fold vibration', 'Principal component analysis']",The glottaltopogram: A method of analyzing high-speed images of the vocal folds☆☆☆
385,"This paper summarizes the results of our investigations into estimating the shape of the glottal excitation source from speech signals. We employ the Liljencrants–Fant (LF) model describing the glottal flow and its derivative. The one-dimensional glottal source shape parameter Rd describes the transition in voice quality from a tense to a breathy voice. The parameter Rd has been derived from a statistical regression of the R waveshape parameters which parameterize the LF model. First, we introduce a variant of our recently proposed adaptation and range extension of the Rd parameter regression. Secondly, we discuss in detail the aspects of estimating the glottal source shape parameter Rd using the phase minimization paradigm. Based on the analysis of a large number of speech signals we describe the major conditions that are likely to result in erroneous Rd estimates. Based on these findings we investigate into means to increase the robustness of the Rd parameter estimation. We use Viterbi smoothing to suppress unnatural jumps of the estimated Rd parameter contours within short time segments. Additionally, we propose to steer the Viterbi algorithm by exploiting the covariation of other voice descriptors to improve Viterbi smoothing. The novel Viterbi steering is based on a Gaussian Mixture Model (GMM) that represents the joint density of the voice descriptors and the Open Quotient (OQ) estimated from corresponding electroglottographic (EGG) signals. A conversion function derived from the mixture model predicts OQ from the voice descriptors. Converted to Rd it defines an additional prior probability to adapt the partial probabilities of the Viterbi algorithm accordingly. Finally, we evaluate the performances of the phase minimization based methods using both variants to adapt and extent the Rd regression on one synthetic test set as well as in combination with Viterbi smoothing and each variant of the novel Viterbi steering on one test set of natural speech. The experimental findings exhibit improvements for both Viterbi approaches.","['Stefan Huber', 'Axel Roebel']",September 2014,Computer Speech & Language,"['Glottal source', 'Voice quality', 'Rd shape parameter', 'LF model', 'Viterbi smoothing']",On the use of voice descriptors for glottal source shape parameter estimation☆
386,"We discuss the use of low-dimensional physical models of the voice source for speech coding and processing applications. A class of waveform-adaptive dynamic glottal models and parameter identification procedures are illustrated. The model and the identification procedures are assessed by addressing signal transformations on recorded speech, achievable by fitting the model to the data, and then acting on the physically oriented parameters of the voice source. The class of models proposed provides in principle a tool for both the estimation of glottal source signals, and the encoding of the speech signal for transformation purposes. The application of this model to time stretching and to fundamental frequency control (pitch shifting) is also illustrated. The experiments show that copy synthesis is perceptually very similar to the target, and that time stretching and “pitch extrapolation” effects can be obtained by simple control strategies.","['Carlo Drioli', 'Andrea Calanca']",September 2014,Computer Speech & Language,"['Glottal modeling', 'Model inversion', 'Model-based transformations', 'Speech synthesis and processing']",Speaker adaptive voice source modeling with applications to speech coding and processing☆
387,"This paper presents a study on the importance of short-term speech parameterizations for expressive statistical parametric synthesis. Assuming a source-filter model of speech production, the analysis is conducted over spectral parameters, here defined as features which represent a minimum-phase synthesis filter, and some excitation parameters, which are features used to construct a signal that is fed to the minimum-phase synthesis filter to generate speech. In the first part, different spectral and excitation parameters that are applicable to statistical parametric synthesis are tested to determine which ones are the most emotion dependent. The analysis is performed through two methods proposed to measure the relative emotion dependency of each feature: one based on K-means clustering, and another based on Gaussian mixture modeling for emotion identification. Two commonly used forms of parameters for the short-term speech spectral envelope, the Mel cepstrum and the Mel line spectrum pairs are utilized. As excitation parameters, the anti-causal cepstrum, the time-smoothed group delay, and band-aperiodicity coefficients are considered. According to the analysis, the line spectral pairs are the most emotion dependent parameters. Among the excitation features, the band-aperiodicity coefficients present the highest correlation with the speaker's emotion. The most emotion dependent parameters according to this analysis were selected to train an expressive statistical parametric synthesizer using a speaker and language factorization framework. Subjective test results indicate that the considered spectral parameters have a bigger impact on the synthesized speech emotion when compared with the excitation ones.","['Ranniery Maia', 'Masami Akamine']",September 2014,Computer Speech & Language,"['Speech synthesis', 'Statistical parametric speech synthesis', 'Expressive speech synthesis', 'Speech parameterization']",On the impact of excitation and spectral parameters for expressive statistical parametric speech synthesis☆
388,"This paper investigates the temporal excitation patterns of creaky voice. Creaky voice is a voice quality frequently used as a phrase-boundary marker, but also as a means of portraying attitude, affective states and even social status. Consequently, the automatic detection and modelling of creaky voice may have implications for speech technology applications. The acoustic characteristics of creaky voice are, however, rather distinct from modal phonation. Further, several acoustic patterns can bring about the perception of creaky voice, thereby complicating the strategies used for its automatic detection, analysis and modelling. The present study is carried out using a variety of languages, speakers, and on both read and conversational data and involves a mutual information-based assessment of the various acoustic features proposed in the literature for detecting creaky voice. These features are then exploited in classification experiments where we achieve an appreciable improvement in detection accuracy compared to the state of the art. Both experiments clearly highlight the presence of several creaky patterns. A subsequent qualitative and quantitative analysis of the identified patterns is provided, which reveals a considerable speaker-dependent variability in the usage of these creaky patterns. We also investigate how creaky voice detection systems perform across creaky patterns.","['Thomas Drugman', 'John Kane', 'Christer Gobl']",September 2014,Computer Speech & Language,"['Creaky voice', 'Vocal fry', 'Irregular phonation', 'Glottal source']",Data-driven detection and analysis of the patterns of creaky voice☆
389,"Most research in the automatic assessment of free text answers written by students address English language. This paper handles the assessment task in Arabic language. This research focuses on applying multiple similarity measures separately and in combination. Many aspects are introduced that depend on translation to overcome the lack of text processing resources in Arabic, such as extracting model answers automatically from an already built database and applying K-means clustering to scale the obtained similarity values. Additionally, this research presents the first benchmark Arabic data set that contains 610 students’ short answers together with their English translations.","['Wael Hassan Gomaa', 'Aly Aly Fahmy']",July 2014,Computer Speech & Language,"['Short answer scoring', 'Text similarity', 'Semantic similarity', 'Arabic corpus']",Automatic scoring for answers to Arabic test questions☆
390,"A speech pre-processing algorithm is presented that improves the speech intelligibility in noise for the near-end listener. The algorithm improves intelligibility by optimally redistributing the speech energy over time and frequency according to a perceptual distortion measure, which is based on a spectro-temporal auditory model. Since this auditory model takes into account short-time information, transients will receive more amplification than stationary vowels, which has been shown to be beneficial for intelligibility of speech in noise. The proposed method is compared to unprocessed speech and two reference methods using an intelligibility listening test. Results show that the proposed method leads to significant intelligibility gains while still preserving quality. Although one of the methods used as a reference obtained higher intelligibility gains, this happened at the cost of decreased quality. Matlab code is provided.","['Cees H. Taal', 'Richard C. Hendriks', 'Richard Heusdens']",July 2014,Computer Speech & Language,"['Near-end speech enhancement', 'Intelligibility improvement', 'Transients']",Speech energy redistribution for intelligibility improvement in noise based on a perceptual distortion measure☆
391,"This article describes an evaluation of a POMDP-based spoken dialogue system (SDS), using crowdsourced calls with real users. The evaluation compares a “Hidden Information State” POMDP system which uses a hand-crafted compression of the belief space, with the same system instead using an automatically computed belief space compression. Automatically computed compressions are a way of introducing automation into the design process of statistical SDSs and promise a principled way of reducing the size of the very large belief spaces which often make POMDP approaches intractable. This is the first empirical comparison of manual and automatic approaches on a problem of realistic scale (restaurant, pub and coffee shop domain) with real users. The evaluation took 2193 calls from 85 users. After filtering for minimal user participation the two systems were compared on more than 1000 calls.","['Paul A. Crook', 'Simon Keizer', 'Zhuoran Wang', 'Wenshuo Tang', 'Oliver Lemon']",July 2014,Computer Speech & Language,"['Spoken dialogue systems', 'Dialogue management', 'Belief compression']",Real user evaluation of a POMDP spoken dialogue system using automatic belief compression
392,"This article investigates speech feature enhancement based on deep bidirectional recurrent neural networks. The Long Short-Term Memory (LSTM) architecture is used to exploit a self-learnt amount of temporal context in learning the correspondences of noisy and reverberant with undistorted speech features. The resulting networks are applied to feature enhancement in the context of the 2013 2nd Computational Hearing in Multisource Environments (CHiME) Challenge track 2 task, which consists of the Wall Street Journal (WSJ-0) corpus distorted by highly non-stationary, convolutive noise. In extensive test runs, different feature front-ends, network training targets, and network topologies are evaluated in terms of frame-wise regression error and speech recognition performance. Furthermore, we consider gradually refined speech recognition back-ends from baseline ‘out-of-the-box’ clean models to discriminatively trained multi-condition models adapted to the enhanced features. In the result, deep bidirectional LSTM networks processing log Mel filterbank outputs deliver best results with clean models, reaching down to 42% word error rate (WER) at signal-to-noise ratios ranging from −6 to 9 dB (multi-condition CHiME Challenge baseline: 55% WER). Discriminative training of the back-end using LSTM enhanced features is shown to further decrease WER to 22%. To our knowledge, this is the best result reported for the 2nd CHiME Challenge WSJ-0 task yet.","['Felix Weninger', 'Jürgen Geiger', 'Martin Wöllmer', 'Björn Schuller', 'Gerhard Rigoll']",July 2014,Computer Speech & Language,"['Automatic speech recognition', 'Feature enhancement', 'Deep neural networks', 'Long Short-Term Memory']",Feature enhancement by deep LSTM networks for ASR in reverberant multisource environments☆
393,"Traditional dialogue systems use a fixed silence threshold to detect the end of users’ turns. Such a simplistic model can result in system behaviour that is both interruptive and unresponsive, which in turn affects user experience. Various studies have observed that human interlocutors take cues from speaker behaviour, such as prosody, syntax, and gestures, to coordinate smooth exchange of speaking turns. However, little effort has been made towards implementing these models in dialogue systems and verifying how well they model the turn-taking behaviour in human–computer interactions. We present a data-driven approach to building models for online detection of suitable feedback response locations in the user's speech. We first collected human–computer interaction data using a spoken dialogue system that can perform the Map Task with users (albeit using a trick). On this data, we trained various models that use automatically extractable prosodic, contextual and lexico-syntactic features for detecting response locations. Next, we implemented a trained model in the same dialogue system and evaluated it in interactions with users. The subjective and objective measures from the user evaluation confirm that a model trained on speaker behavioural cues offers both smoother turn-transitions and more responsive system behaviour.","['Raveesh Meena', 'Gabriel Skantze', 'Joakim Gustafson']",July 2014,Computer Speech & Language,"['Spoken dialogue systems', 'Timing feedback', 'Turn-taking', 'User evaluation']",Data-driven models for timing feedback responses in a Map Task dialogue system☆
394,"This paper describes a new kind of language models based on the possibility theory. The purpose of these new models is to better use the data available on the Web for language modeling. These models aim to integrate information relative to impossible word sequences. We address the two main problems of using this kind of model: how to estimate the measures for word sequences and how to integrate this kind of model into the ASR system.We propose a word-sequence possibilistic measure and a practical estimation method based on word-sequence statistics, which is particularly suited for estimating from Web data. We develop several strategies and formulations for using these models in a classical automatic speech recognition engine, which relies on a probabilistic modeling of the speech recognition process. This work is evaluated on two typical usage scenarios: broadcast news transcription with very large training sets and transcription of medical videos, in a specialized domain, with only very limited training data.The results show that the possibilistic models provide significantly lower word error rate on the specialized domain task, where classical n-gram models fail due to the lack of training materials. For the broadcast news, the probabilistic models remain better than the possibilistic ones. However, a log-linear combination of the two kinds of models outperforms all the models used individually, which indicates that possibilistic models bring information that is not modeled by probabilistic ones.","['Stanislas Oger', 'Georges Linarès']",July 2014,Computer Speech & Language,"['Speech processing', 'Language modeling', 'Theory of possibilities']",Web-based possibilistic language models for automatic speech recognition☆
395,"This paper presents a simplified and supervised i-vector modeling approach with applications to robust and efficient language identification and speaker verification. First, by concatenating the label vector and the linear regression matrix at the end of the mean supervector and the i-vector factor loading matrix, respectively, the traditional i-vectors are extended to label-regularized supervised i-vectors. These supervised i-vectors are optimized to not only reconstruct the mean supervectors well but also minimize the mean square error between the original and the reconstructed label vectors to make the supervised i-vectors become more discriminative in terms of the label information. Second, factor analysis (FA) is performed on the pre-normalized centered GMM first order statistics supervector to ensure each gaussian component's statistics sub-vector is treated equally in the FA, which reduces the computational cost by a factor of 25 in the simplified i-vector framework. Third, since the entire matrix inversion term in the simplified i-vector extraction only depends on one single variable (total frame number), we make a global table of the resulting matrices against the frame numbers’ log values. Using this lookup table, each utterance's simplified i-vector extraction is further sped up by a factor of 4 and suffers only a small quantization error. Finally, the simplified version of the supervised i-vector modeling is proposed to enhance both the robustness and efficiency. The proposed methods are evaluated on the DARPA RATS dev2 task, the NIST LRE 2007 general task and the NIST SRE 2010 female condition 5 task for noisy channel language identification, clean channel language identification and clean channel speaker verification, respectively. For language identification on the DARPA RATS, the simplified supervised i-vector modeling achieved 2%, 16%, and 7% relative equal error rate (EER) reduction on three different feature sets and sped up by a factor of more than 100 against the baseline i-vector method for the 120 s task. Similar results were observed on the NIST LRE 2007 30 s task with 7% relative average cost reduction. Results also show that the use of Gammatone frequency cepstral coefficients, Mel-frequency cepstral coefficients and spectro-temporal Gabor features in conjunction with shifted-delta-cepstral features improves the overall language identification performance significantly. For speaker verification, the proposed supervised i-vector approach outperforms the i-vector baseline by relatively 12% and 7% in terms of EER and norm old minDCF values, respectively.","['Ming Li', 'Shrikanth Narayanan']",July 2014,Computer Speech & Language,"['Language identification', 'Speaker verification', 'I-vector', 'Supervised i-vector', 'Simplified i-vector', 'Simplified supervised i-vector']",Simplified supervised i-vector modeling with application to robust and efficient language identification and speaker verification☆☆☆
396,"This paper outlines a comprehensive system for automatically generating a phonetic transcription of a given Arabic text which closely matches the pronunciation of the speakers. The presented system is based on a set of (language-dependent) pronunciation rules that works on converting fully diacriticised Arabic text into the actual sounds, along with a lexicon for exceptional words. This is a two-phase process: one-to-one grapheme to phoneme conversion and then phoneme-to-allophone conversion using a set of “phonological rules”. Phonological rules operate on the phonemes and convert them to the actual sounds considering the neighbouring phones or the containing syllable or word. This system is developed for the purpose of delivering a robust Automatic Arabic Speech Recognition (AASR) system which is able to handle speech variation resulting from the mismatch between the text and the pronunciation. We anticipate that it could also be used for producing natural sounding speech from an Arabic text-to-speech (ATTS) system as well, but we have not extensively tested it in this application.","['Allan Ramsay', 'Iman Alsharhan', 'Hanady Ahmed']",July 2014,Computer Speech & Language,"['Phonological rules', 'Phonetic transcription', 'Speech processing', 'Modern standard Arabic', 'Sound-spelling correspondences']",Generation of a phonetic transcription for modern standard Arabic: A knowledge-based model☆
397,"Accurate phonetic transcription of proper nouns can be an important resource for commercial applications that embed speech technologies, such as audio indexing and vocal phone directory lookup. However, an accurate phonetic transcription is more difficult to obtain for proper nouns than for regular words. Indeed, phonetic transcription of a proper noun depends on both the origin of the speaker pronouncing it and the origin of the proper noun itself.This work proposes a method that allows the extraction of phonetic transcriptions of proper nouns using actual utterances of those proper nouns, thus yielding transcriptions based on practical use instead of mere pronunciation rules.The proposed method consists in a process that first extracts phonetic transcriptions, and then iteratively filters them. In order to initialize the process, an alignment dictionary is used to detect word boundaries. A rule-based grapheme-to-phoneme generator (LIA_PHON), a knowledge-based approach (JSM), and a Statistical Machine Translation based system were evaluated for this alignment. As a result, compared to our reference dictionary (BDLEX supplemented by LIA_PHON for missing words) on the ESTER 1 French broadcast news corpus, we were able to significantly decrease the Word Error Rate (WER) on segments of speech with proper nouns, without negatively affecting the WER on the rest of the corpus.","['Antoine Laurent', 'Sylvain Meignier', 'Paul Deléglise']",July 2014,Computer Speech & Language,"['Speech recognition', 'Phonetic transcription', 'Proper nouns', 'SMT', 'Moses', 'G2P']",Improving recognition of proper nouns in ASR through generating and filtering phonetic transcriptions☆
398,"In command-and-control applications, a vocal user interface (VUI) is useful for handsfree control of various devices, especially for people with a physical disability. The spoken utterances are usually restricted to a predefined list of phrases or to a restricted grammar, and the acoustic models work well for normal speech. While some state-of-the-art methods allow for user adaptation of the predefined acoustic models and lexicons, we pursue a fully adaptive VUI by learning both vocabulary and acoustics directly from interaction examples. A learning curve usually has a steep rise in the beginning and an asymptotic ceiling at the end. To limit tutoring time and to guarantee good performance in the long run, the word learning rate of the VUI should be fast and the learning curve should level off at a high accuracy. In order to deal with these performance indicators, we propose a multi-level VUI architecture and we investigate the effectiveness of alternative processing schemes. In the low-level layer, we explore the use of MIDA features (Mutual Information Discrimination Analysis) against conventional MFCC features. In the mid-level layer, we enhance the acoustic representation by means of phone posteriorgrams and clustering procedures. In the high-level layer, we use the NMF (Non-negative Matrix Factorization) procedure which has been demonstrated to be an effective approach for word learning. We evaluate and discuss the performance and the feasibility of our approach in a realistic experimental setting of the VUI-user learning context.","['Bart Ons', 'Jort F. Gemmeke', 'Hugo Van hamme']",July 2014,Computer Speech & Language,"['MIDA', 'Phone posteriorgram', 'NMF', 'Fast learning', 'Vocabulary acquisition']",Fast vocabulary acquisition in an NMF-based self-learning vocal user interface
399,"The maximum a posteriori (MAP) criterion is popularly used for feature compensation (FC) and acoustic model adaptation (MA) to reduce the mismatch between training and testing data sets. MAP-based FC and MA require prior densities of mapping function parameters, and designing suitable prior densities plays an important role in obtaining satisfactory performance. In this paper, we propose to use an environment structuring framework to provide suitable prior densities for facilitating MAP-based FC and MA for robust speech recognition. The framework is constructed in a two-stage hierarchical tree structure using environment clustering and partitioning processes. The constructed framework is highly capable of characterizing local information about complex speaker and speaking acoustic conditions. The local information is utilized to specify hyper-parameters in prior densities, which are then used in MAP-based FC and MA to handle the mismatch issue. We evaluated the proposed framework on Aurora-2, a connected digit recognition task, and Aurora-4, a large vocabulary continuous speech recognition (LVCSR) task. On both tasks, experimental results showed that with the prepared environment structuring framework, we could obtain suitable prior densities for enhancing the performance of MAP-based FC and MA.","['Yu Tsao', 'Xugang Lu', 'Paul Dixon', 'Ting-yao Hu', 'Shigeki Matsuda', 'Chiori Hori']",May 2014,Computer Speech & Language,"['MAP', 'Feature compensation', 'Acoustic model adaptation', 'Local information', 'Hyper-parameter specification', 'Noise robustness']",Incorporating local information of the acoustic environments to MAP-based feature compensation and acoustic model adaptation☆☆
400,"Automatic emotion recognition from speech signals is one of the important research areas, which adds value to machine intelligence. Pitch, duration, energy and Mel-frequency cepstral coefficients (MFCC) are the widely used features in the field of speech emotion recognition. A single classifier or a combination of classifiers is used to recognize emotions from the input features. The present work investigates the performance of the features of Autoregressive (AR) parameters, which include gain and reflection coefficients, in addition to the traditional linear prediction coefficients (LPC), to recognize emotions from speech signals. The classification performance of the features of AR parameters is studied using discriminant, k-nearest neighbor (KNN), Gaussian mixture model (GMM), back propagation artificial neural network (ANN) and support vector machine (SVM) classifiers and we find that the features of reflection coefficients recognize emotions better than the LPC. To improve the emotion recognition accuracy, we propose a class-specific multiple classifiers scheme, which is designed by multiple parallel classifiers, each of which is optimized to a class. Each classifier for an emotional class is built by a feature identified from a pool of features and a classifier identified from a pool of classifiers that optimize the recognition of the particular emotion. The outputs of the classifiers are combined by a decision level fusion technique. The experimental results show that the proposed scheme improves the emotion recognition accuracy. Further improvement in recognition accuracy is obtained when the scheme is built by including MFCC features in the pool of features.","['A. Milton', 'S. Tamil Selvi']",May 2014,Computer Speech & Language,"['Multiple classifiers', 'Class specific classification', 'Classifier fusion', 'Speech emotion recognition', 'AR parameters']",Class-specific multiple classifiers scheme to recognize emotions from speech signals☆
401,"This paper proposes a domain-independent statistical methodology to develop dialog managers for spoken dialog systems. Our methodology employs a data-driven classification procedure to generate abstract representations of system turns taking into account the previous history of the dialog. A statistical framework is also introduced for the development and evaluation of dialog systems created using the methodology, which is based on a dialog simulation technique. The benefits and flexibility of the proposed methodology have been validated by developing statistical dialog managers for four spoken dialog systems of different complexity, designed for different languages (English, Italian, and Spanish) and application domains (from transactional to problem-solving tasks). The evaluation results show that the proposed methodology allows rapid development of new dialog managers as well as to explore new dialog strategies, which permit developing new enhanced versions of already existing systems.","['David Griol', 'Zoraida Callejas', 'Ramón López-Cózar', 'Giuseppe Riccardi']",May 2014,Computer Speech & Language,"['Spoken dialog systems', 'Dialog management', 'Statistical methodologies', 'User modeling', 'Dialog simulation', 'Systems evaluation']",A domain-independent statistical methodology for dialog management in spoken dialog systems☆
402,"We compared the performance of an automatic speech recognition system using n-gram language models, HMM acoustic models, as well as combinations of the two, with the word recognition performance of human subjects who either had access to only acoustic information, had information only about local linguistic context, or had access to a combination of both. All speech recordings used were taken from Japanese narration and spontaneous speech corpora.Humans have difficulty recognizing isolated words taken out of context, especially when taken from spontaneous speech, partly due to word-boundary coarticulation. Our recognition performance improves dramatically when one or two preceding words are added. Short words in Japanese mainly consist of post-positional particles (i.e. wa, ga, wo, ni, etc.), which are function words located just after content words such as nouns and verbs. So the predictability of short words is very high within the context of the one or two preceding words, and thus recognition of short words is drastically improved. Providing even more context further improves human prediction performance under text-only conditions (without acoustic signals). It also improves speech recognition, but the improvement is relatively small.Recognition experiments using an automatic speech recognizer were conducted under conditions almost identical to the experiments with humans. The performance of the acoustic models without any language model, or with only a unigram language model, were greatly inferior to human recognition performance with no context. In contrast, prediction performance using a trigram language model was superior or comparable to human performance when given a preceding and a succeeding word. These results suggest that we must improve our acoustic models rather than our language models to make automatic speech recognizers comparable to humans in recognition performance under conditions where the recognizer has limited linguistic context.","['Norihide Kitaoka', 'Daisuke Enami', 'Seiichi Nakagawa']",May 2014,Computer Speech & Language,"['Continuous speech recognition', 'Human speech recognition ability', 'Acoustic model', 'Language model']",Effect of acoustic and linguistic contexts on human and machine speech recognition☆
403,"One of the aims of Assistive Technologies is to help people with disabilities to communicate with others and to provide means of access to information. As an aid to Deaf people, we present in this work a production-quality rule-based machine system for translating from Spanish to Spanish Sign Language (LSE) glosses, which is a necessary precursor to building a full machine translation system that eventually produces animation output. The system implements a transfer-based architecture from the syntactic functions of dependency analyses. A sketch of LSE is also presented. Several topics regarding translation to sign languages are addressed: the lexical gap, the bootstrapping of a bilingual lexicon, the generation of word order for topic-oriented languages, and the treatment of classifier predicates and classifier names. The system has been evaluated with an open-domain testbed, reporting a 0.30 BLEU (BiLingual Evaluation Understudy) and 42% TER (Translation Error Rate). These results show consistent improvements over a statistical machine translation baseline, and some improvements over the same system preserving the word order in the source sentence. Finally, the linguistic analysis of errors has identified some differences due to a certain degree of structural variation in LSE.","['Jordi Porta', 'Fernando López-Colino', 'Javier Tejedor', 'José Colás']",May 2014,Computer Speech & Language,"['Machine translation', 'Spanish', 'Spanish Sign Language', 'LSE', 'Deaf people communication']",A rule-based translation from written Spanish to Spanish Sign Language glosses☆
404,"While there is great potential for sign language animation generation software to improve the accessibility of information for deaf individuals with low written-language literacy, the understandability of current sign language animation systems is limited. Data-driven methodologies using annotated sign language corpora encoding detailed human movement have enabled some researchers to address several key linguistic challenges in ASL generation. This article motivates and describes our current research on collecting a motion-capture corpus of American Sign Language (ASL). As an evaluation of our motion-capture configuration, calibration, and recording protocol, we have conducted several rounds of evaluation studies with native ASL signers, and we have made use of our collected data to synthesize novel animations of ASL, which have also been evaluated in experimental studies with native signers.","['Pengfei Lu', 'Matt Huenerfauth']",May 2014,Computer Speech & Language,"['American Sign Language', 'ASL', 'Sign language', 'Motion-capture', 'Corpus creation', 'Sign language generation', 'Animation', 'Sign language synthesis', 'Generation', 'Evaluation', 'User-based studies', 'Assistive technology for people who are deaf']",Collecting and evaluating the CUNY ASL corpus for research on American Sign Language animation☆
405,,"['Björn Schuller', 'Stefan Steidl', 'Anton Batliner', 'Florian Schiel', 'Jarek Krajewski']",March 2014,Computer Speech & Language,[],Introduction to the Special Issue on Broadening the View on Speaker Analysis
406,"In the emerging field of computational paralinguistics, most research efforts are devoted to either short-term speaker states such as emotions, or long-term traits such as personality, gender, or age. To bridge this gap on the time axis, and hence broaden the scope of the field, the INTERSPEECH 2011 Speaker State Challenge addressed the algorithmic analysis of medium-term speaker states: alcohol intoxication and sleepiness, both of which are highly relevant in high risk environments. Preserving the paradigms of the two previous INTERSPEECH Challenges, researchers were invited to participate in a large-scale evaluation providing unified testing conditions. This article reviews previous efforts to automatically recognise intoxication and sleepiness from speech signals, and gives an overview on the Challenge conditions and data sets, the methods used by the participants, and their results. By fusing participants’ systems, we show that binary classification of alcoholisation and sleepiness from short-term observations, i.e., single utterances, can both reach over 72% accuracy on unseen test data; furthermore, we demonstrate that these medium-term states can be recognised more robustly by fusing short-term classifiers along the time axis, reaching up to 91% accuracy for intoxication and 75% for sleepiness.","['Björn Schuller', 'Stefan Steidl', 'Anton Batliner', 'Florian Schiel', 'Jarek Krajewski', 'Felix Weninger', 'Florian Eyben']",March 2014,Computer Speech & Language,"['Computational paralinguistics', 'Intoxication', 'Sleepiness', 'Survey', 'Challenge']","Medium-term speaker states—A review on intoxication, sleepiness and the first challenge☆"
407,"Segmental and suprasegmental speech signal modulations offer information about paralinguistic content such as affect, age and gender, pathology, and speaker state. Speaker state encompasses medium-term, temporary physiological phenomena influenced by internal or external bio-chemical actions (e.g., sleepiness, alcohol intoxication). Perceptual and computational research indicates that detecting speaker state from speech is a challenging task. In this paper, we present a system constructed with multiple representations of prosodic and spectral features that provided the best result at the Intoxication Subchallenge of Interspeech 2011 on the Alcohol Language Corpus. We discuss the details of each classifier and show that fusion improves performance. We additionally address the question of how best to construct a speaker state detection system in terms of robust and practical marginalization of associated variability such as through modeling speakers, utterance type, gender, and utterance length. As is the case in human perception, speaker normalization provides significant improvements to our system. We show that a held-out set of baseline (sober) data can be used to achieve comparable gains to other speaker normalization techniques. Our fused frame-level statistic-functional systems, fused GMM systems, and final combined system achieve unweighted average recalls (UARs) of 69.7%, 65.1%, and 68.8%, respectively, on the test set. More consistent numbers compared to development set results occur with matched-prompt training, where the UARs are 70.4%, 66.2%, and 71.4%, respectively. The combined system improves over the Challenge baseline by 5.5% absolute (8.4% relative), also improving upon our previously best result.","['Daniel Bone', 'Ming Li', 'Matthew P. Black', 'Shrikanth S. Narayanan']",March 2014,Computer Speech & Language,"['GMM supervectors', 'Speaker normalization', 'Hierarchical features', 'Intoxication detection', 'Speaker state', 'Cognitive and motor load']",Intoxicated speech detection: A fusion framework with speaker-normalized hierarchical functionals and GMM supervectors☆
408,"This paper presents our studies of the effects of acoustic features, speaker normalization methods, and statistical modeling techniques on speaker state classification. We focus on the investigation of the effect of simple partial least squares (SIMPLS) in unbalanced binary classification. Beyond dimension reduction and low computational complexity, SIMPLS classifier (SIMPLSC) shows, especially, higher prediction accuracy to the class with the smaller data number. Therefore, an asymmetric SIMPLS classifier (ASIMPLSC) is proposed to enhance the performance of SIMPLSC to the class with the larger data number. Furthermore, we combine multiple system outputs (ASIMPLS classifier and Support Vector Machines) by score-level fusion to exploit the complementary information in diverse systems. The proposed speaker state classification system is evaluated with several experiments on unbalanced data sets. Within the Interspeech 2011 Speaker State Challenge, we could achieve the best results for the 2-class task of the Sleepiness Sub-Challenge with an unweighted average recall of 71.7%. Further experimental results on the SEMAINE data sets show that the ASIMPLSC achieves an absolute improvement of 6.1%, 6.1%, 24.5%, and 1.3% on the weighted average recall value, over the AVEC 2011 baseline system on the emotional speech binary classification tasks of four dimensions, namely, activation, expectation, power, and valence, respectively.","['Dong-Yan Huang', 'Zhengchen Zhang', 'Shuzhi Sam Ge']",March 2014,Computer Speech & Language,"['Speaker state recognition', 'Partial least squares', 'Asymmetric SIMPLS', 'Support Vector Machine', 'Fusion', 'Sleepiness detection', 'Speech emotion recognition']",Speaker state classification based on fusion of asymmetric simple partial least squares (SIMPLS) and support vector machines☆
409,"Automatic detection of a user's interest in spoken dialog plays an important role in many applications, such as tutoring systems and customer service systems. In this study, we propose a decision-level fusion approach using acoustic and lexical information to accurately sense a user's interest at the utterance level. Our system consists of three parts: acoustic/prosodic model, lexical model, and a model that combines their decisions for the final output. We use two different regression algorithms to complement each other for the acoustic model. For lexical information, in addition to the bag-of-words model, we propose new features including a level-of-interest value for each word, length information using the number of words, estimated speaking rate, silence in the utterance, and similarity with other utterances. We also investigate the effectiveness of using more automatic speech recognition (ASR) hypotheses (n-best lists) to extract lexical features. The outputs from the acoustic and lexical models are combined at the decision level. Our experiments show that combining acoustic evidence with lexical information improves level-of-interest detection performance, even when lexical features are extracted from ASR output with high word error rate.","['Je Hun Jeon', 'Rui Xia', 'Yang Liu']",March 2014,Computer Speech & Language,"['Level of interest', 'Decision-level fusion', 'Human–machine interaction']",Level of interest sensing in spoken dialog using decision-level fusion of acoustic and lexical evidence☆
410,"Obstructive sleep apnoea (OSA) is a highly prevalent disease affecting an estimated 2–4% of the adult male population that is difficult and very costly to diagnose because symptoms can remain unnoticed for years. The reference diagnostic method, Polysomnography (PSG), requires the patient to spend a night at the hospital monitored by specialized equipment. Therefore fast and less costly screening techniques are normally applied for setting priorities to proceed to the polysomnography diagnosis. In this article the use of speech analysis is proposed as an alternative or complement to existing screening methods. A set of voice features that could be related to apnoea are defined, based on previous results from other authors and our own analysis. These features are analyzed first in isolation and then in combination to assess their discriminative power to classify voices as corresponding to apnoea patients and healthy subjects. This analysis is performed in a database containing three repetitions of four carefully designed sentences read by 40 healthy subjects and 42 subjects suffering from severe apnoea. As a result of the analysis, a linear discriminant model (LDA) was defined including a subset of eight features (signal-to-disperiodicity ratio, a nasality measure, harmonic-to-noise ratio, jitter, difference between third and second formants on a specific vowel, duration of two of the sentences and the percentage of silence in one of the sentences). This model was tested on a separate database containing 20 healthy and 20 apnoea subjects yielding a sensitivity of 85% and a specificity of 75%, with a F1-measure of 81%. These results indicate that the proposed method, only requiring a few minutes to record and analyze the patient's voice during the visit to the specialist, could help in the development of a non-intrusive, fast and convenient PSG-complementary screening technique for OSA.","['Ana Montero Benavides', 'Rubén Fernández Pozo', 'Doroteo T. Toledano', 'José Luis Blanco Murillo', 'Eduardo López Gonzalo', 'Luis Hernández Gómez']",March 2014,Computer Speech & Language,"['OSAobstructive sleep apnoea', 'obstructive sleep apnoea', 'PSGpolysomnography', 'polysomnography', 'AHIapnoea–hypopnoea index', 'apnoea–hypopnoea index', 'BMIbody mass index', 'body mass index', 'CERclassification error rate', 'classification error rate', 'EERequal error rate', 'equal error rate', 'LDAlinear discriminant analysis', 'linear discriminant analysis', 'MLRmultiple linear regression', 'multiple linear regression', 'OSA screening tools', 'Apnoea discrimination', 'Voice features', 'Feature selection', 'Voice pathology']",Analysis of voice features related to obstructive sleep apnoea and their application in diagnosis support☆
411,"This article uses prolonged oral reading corpora for various experiments to analyze and detect vocal fatigue. Vocal fatigue particularly concerns voice professionals, including teachers, telemarketing operators, users of automatic speech recognition technology and actors. In analyzing and detecting vocal fatigue, we focused our investigations on three main experiments: a prosodic analysis that can be compared to the results found in related work, a two-class Support Vector Machines (SVM) classifier into Fatigue and Non-Fatigue states using a large set of audio features and a comparison function that estimates the difference in fatigue level between two speech segments using a combination of multiple phoneme-based comparison functions. The experiments on prosodic analysis showed that vocal fatigue was not associated with an increase in fundamental frequency and voice intensity. A two-class SVM classifier using the Paralinguistic Challenge 2010 audio feature set gave an unweighted accuracy of 94.1% for the training set (10-fold cross-validation) and 68.2% for the test set. These results show that the phenomenon of vocal fatigue can be modeled and detected. The comparison function was assessed by detecting increased fatigue levels between two speech segments. The fatigue level detection performance in Equal Error Rate (EER) was 31% using all phonetic segments and yielded EER of 21% after filtering phonetic segments and 19% after filtering phonetic segments and cepstral features. These results show that some phonemes are more sensitive than others to vocal fatigue. These experiments show that the fatigued voice has specific characteristics for prolonged oral reading and suggest the feasibility of vocal fatigue detection.","['Marie-José Caraty', 'Claude Montacié']",March 2014,Computer Speech & Language,"['Vocal fatigue', 'Fatigue detection', 'Voice analysis', 'Audio features', 'Automatic classification']",Vocal fatigue induced by prolonged oral reading: Analysis and detection?
412,"It is generally acknowledged that an unbiased and objective assessment of the communication deficiency caused by a speech disorder calls for automatic speech processing tools. In this paper, a new automatic intelligibility assessment method is presented. The method can predict running speech intelligibility in a way that is robust against changes in the text and against differences in the accent of the speaker. It is evaluated on a Dutch corpus comprising longitudinal data of several speakers who have been treated for cancer of the head and the neck. The results show that the method is as accurate as a human listener in detecting trends in the intelligibility over time. By evaluating the intelligibility predictions made with different models trained on distinct texts and accented speech data, evidence for the robustness of the method against text and accent factors is offered.","['Catherine Middag', 'Renee Clapham', 'Rob van Son', 'Jean-Pierre Martens']",March 2014,Computer Speech & Language,"['Intelligibility', 'Pathological speech', 'Text-independent', 'Phonological features', 'Automatic intelligibility assessment']",Robust automatic intelligibility assessment techniques evaluated on speakers treated for head and neck cancer
413,"The role of automatic emotion recognition from speech is growing continuously because of the accepted importance of reacting to the emotional state of the user in human–computer interaction. Most state-of-the-art emotion recognition methods are based on turn- and frame-level analysis independent from phonetic transcription. Here, we are interested in a phoneme-based classification of the level of arousal in acted and spontaneous emotions. To start, we show that our previously published classification technique which showed high-level results in the Interspeech 2009 Emotion Challenge cannot provide sufficiently good classification in cross-corpora evaluation (a condition close to real-life applications). To prove the robustness of our emotion classification techniques we use cross-corpora evaluation for a simplified two-class problem; namely high and low arousal emotions. We use emotion classes on a phoneme-level for classification. We build our speaker-independent emotion classifier with HMMs, using GMMs-based production probabilities and MFCC features. This classifier performs equally well when using a complete phoneme set, as it does in the case of a reduced set of indicative vowels (7 out of 39 phonemes in the German SAM-PA list). Afterwards we compare emotion classification performance of the technique used in the Emotion Challenge with phoneme-based classification within the same experimental setup. With phoneme-level emotion classes we increase cross-corpora classification performance by about 3.15% absolute (4.69% relative) for models trained on acted emotions (EMO-DB dataset) and evaluated on spontaneous emotions (VAM dataset); within vice versa experimental conditions (trained on VAM, tested on EMO-DB) we obtain 15.43% absolute (23.20% relative) improvement. We show that using phoneme-level emotion classes can improve classification performance even with comparably low speech recognition performance obtained with scant a priori knowledge about the language, implemented as a zero-gram for word-level modeling and a bi-gram for phoneme-level modeling. Finally we compare our results with the state-of-the-art cross-corpora evaluations on the VAM database. For training our models, we use an almost 15 times smaller training set, consisting of 456 utterances (210 low and 246 high arousal emotions) instead of 6820 utterances (4685 high and 2135 low arousal emotions). We are yet able to increase cross-corpora classification performance by about 2.25% absolute (3.22% relative) from UA = 69.7% obtained by Zhang et al. to UA = 71.95%.","['Bogdan Vlasenko', 'Dmytro Prylipko', 'Ronald Böck', 'Andreas Wendemuth']",March 2014,Computer Speech & Language,"['Emotion phonetic pattern', 'Emotion phoneme classes', 'Cross-corpora evaluation', 'Affective speech', 'Emotion classification', 'Level of arousal classification']",Modeling phonetic pattern variability in favor of the creation of robust emotion classifiers for real-life applications☆
414,"Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: “singing enthusiasm”. First, we investigated whether human listeners can evaluate singing enthusiasm consistently and whether the listener's perception matches the singer's intended enthusiasm. We then identified three acoustic features relevant to the perception of singing enthusiasm: A-weighted power, “fall-down”, and vibrato extent. Finally, we developed a method for combining the selected three features to estimate the value of singing enthusiasm, and obtained a correlation coefficient of 0.65 between the estimated value and human evaluation.","['Ryunosuke Daido', 'Masashi Ito', 'Shozo Makino', 'Akinori Ito']",March 2014,Computer Speech & Language,"['Singing enthusiasm', 'Singing voice', 'Perception of singing voice', 'Karaoke']",Automatic evaluation of singing enthusiasm for karaoke☆
415,"In human–human interactions, entrainment is a naturally occurring phenomenon that happens when interlocutors mutually adapt their behaviors through the course of an interaction. This mutual behavioral dependency has been at the center of psychological studies of human communication for decades. Quantitative descriptors of the degree of entrainment can provide psychologists an objective method to advance studies of human communication including in mental health domains. However, the subtle nature of the entrainment phenomenon makes it challenging for computing such an effect based on just human annotations. In this paper, we propose an unsupervised signal-derived approach within a principal component analysis framework for quantifying one aspect of entrainment in communication, namely, vocal entrainment. The proposed approach to quantify the degree of vocal entrainment involves measuring the similarity of specific vocal characteristics between the interlocutors in a dialog. These quantitative descriptors were analyzed using two psychology-inspired hypothesis tests to not only establish that these signal-derived measures carry meaningful information in interpersonal communication but also offer statistical evidence into aspects of behavioral dependency and associated affective states in marital conflictual interactions. Finally, affect recognition experiments were performed with the proposed vocal entrainment descriptors as features using a large database of real distressed married couples’ interactions. An accuracy of 62.56% in differentiating between positive and negative affect was obtained using these entrainment measures with Factorial Hidden Markov Models lending further support that entrainment is an active component underlying affective processes in interactions.","['Chi-Chun Lee', 'Athanasios Katsamanis', 'Matthew P. Black', 'Brian R. Baucom', 'Andrew Christensen', 'Panayiotis G. Georgiou', 'Shrikanth S. Narayanan']",March 2014,Computer Speech & Language,"['Entrainment', 'Interaction synchrony', 'Principal component analysis (PCA)', 'Couple therapy', 'Behavioral signal processing (BSP)', 'Factorial Hidden Markov Model', 'Affect recognition', 'Dyadic interaction']",Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions☆☆☆
416,,"['Martin Cooke', 'Simon King', 'Bastiaan Kleijn', 'Yannis Stylianou']",March 2014,Computer Speech & Language,[],Introduction to the Special Issue on The listening talker: context-dependent speech production and perception
417,"Speech output technology is finding widespread application, including in scenarios where intelligibility might be compromised – at least for some listeners – by adverse conditions. Unlike most current algorithms, talkers continually adapt their speech patterns as a response to the immediate context of spoken communication, where the type of interlocutor and the environment are the dominant situational factors influencing speech production. Observations of talker behaviour can motivate the design of more robust speech output algorithms. Starting with a listener-oriented categorisation of possible goals for speech modification, this review article summarises the extensive set of behavioural findings related to human speech modification, identifies which factors appear to be beneficial, and goes on to examine previous computational attempts to improve intelligibility in noise. The review concludes by tabulating 46 speech modifications, many of which have yet to be perceptually or algorithmically evaluated. Consequently, the review provides a roadmap for future work in improving the robustness of speech output.","['Martin Cooke', 'Simon King', 'Maëva Garnier', 'Vincent Aubanel']",March 2014,Computer Speech & Language,"['Speech production', 'Modification algorithms']",The listening talker: A review of human and algorithmic context-induced modifications of speech☆
418,"This study investigated whether the signal-to-noise ratio (SNR) of the interlocutor (speech partner) influences a speaker's vocal intensity in conversational speech. Twenty participants took part in artificial conversations with controlled levels of interlocutor speech and background noise. Three different levels of background noise were presented over headphones and the participant engaged in a “live interaction” with the experimenter. The experimenter's vocal intensity was manipulated in order to modify the SNR. The participants’ vocal intensity was measured. As observed previously, vocal intensity increased as background noise level increased. However, the SNR of the interlocutor did not have a significant effect on participants’ vocal intensity. These results suggest that increasing the signal level of the other party at the earpiece would not reduce the tendency of telephone users to talk loudly","['Rebecca S. Tweedy', 'John F. Culling']",March 2014,Computer Speech & Language,"['Vocal effort', 'Signal-to-noise ratio', 'Conversation']",Does the signal-to-noise ratio of an interlocutor influence a speaker's vocal intensity?☆
419,"What makes speech produced in the presence of noise (Lombard speech) more intelligible than conversational speech produced in quiet conditions? This study investigates the hypothesis that speakers modify their speech in the presence of noise in such a way that acoustic contrasts between their speech and the background noise are enhanced, which would improve speech audibility.Ten French speakers were recorded while playing an interactive game first in quiet condition, then in two types of noisy conditions with different spectral characteristics: a broadband noise (BB) and a cocktail-party noise (CKTL), both played over loudspeakers at 86 dB SPL.Similarly to (Lu and Cooke, 2009b), our results suggest no systematic “active” adaptation of the whole speech spectrum or vocal intensity to the spectral characteristics of the ambient noise. Regardless of the type of noise, the gender or the type of speech segment, the primary strategy was to speak louder in noise, with a greater adaptation in BB noise and an emphasis on vowels rather than any type of consonants.Active strategies were evidenced, but were subtle and of second order to the primary strategy of speaking louder: for each gender, fundamental frequency (f0) and first formant frequency (F1) were modified in cocktail-party noise in a way that optimized the release in energetic masking induced by this type of noise. Furthermore, speakers showed two additional modifications as compared to shouted speech, which therefore cannot be interpreted in terms of vocal effort only: they enhanced the modulation of their speech in f0 and vocal intensity and they boosted their speech spectrum specifically around 3 kHz, in the region of maximum ear sensitivity associated with the actor's or singer's formant.","['Maëva Garnier', 'Nathalie Henrich']",March 2014,Computer Speech & Language,"['Lombard speech', 'Noise', 'Production', 'Speech audibility', 'Auditory detection', 'Segregation', 'Energetic masking']",Speaking in noise: How does the Lombard effect improve acoustic contrasts between speech and ambient noise?☆
420,"The study investigated whether properties of speech produced in noise (Lombard speech) were more distributed (thus potentially more distinct) and/or more consistent than those from speech produced in quiet. This was examined for auditory tokens by measuring vowel space dispersion and by determining the consistency of formant production across repeated instances. Vowel space was not expanded for speech produced in noise; there was a tendency for formants to be produced more consistently in noise (with less variation in formant frequency across repeated instances) but this was not a secure effect. The distinctiveness and consistency of Lombard visual speech were also examined using motion capture data. Relative distinctiveness was determined by comparing the amount of mouth and jaw motion for speech produced in noise and quiet; relative consistency by comparing the size of correlations for motion produced across repeated instances in the noise or in quiet conditions. Mouth, and jaw motion was larger for speech in noise, however there was no greater association between the movement measures for repeated instances of speech in noise compared to in quiet. We also examined whether the correlation between auditory and motion properties was greater for speech produced in noise than in quiet. It was found that the association between speech RMS energy and jaw motion was greater for speech in noise. The results show that although Lombard speech affects both auditory and visible articulatory properties in ways likely to enhance speech perception it does not increase production consistency.","['Jeesun Kim', 'Chris Davis']",March 2014,Computer Speech & Language,"['Vowel dispersion', 'Speech consistency', 'Lombard speech', 'Visual speech', 'Auditory visual correlation', 'Speech in noise', 'Vowel space']",Comparing the consistency and distinctiveness of speech produced in quiet and in noise☆
421,"In this paper we study the production and perception of speech in diverse conditions for the purposes of accurate, flexible and highly intelligible talking face animation. We recorded audio, video and facial motion capture data of a talker uttering a set of 180 short sentences, under three conditions: normal speech (in quiet), Lombard speech (in noise), and whispering. We then produced an animated 3D avatar with similar shape and appearance as the original talker and used an error minimization procedure to drive the animated version of the talker in a way that matched the original performance as closely as possible. In a perceptual intelligibility study with degraded audio we then compared the animated talker against the real talker and the audio alone, in terms of audio-visual word recognition rate across the three different production conditions. We found that the visual intelligibility of the animated talker was on par with the real talker for the Lombard and whisper conditions. In addition we created two incongruent conditions where normal speech audio was paired with animated Lombard speech or whispering. When compared to the congruent normal speech condition, Lombard animation yields a significant increase in intelligibility, despite the AV-incongruence. In a separate evaluation, we gathered subjective opinions on the different animations, and found that some degree of incongruence was generally accepted.","['Simon Alexanderson', 'Jonas Beskow']",March 2014,Computer Speech & Language,"['Lombard effect', 'Motion capture', 'Speech-reading', 'Lip-reading', 'Facial animation', 'Audio-visual intelligibility']","Animated Lombard speech: Motion capture, facial animation and visual intelligibility of speech produced in adverse conditions"
422,Post-filtering can be used in mobile communications to improve the quality and intelligibility of speech. Energy reallocation with a high-pass type filter has been shown to work effectively in improving the intelligibility of speech in difficult noise conditions. This paper introduces a post-filtering algorithm that adapts to the background noise level as well as to the fundamental frequency of the speaker and models the spectral effects observed in natural Lombard speech. The introduced method and another post-filtering technique were compared to unprocessed telephone speech in subjective listening tests in terms of intelligibility and quality. The results indicate that the proposed method outperforms the reference method in difficult noise conditions.,"['Emma Jokinen', 'Marko Takanen', 'Martti Vainio', 'Paavo Alku']",March 2014,Computer Speech & Language,"['Speech enhancement', 'Telephone speech', 'Post-filtering', 'Intelligibility', 'Lombard effect']",An adaptive post-filtering method producing an artificial Lombard-like effect for intelligibility enhancement of narrowband telephone speech☆
423,"Lombard and Clear speech represent two acoustically and perceptually distinct speaking styles that humans employ to increase intelligibility. For Lombard speech, increased spectral energy in a band spanning the range of formants is consistent, effectively augmenting loudness, while vowel space expansion is exhibited in Clear speech, indicating greater articulation. On the other hand, analyses in the first part of this work illustrate that Clear speech does not exhibit significant spectral energy boosting, nor does the Lombard effect invoke an expansion of vowel space. Accordingly, though these two acoustic phenomena are largely attributed with the respective intelligibility gains of the styles, present analyses would suggest that they are mutually exclusive in human speech production. However, these phenomena can be used to inspire signal processing algorithms that seek to exploit and ultimately compound their respective intelligibility gains, as is explored in the second part of this work. While Lombard-inspired spectral shaping has been shown to successfully increase intelligibility, Clear speech-inspired modifications to expand vowel space are rarely explored. With this in mind, the latter part of this work focuses mainly on a novel frequency warping technique that is shown to achieve vowel space expansion. The frequency warping is then incorporated into an established Lombard-inspired Spectral Shaping method that pairs with dynamic range compression to maximize speech audibility (SSDRC). Finally, objective and subjective evaluations are presented in order to assess and compare the intelligibility gains of the different styles and their inspired modifications.","['Elizabeth Godoy', 'Maria Koutsogiannaki', 'Yannis Stylianou']",March 2014,Computer Speech & Language,"['Lombard effect', 'Clear speech', 'Intelligibility enhancement']",Approaching speech intelligibility enhancement with inspiration from Lombard and Clear speaking styles☆
424,"This papers studies the synthesis of speech over a wide vocal effort continuum and its perception in the presence of noise. Three types of speech are recorded and studied along the continuum: breathy, normal, and Lombard speech. Corresponding synthetic voices are created by training and adapting the statistical parametric speech synthesis system GlottHMM. Natural and synthetic speech along the continuum is assessed in listening tests that evaluate the intelligibility, quality, and suitability of speech in three different realistic multichannel noise conditions: silence, moderate street noise, and extreme street noise. The evaluation results show that the synthesized voices with varying vocal effort are rated similarly to their natural counterparts both in terms of intelligibility and suitability.","['Tuomo Raitio', 'Antti Suni', 'Martti Vainio', 'Paavo Alku']",March 2014,Computer Speech & Language,"['Statistical parametric speech synthesis', 'Adaptation', 'Vocal effort', 'Lombard speech', 'Breathy speech', 'Intelligibility']","Synthesis and perception of breathy, normal, and Lombard speech in the presence of noise☆"
425,"This paper describes speech intelligibility enhancement for Hidden Markov Model (HMM) generated synthetic speech in noise. We present a method for modifying the Mel cepstral coefficients generated by statistical parametric models that have been trained on plain speech. We update these coefficients such that the glimpse proportion – an objective measure of the intelligibility of speech in noise – increases, while keeping the speech energy fixed. An acoustic analysis reveals that the modified speech is boosted in the region 1–4 kHz, particularly for vowels, nasals and approximants. Results from listening tests employing speech-shaped noise show that the modified speech is as intelligible as a synthetic voice trained on plain speech whose duration, Mel cepstral coefficients and excitation signal parameters have been adapted to Lombard speech from the same speaker. Our proposed method does not require these additional recordings of Lombard speech. In the presence of a competing talker, both modification and adaptation of spectral coefficients give more modest gains.","['Cassia Valentini-Botinhao', 'Junichi Yamagishi', 'Simon King', 'Ranniery Maia']",March 2014,Computer Speech & Language,"['Intelligibility of speech in noise', 'HMM-based speech synthesis', 'Mel cepstral coefficients', 'Glimpse proportion measure']",Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the glimpse proportion☆
426,"Hypo and hyperarticulation refer to the production of speech with respectively a reduction and an increase of the articulatory efforts compared to the neutral style. Produced consciously or not, these variations of articulatory efforts depend upon the surrounding environment, the communication context and the motivation of the speaker with regard to the listener. The goal of this work is to integrate hypo and hyperarticulation into speech synthesizers, such that they are more realistic by automatically adapting their way of speaking to the contextual situation, like humans do. Based on our preliminary work, this paper provides a thorough and detailed study on the analysis and synthesis of hypo and hyperarticulated speech. It is divided into three parts. In the first one, we focus on both acoustic and phonetic modifications due to articulatory effort changes. The second part aims at developing a HMM-based speech synthesizer allowing a continuous control of the degree of articulation. This requires to first tackle the issue of speaking style adaptation to derive hypo and hyperarticulated speech from the neutral synthesizer. Once this is done, an interpolation and extrapolation of the resulting models enables to finely tune the voice so that it is generated with the desired articulatory efforts. Finally the third and last part focuses on a perceptual study of speech with a variable articulation degree, where it is analyzed how intelligibility and various other voice dimensions are affected.","['Benjamin Picart', 'Thomas Drugman', 'Thierry Dutoit']",March 2014,Computer Speech & Language,"['Speech synthesis', 'HTS', 'Speech analysis', 'Expressive speech', 'Speaking style adaptation', 'Voice quality', 'Speech intelligibility']",Analysis and HMM-based synthesis of hypo and hyperarticulated speech☆
427,"Recent years have witnessed a surge of interest in computational methods for affect, ranging from opinion mining, to subjectivity detection, to sentiment and emotion analysis. This article presents a brief overview of the latest trends in the field and describes the manner in which the articles contained in the special issue contribute to the advancement of the area. Finally, we comment on the current challenges and envisaged developments of the subjectivity and sentiment analysis fields, as well as their application to other Natural Language Processing tasks and related domains.","['Alexandra Balahur', 'Rada Mihalcea', 'Andrés Montoyo']",January 2014,Computer Speech & Language,"['Subjectivity analysis', 'Sentiment analysis', 'Multilingual resources', 'Social Media mining', 'Chat analysis']",PrefaceComputational approaches to subjectivity and sentiment analysis: Present and envisaged methods and applications
428,"Recent research on English word sense subjectivity has shown that the subjective aspect of an entity is a characteristic that is better delineated at the sense level, instead of the traditional word level. In this paper, we seek to explore whether senses aligned across languages exhibit this trait consistently, and if this is the case, we investigate how this property can be leveraged in an automatic fashion. We first conduct a manual annotation study to gauge whether the subjectivity trait of a sense can be robustly transferred across language boundaries. An automatic framework is then introduced that is able to predict subjectivity labeling for unseen senses using either cross-lingual or multilingual training enhanced with bootstrapping. We show that the multilingual model consistently outperforms the cross-lingual one, with an accuracy of over 73% across all iterations.","['Carmen Banea', 'Rada Mihalcea', 'Janyce Wiebe']",January 2014,Computer Speech & Language,"['Sentiment and text classification', 'Multilingual subjectivity analysis', 'Sense level subjectivity']",Sense-level subjectivity in a multilingual setting☆
429,"SAMAR is a system for subjectivity and sentiment analysis (SSA) for Arabic social media genres. Arabic is a morphologically rich language, which presents significant complexities for standard approaches to building SSA systems designed for the English language. Apart from the difficulties presented by the social media genres processing, the Arabic language inherently has a high number of variable word forms leading to data sparsity. In this context, we address the following 4 pertinent issues: how to best represent lexical information; whether standard features used for English are useful for Arabic; how to handle Arabic dialects; and, whether genre specific features have a measurable impact on performance. Our results show that using either lemma or lexeme information is helpful, as well as using the two part of speech tagsets (RTS and ERTS). However, the results show that we need individualized solutions for each genre and task, but that lemmatization and the ERTS POS tagset are present in a majority of the settings.","['Muhammad Abdul-Mageed', 'Mona Diab', 'Sandra Kübler']",January 2014,Computer Speech & Language,"['Subjectivity and sentiment analysis', 'Morphologically rich language', 'Arabic', 'Social media data']",SAMAR: Subjectivity and sentiment analysis for Arabic social media☆
430,"This paper presents our research on automatic annotation of a five-billion-word corpus of Japanese blogs with information on affect and sentiment. We first perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the Japanese language. We choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis: ML-Ask for word- and sentence-level affect analysis and CAO for detailed analysis of emoticons. The annotated information includes affective features like sentence subjectivity (emotive/non-emotive) or emotion classes (joy, sadness, etc.), useful in affect analysis. The annotations are also generalized on a two-dimensional model of affect to obtain information on sentence valence (positive/negative), useful in sentiment analysis. The annotations are evaluated in several ways. Firstly, on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents. Secondly, the statistics of annotations are compared to other existing emotion blog corpora. Finally, the corpus is applied in several tasks, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions.","['Michal Ptaszynski', 'Rafal Rzepka', 'Kenji Araki', 'Yoshio Momouchi']",January 2014,Computer Speech & Language,"['Emotion corpora', 'Corpus annotation', 'Sentiment analysis', 'Affect analysis']",Automatically annotating a five-billion-word corpus of Japanese blogs for sentiment and affect analysis☆
431,"Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts. In recent years, due to the growth in the quantity and fast spreading of user-generated contents online and the impact such information has on events, people and companies worldwide, this task has been approached in an important body of research in the field. Despite different methods having been proposed for distinct types of text, the research community has concentrated less on developing methods for languages other than English. In the above-mentioned context, the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less/no resources are available for this task when compared to English, stressing upon the impact of translation quality on the sentiment classification performance. Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can, in combination to appropriate machine learning algorithms and carefully chosen features, be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.","['Alexandra Balahur', 'Marco Turchi']",January 2014,Computer Speech & Language,"['Multilingual sentiment analysis', 'Opinion mining', 'Machine translation', 'Supervised learning']",Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis☆
432,"A set of words labeled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. It may be that no simple function of the labels on the individual words captures the overall emotion of the sentence; words are interrelated and they mutually influence their affect-related interpretation. It happens quite often that a word which invokes emotion appears in a neutral sentence, or that a sentence with no emotional word carries an emotion. This could also happen among different emotion classes. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important in this task. We present a set of features which enable us to take the contextual emotion of a word and the syntactic structure of the sentence into account to put sentences into emotion classes. The evaluation includes assessing the performance of different feature sets across multiple classification methods. We show the features and a promising learning method which significantly outperforms two reasonable baselines. We group our features by the similarity of their nature. That is why another facet of our evaluation is to consider each group of the features separately and investigate how well they contribute to the result. The experiments show that all features contribute to the result, but it is the combination of all the features that gives the best performance.","['Diman Ghazi', 'Diana Inkpen', 'Stan Szpakowicz']",January 2014,Computer Speech & Language,"['Sentiment analysis', 'Polarity', 'Emotion', 'Prior emotion', 'Contextual emotion', 'Syntactic features', 'Natural language processing', 'Machine learning']",Prior and contextual emotion of words in sentential context☆
433,"This paper presents a novel approach to Sentiment Polarity Classification in Twitter posts, by extracting a vector of weighted nodes from the graph of WordNet. These weights are used in SentiWordNet to compute a final estimation of the polarity. Therefore, the method proposes a non-supervised solution that is domain-independent. The evaluation of a generated corpus of tweets shows that this technique is promising.","['Arturo Montejo-Ráez', 'Eugenio Martínez-Cámara', 'M. Teresa Martín-Valdivia', 'L. Alfonso Ureña-López']",January 2014,Computer Speech & Language,"['Sentiment Analysis', 'Opinion Mining', 'Unsupervised method', 'Twitter', 'PageRank', 'SentiWordNet', 'Polarity classification']",Ranked WordNet graph for Sentiment Polarity Classification in Twitter☆
434,"In this paper, we suggest a list of high-level features and study their applicability in detection of cyberpedophiles. We used a corpus of chats downloaded from http://www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online, and the NPS chat corpus. The classification results show that the NPS data and the pedophiles’ conversations can be accurately discriminated from each other with character n-grams, while in the more complicated case of cybersex logs there is need for high-level features to reach good accuracy levels. In this latter setting our results show that features that model behaviour and emotion significantly outperform the low-level ones, and achieve a 97% accuracy.","['Dasha Bogdanova', 'Paolo Rosso', 'Thamar Solorio']",January 2014,Computer Speech & Language,"['Cyberpedophilia', 'Sentiment analysis', 'Emotion detection']",Exploring high-level features for detecting cyberpedophilia
435,"This paper investigates advanced channel compensation techniques for the purpose of improving i-vector speaker verification performance in the presence of high intersession variability using the NIST 2008 and 2010 SRE corpora. The performance of four channel compensation techniques: (a) weighted maximum margin criterion (WMMC), (b) source-normalized WMMC (SN-WMMC), (c) weighted linear discriminant analysis (WLDA) and (d) source-normalized WLDA (SN-WLDA) have been investigated. We show that, by extracting the discriminatory information between pairs of speakers as well as capturing the source variation information in the development i-vector space, the SN-WLDA based cosine similarity scoring (CSS) i-vector system is shown to provide over 20% improvement in EER for NIST 2008 interview and microphone verification and over 10% improvement in EER for NIST 2008 telephone verification, when compared to SN-LDA based CSS i-vector system. Further, score-level fusion techniques are analyzed to combine the best channel compensation approaches, to provide over 8% improvement in DCF over the best single approach, SN-WLDA, for NIST 2008 interview/telephone enrolment-verification condition. Finally, we demonstrate that the improvements found in the context of CSS also generalize to state-of-the-art GPLDA with up to 14% relative improvement in EER for NIST SRE 2010 interview and microphone verification and over 7% relative improvement in EER for NIST SRE 2010 telephone verification.","['Ahilan Kanagasundaram', 'David Dean', 'Sridha Sridharan', 'Mitchell McLaren', 'Robbie Vogt']",January 2014,Computer Speech & Language,"['Speaker verification', 'I-vector', 'GPLDA', 'LDA', 'SN-LDA', 'WLDA', 'SN-WLDA']",I-vector based speaker recognition using advanced channel compensation techniques☆
436,"This paper presents a two-stage mixed language model technique for detecting and recognizing words that are not included in the vocabulary of a large vocabulary continuous speech recognition system. The main idea is to spot the out-of-vocabulary words and to produce a transcription for these words in terms of subword units with the help of a mixed word/subword language model in the first stage, and to convert the subword transcriptions to word hypotheses by means of a look-up table in the second stage. The performance of the proposed approach is compared to that of the state-of-the-art hybrid method reported in the literature, both on in-domain and on out-of-domain Dutch spoken material, where the term ‘domain’ refers to the ensemble of topics that were covered in the material from which the lexicon and language model were retrieved. It turns out that the proposed approach is at least equally effective as a hybrid approach when it comes to recognizing in-domain material, and significantly more effective when applied to out-of-domain data. This proves that the proposed approach is easily adaptable to new domains and to new words (e.g. proper names) in the same domain. On the out-of-domain recognition task, the word error rate could be reduced by 12% relative over a baseline system incorporating a 100k word vocabulary and a basic garbage OOV word model.","['Bert Réveil', 'Kris Demuynck', 'Jean-Pierre Martens']",January 2014,Computer Speech & Language,"['Out-of-vocabulary words', 'OOV detection', 'OOV modeling', 'Phoneme-to-grapheme conversion']",An improved two-stage mixed language model approach for handling out-of-vocabulary words in large vocabulary continuous speech recognition☆
437,"Reproducing the smooth vocal tract trajectories is critical for high quality articulatory speech synthesis. This paper presents an adaptive neural control scheme for such a task using fuzzy logic and neural networks. The control scheme estimates motor commands from trajectories of flesh-points on selected articulators. These motor commands are then used to reproduce the trajectories of the underlying articulators in a 2nd order dynamical system. Initial experiments show that the control scheme is able to manipulate the mass-spring based elastic tract walls in a 2-dimensional articulatory synthesizer and to realize efficient speech motor control. The proposed controller achieves high accuracy during on-line tracking of the lips, the tongue, and the jaw in the simulation of consonant–vowel sequences. It also offers salient features such as generality and adaptability for future developments of control models in articulatory synthesis.","['Guangpu Huang', 'Meng Joo Er']",January 2014,Computer Speech & Language,"['Articulatory synthesis', 'Speech motor control', 'Neural networks', 'Fuzzy logic', 'Mass spring damper']",An adaptive neural control scheme for articulatory synthesis of CV sequences☆
438,"This paper describes a new method for building compact context-dependency transducers for finite-state transducer-based ASR decoders. Instead of the conventional phonetic decision tree growing followed by FST compilation, this approach incorporates the phonetic context splitting directly into the transducer construction. The objective function of the split optimization is augmented with a regularization term that measures the number of transducer states introduced by a split. We give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy. This permits using context sizes and features that might otherwise be unmanageable.","['David Rybach', 'Michael Riley', 'Chris Alberti']",January 2014,Computer Speech & Language,"['WFST', 'LVCSR']",Direct construction of compact context-dependency transducers from data☆
439,"Language models are crucial for many tasks in NLP (Natural Language Processing) and n-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement.In this article we show another way of improving the models that also requires no external information. We examine patterns that can be found in large corpora by building semantic spaces (HAL, COALS, BEAGLE and others described in this article). These semantic spaces have never been tested in language modeling before. Our method uses semantic spaces and clustering to build classes for a class-based language model. The class-based model is then coupled with a standard n-gram model to create a very effective language model.Our experiments show that our models reduce the perplexity and improve the accuracy of n-gram language models with no external information added. Training of our models is fully unsupervised. Our models are very effective for inflectional languages, which are particularly hard to model. We show results for five different semantic spaces with different settings and different number of classes. The perplexity tests are accompanied with machine translation tests that prove the ability of proposed models to improve performance of a real-world application.","['Tomáš Brychcín', 'Miloslav Konopík']",January 2014,Computer Speech & Language,"['Class-based language models', 'Semantic spaces', 'HAL', 'COALS', 'BEAGLE', 'Random Indexing', 'Purandare and Pedersen', 'Clustering', 'Inflectional languages', 'Machine translation']",Semantic spaces for improving language modeling☆
440,"We present our approach to unsupervised training of speech recognizers. Our approach iteratively adjusts sound units that are optimized for the acoustic domain of interest. We thus enable the use of speech recognizers for applications in speech domains where transcriptions do not exist. The resulting recognizer is a state-of-the-art recognizer on the optimized units. Specifically we propose building HMM-based speech recognizers without transcribed data by formulating the HMM training as an optimization over both the parameter and transcription sequence space. Audio is then transcribed into these self-organizing units (SOUs). We describe how SOU training can be easily implemented using existing HMM recognition tools. We tested the effectiveness of SOUs on the task of topic classification on the Switchboard and Fisher corpora. On the Switchboard corpus, the unsupervised HMM-based SOU recognizer, initialized with a segmental tokenizer, performed competitively with an HMM-based phoneme recognizer trained with 1 h of transcribed data, and outperformed the Brno University of Technology (BUT) Hungarian phoneme recognizer (Schwartz et al., 2004). We also report improvements, including the use of context dependent acoustic models and lattice-based features, that together reduce the topic verification equal error rate from 12% to 7%. In addition to discussing the effectiveness of the SOU approach, we describe how we analyzed some selected SOU n-grams and found that they were highly correlated with keywords, demonstrating the ability of the SOU technology to discover topic relevant keywords.","['Man-hung Siu', 'Herbert Gish', 'Arthur Chan', 'William Belfield', 'Steve Lowe']",January 2014,Computer Speech & Language,"['Unsupervised training', 'Keyword discovery', 'Self-learning', 'Speech recognition', 'Topic identification']",Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery☆
441,"Language is being increasingly harnessed to not only create natural human–machine interfaces but also to infer social behaviors and interactions. In the same vein, we investigate a novel spoken language task, of inferring social relationships in two-party conversations: whether the two parties are related as family, strangers or are involved in business transactions. For our study, we created a corpus of all incoming and outgoing calls from a few homes over the span of a year. On this unique naturalistic corpus of everyday telephone conversations, which is unlike Switchboard or any other public domain corpora, we demonstrate that standard natural language processing techniques can achieve accuracies of about 88%, 82%, 74% and 80% in differentiating business from personal calls, family from non-family calls, familiar from unfamiliar calls and family from other personal calls respectively. Through a series of experiments with our classifiers, we characterize the properties of telephone conversations and find: (a) that 30 words of openings (beginnings) are sufficient to predict business from personal calls, which could potentially be exploited in designing context sensitive interfaces in smart phones; (b) our corpus-based analysis does not support Schegloff and Sack's manual analysis of exemplars in which they conclude that pre-closings differ significantly between business and personal calls – closing fared no better than a random segment; and (c) the distribution of different types of calls are stable over durations as short as 1–2 months. In summary, our results show that social relationships can be inferred automatically in two-party conversations with sufficient accuracy to support practical applications.","['Anthony Stark', 'Izhak Shafran', 'Jeffrey Kaye']",January 2014,Computer Speech & Language,"['Conversation telephone speech', 'Social networks', 'Social relationships']",Inferring social nature of conversations from words: Experiments on a corpus of everyday telephone conversations☆
442,"We present work on understanding natural language in a situated domain in an incremental, word-by-word fashion. We explore a set of models specified as Markov Logic Networks and show that a model that has access to information about the visual context during an utterance, its discourse context, the words of the utterance, as well as the linguistic structure of the utterance performs best and is robust to noisy speech input. We explore the incremental properties of the models and offer some analysis. We conclude that mlns provide a promising framework for specifying such models in a general, possibly domain-independent way.","['Casey Kennington', 'David Schlangen']",January 2014,Computer Speech & Language,"['Incremental', 'Situated', 'Natural language understanding', 'Dialog systems', 'Markov Logic Networks']",Situated incremental natural language understanding using Markov Logic Networks☆
443,"This paper describes a noisy-channel approach for the normalization of informal text, such as that found in emails, chat rooms, and SMS messages. In particular, we introduce two character-level methods for the abbreviation modeling aspect of the noisy channel model: a statistical classifier using language-based features to decide whether a character is likely to be removed from a word, and a character-level machine translation model. A two-phase approach is used; in the first stage the possible candidates are generated using the selected abbreviation model and in the second stage we choose the best candidate by decoding using a language model. Overall we find that this approach works well and is on par with current research in the field.","['Deana L. Pennell', 'Yang Liu']",January 2014,Computer Speech & Language,"['Text normalization', 'Noisy text', 'NLP applications']",Normalization of informal text
444,"This paper proposes the use of neutral reference models to detect local emotional prominence in the fundamental frequency. A novel approach based on functional data analysis (FDA) is presented, which aims to capture the intrinsic variability of F0 contours. The neutral models are represented by a basis of functions and the testing F0 contour is characterized by the projections onto that basis. For a given F0 contour, we estimate the functional principal component analysis (PCA) projections, which are used as features for emotion detection. The approach is evaluated with lexicon-dependent (i.e., one functional PCA basis per sentence) and lexicon-independent (i.e., a single functional PCA basis across sentences) models. The experimental results show that the proposed system can lead to accuracies as high as 75.8% in binary emotion classification, which is 6.2% higher than the accuracy achieved by a benchmark system trained with global F0 statistics. The approach can be implemented at sub-sentence level (e.g., 0.5 s segments), facilitating the detection of localized emotional information conveyed within the sentence. The approach is validated with the SEMAINE database, which is a spontaneous corpus. The results indicate that the proposed scheme can be effectively employed in real applications to detect emotional speech.","['Juan Pablo Arias', 'Carlos Busso', 'Nestor Becerra Yoma']",January 2014,Computer Speech & Language,"['Emotion detection', 'F0 contour modeling', 'Emotional speech analysis', 'Expressive speech']",Shape-based modeling of the fundamental frequency contour for emotion detection in speech
445,"Since 2008, interview-style speech has become an important part of the NIST speaker recognition evaluations (SREs). Unlike telephone speech, interview speech has lower signal-to-noise ratio, which necessitates robust voice activity detectors (VADs). This paper highlights the characteristics of interview speech files in NIST SREs and discusses the difficulties in performing speech/non-speech segmentation in these files. To overcome these difficulties, this paper proposes using speech enhancement techniques as a pre-processing step for enhancing the reliability of energy-based and statistical-model-based VADs. A decision strategy is also proposed to overcome the undesirable effects caused by impulsive signals and sinusoidal background signals. The proposed VAD is compared with the ASR transcripts provided by NIST, VAD in the ETSI-AMR Option 2 coder, satistical-model (SM) based VAD, and Gaussian mixture model (GMM) based VAD. Experimental results based on the NIST 2010 SRE dataset suggest that the proposed VAD outperforms these conventional ones whenever interview-style speech is involved. This study also demonstrates that (1) noise reduction is vital for energy-based VAD under low SNR; (2) the ASR transcripts and ETSI-AMR speech coder do not produce accurate speech and non-speech segmentations; and (3) spectral subtraction makes better use of background spectra than the likelihood-ratio tests in the SM-based VAD. The segmentation files produced by the proposed VAD can be found in http://bioinfo.eie.polyu.edu.hk/ssvad.","['Man-Wai Mak', 'Hon-Bill Yu']",January 2014,Computer Speech & Language,"['Speaker verification', 'Voice activity detection', 'NIST SRE', 'Statistical model based VAD', 'Spectral subtraction']",A study of voice activity detection techniques for NIST speaker recognition evaluations
446,We report progress towards developing a sensor module that categorizes types of laughter for application in dialogue systems or social-skills training situations. The module will also function as a component to measure discourse engagement in natural conversational speech. This paper presents the results of an analysis into the sounds of human laughter in a very large corpus of naturally occurring conversational speech and our classification of the laughter types according to social function. Various types of laughter were categorized into either polite or genuinely mirthful categories and the analysis of these laughs forms the core of this report. Statistical analysis of the acoustic features of each laugh was performed and a Principal Component Analysis and Classification Tree analysis were performed to determine the main contributing factors in each case. A statistical model was then trained using a Support Vector Machine to predict the most likely category for each laugh in both speaker-specific and speaker-independent manner. Better than 70% accuracy was obtained in automatic classification tests.,"['Hiroki Tanaka', 'Nick Campbell']",January 2014,Computer Speech & Language,"['Laughter', 'Prosody', 'Paralinguistic information', 'Non-verbal behaviour', 'Classification', 'Support Vector Machines']",Classification of social laughter in natural conversational speech☆
447,"This paper presents an original approach to automatic prosodic labeling. Fuzzy logic techniques are used for representing situations of high uncertainty with respect to the category to be assigned to a given prosodic unit. The Fuzzy Integer technique is used to combine the output of different base classifiers. The resulting fuzzy classifier benefits from the different capabilities of the base classifiers for identifying different types of prosodic events. At the same time, the fuzzy classifier identifies the events that are potentially more difficult to be labeled. The classifier has been applied to the identification of ToBI pitch accents. The state of the art on pitch accent multiclass classification reports around 70% accuracy rate. In this paper we describe a fuzzy classifier which assigns more than one label in confusing situations. We show that the pairs of labels that appear in these uncertain situations are consistent with the most confused pairs of labels reported in manual prosodic labeling experiments. Our fuzzy classifier obtains a soft classification rate of 81.8%, which supports the potential of the proposed system for computer assisted prosodic labeling.","['David Escudero-Mancebo', 'César González-Ferreras', 'Carlos Vivaracho-Pascual', 'Valentín Cardeñoso-Payo']",January 2014,Computer Speech & Language,"['Automatic prosodic labeling', 'Fuzzy classification', 'ToBI']",A fuzzy classifier to deal with similarity between labels on automatic prosodic labeling☆
448,"We are delighted to bring you this special issue on speech and language processing for assistive technology. It addresses an important research area that is gaining increased recognition from researchers in speech and language processing as a rich and fulfilling area on which to focus their work, and by researchers in assistive technology as the means to dramatically improve communication technologies for individuals with disabilities. This special issue brings a wide swath of approaches and applications highlighting the variety this area offers.","['Kathleen F. McCoy', 'John L. Arnott', 'Leo Ferres', 'Melanie Fried-Oken', 'Brian Roark']",September 2013,Computer Speech & Language,"['Assistive technology', 'Speech processing', 'Language processing']",ReviewSpeech and Language processing as assistive technologies☆
449,"Speech production errors characteristic of dysarthria are chiefly responsible for the low accuracy of automatic speech recognition (ASR) when used by people diagnosed with it. A person with dysarthria produces speech in a rather reduced acoustic working space, causing typical measures of speech acoustics to have values in ranges very different from those characterizing unimpaired speech. It is unlikely then that models trained on unimpaired speech will be able to adjust to this mismatch when acted on by one of the currently well-studied adaptation algorithms (which make no attempt to address this extent of mismatch in population characteristics).In this work, we propose an interpolation-based technique for obtaining a prior acoustic model from one trained on unimpaired speech, before adapting it to the dysarthric talker. The method computes a ‘background’ model of the dysarthric talker's general speech characteristics and uses it to obtain a more suitable prior model for adaptation (compared to the speaker-independent model trained on unimpaired speech). The approach is tested with a corpus of dysarthric speech acquired by our research group, on speech of sixteen talkers with varying levels of dysarthria severity (as quantified by their intelligibility). This interpolation technique is tested in conjunction with the well-known maximum a posteriori (MAP) adaptation algorithm, and yields improvements of up to 8% absolute and up to 40% relative, over the standard MAP adapted baseline.","['Harsh Vardhan Sharma', 'Mark Hasegawa-Johnson']",September 2013,Computer Speech & Language,"['HMM', 'Dysarthria', 'Acoustic model', 'Adaptation', 'UBM', 'MAP']",Acoustic model adaptation using in-domain background models for dysarthric speech recognition☆
450,"This paper presents a system that transforms the speech signals of speakers with physical speech disabilities into a more intelligible form that can be more easily understood by listeners. These transformations are based on the correction of pronunciation errors by the removal of repeated sounds, the insertion of deleted sounds, the devoicing of unvoiced phonemes, the adjustment of the tempo of speech by phase vocoding, and the adjustment of the frequency characteristics of speech by anchor-based morphing of the spectrum. These transformations are based on observations of disabled articulation including improper glottal voicing, lessened tongue movement, and lessened energy produced by the lungs. This system is a substantial step towards full automation in speech transformation without the need for expert or clinical intervention.Among human listeners, recognition rates increased up to 191% (from 21.6% to 41.2%) relative to the original speech by using the module that corrects pronunciation errors. Several types of modified dysarthric speech signals are also supplied to a standard automatic speech recognition system. In that study, the proportion of words correctly recognized increased up to 121% (from 72.7% to 87.9%) relative to the original speech, across various parameterizations of the recognizer. This represents a significant advance towards human-to-human assistive communication software and human–computer interaction.",['Frank Rudzicz'],September 2013,Computer Speech & Language,"['Speech transformation', 'Dysarthria', 'Intelligibility']",Adjusting dysarthric speech signals to be more intelligible☆
451,"For individuals with severe speech impairment accurate spoken communication can be difficult and require considerable effort. Some may choose to use a voice output communication aid (or VOCA) to support their spoken communication needs. A VOCA typically takes input from the user through a keyboard or switch-based interface and produces spoken output using either synthesised or recorded speech. The type and number of synthetic voices that can be accessed with a VOCA is often limited and this has been implicated as a factor for rejection of the devices. Therefore, there is a need to be able to provide voices that are more appropriate and acceptable for users.This paper reports on a study that utilises recent advances in speech synthesis to produce personalised synthetic voices for 3 speakers with mild to severe dysarthria, one of the most common speech disorders. Using a statistical parametric approach to synthesis, an average voice trained on data from several unimpaired speakers was adapted using recordings of the impaired speech of 3 dysarthric speakers. By careful selection of the speech data and the model parameters, several exemplar voices were produced for each speaker. A qualitative evaluation was conducted with the speakers and listeners who were familiar with the speaker. The evaluation showed that for one of the 3 speakers a voice could be created which conveyed many of his personal characteristics, such as regional identity, sex and age.","['Sarah Creer', 'Stuart Cunningham', 'Phil Green', 'Junichi Yamagishi']",September 2013,Computer Speech & Language,"['Speech synthesis', 'Augmentative and alternative communication', 'Disordered speech', 'Voice output communication aid']",Building personalised synthetic voices for individuals with severe speech impairment☆
452,"Non-speaking people who use Augmentative and Alternative Communication (AAC) systems typically have low rates of communication which reduces their ability to interact with others. Research and development continues in the quest to improve the effectiveness of AAC systems in terms of communication rate and impact. One strategy involves making the basic unit of communication an entire utterance, and designing the AAC system to make the storage, retrieval and production of utterances as easy and efficient as possible. Some approaches take this further and include texts, narratives and/or multimedia material for use in conversation. AAC systems operating in such a manner require a structure for containing and managing conversational material and supporting the production of output during conversation. Ideally such a structure should be modelled on the way actual conversations proceed. A number of partial models for this have been presented thus far. These are reviewed in the paper and an integrated model is then proposed that includes both the structure of a conversation and the way in which an AAC system might produce conversational output (e.g. utterances, texts, multimedia items or combinations of these). Modelling the process in this way gives a structure with which an AAC system can organize the support and guidance that it offers to the person using the system. The paper concludes with consideration of three areas of development for further investigation.","['John L. Arnott', 'Norman Alm']",September 2013,Computer Speech & Language,"['AAC', 'Augmentative communication', 'Conversation', 'Modelling', 'Non-speaking', 'Scaffolding']",Towards the improvement of Augmentative and Alternative Communication through the modelling of conversation
453,"Individuals with severe motor impairments commonly enter text using a single binary switch and symbol scanning methods. We present a new scanning method – Huffman scanning – which uses Huffman coding to select the symbols to highlight during scanning, thus minimizing the expected bits per symbol. With our method, the user can select the intended symbol even after switch activation errors. We describe two varieties of Huffman scanning – synchronous and asynchronous – and present experimental results, demonstrating speedups over row/column and linear scanning.","['Brian Roark', 'Russell Beckley', 'Chris Gibbons', 'Melanie Fried-Oken']",September 2013,Computer Speech & Language,"['Keyboard emulation', 'Language modeling', 'Binary coding', 'Text entry']",Huffman scanning: Using language models within fixed-grid keyboard emulation☆☆☆
454,"One of the most common effects among aphasia patients is the difficulty to recall names or words. Typically, word retrieval problems can be treated through word naming therapeutic exercises. In fact, the frequency and the intensity of speech therapy are key factors in the recovery of lost communication functionalities. In this sense, speech and language technology can have a relevant contribution in the development of automatic therapy methods. In this work, we present an on-line system designed to behave as a virtual therapist incorporating automatic speech recognition technology that permits aphasia patients to perform word naming training exercises. We focus on the study of the automatic word naming detector module and on its utility for both global evaluation and treatment. For that purpose, a database consisting of word naming therapy sessions of aphasic Portuguese native speakers has been collected. In spite of the different patient characteristics and speech quality conditions of the collected data, encouraging results have been obtained thanks to a calibration method that makes use of the patients’ word naming ability to automatically adapt to the patients’ speech particularities.","['Alberto Abad', 'Anna Pompili', 'Angela Costa', 'Isabel Trancoso', 'José Fonseca', 'Gabriela Leal', 'Luisa Farrajota', 'Isabel P. Martins']",September 2013,Computer Speech & Language,"['Aphasia', 'Word naming', 'speech disorder', 'Virtual therapy']",Automatic word naming recognition for an on-line aphasia treatment system☆
455,"The relationship between written and spoken words is convoluted in languages with a deep orthography such as English and therefore it is difficult to devise explicit rules for generating the pronunciations for unseen words. Pronunciation by analogy (PbA) is a data-driven method of constructing pronunciations for novel words from concatenated segments of known words and their pronunciations. PbA performs relatively well with English and outperforms several other proposed methods. However, the method inherently generates several candidate pronunciations and its performance depends critically on a good scoring function to choose the best one of them.Previous PbA algorithms have used several different scoring heuristics such as the product of the frequencies of the component pronunciations of the segments, or the number of different segmentations that yield the same pronunciation, and different combinations of these methods, to evaluate the candidate pronunciations. In this article, we instead propose to use a probabilistically justified scoring rule. We show that this principled approach alone yields better accuracy than any previously published PbA algorithm. Furthermore, combined with certain ad hoc modifications motivated by earlier algorithms, the performance can in some cases be further increased.",['Janne V. Kujala'],August 2013,Computer Speech & Language,"['Pronunciation by analogy', 'Probability estimation', 'Bayesian model averaging']",A probabilistic approach to pronunciation by analogy☆
456,"A challenge in automatic speaker verification is to create a system that is robust to the effects of vocal ageing. To observe the ageing effect, a speaker's voice must be analysed over a period of time, over which, variation in the quality of the voice samples is likely to be encountered. Thus, in dealing with the ageing problem, the related issue of quality must also be addressed. We present a solution to speaker verification across ageing by using a stacked classifier framework to combine ageing and quality information with the scores of a baseline classifier. In tandem, the Trinity College Dublin Speaker Ageing database of 18 speakers, each covering a 30–60 year time range, is presented. An evaluation of a baseline Gaussian Mixture Model–Universal Background Model (GMM–UBM) system using this database demonstrates a progressive degradation in genuine speaker verification scores as ageing progresses. Consequently, applying a conventional threshold, determined using scores at the time of enrolment, results in poor long-term performance. The influence of quality on verification scores is investigated via a number of quality measures. Alongside established signal-based measures, a new model-based measure, Wnorm, is proposed, and its utility is demonstrated on the CSLU database. Combining ageing information with quality measures and the scores from the GMM–UBM system, a verification decision boundary is created in score-ageing-quality space. The best performance is achieved by using scores and ageing in conjunction with the new Wnorm quality measure, reducing verification error by 45% relative to the baseline. This work represents the first comprehensive analysis of speaker verification on a longitudinal speaker database and successfully addresses the associated variability from ageing and quality arte-facts.","['Finnian Kelly', 'Andrzej Drygajlo', 'Naomi Harte']",August 2013,Computer Speech & Language,"['Speaker verification', 'Ageing', 'Quality measures']",Speaker verification in score-ageing-quality classification space☆
457,"LTAG is a rich formalism for performing NLP tasks such as semantic interpretation, parsing, machine translation and information retrieval. Depend on the specific NLP task, different kinds of LTAGs for a language may be developed. Each of these LTAGs is enriched with some specific features such as semantic representation and statistical information that make them suitable to be used in that task. The distribution of these capabilities among the LTAGs makes it difficult to get the benefit from all of them in NLP applications.This paper discusses a statistical model to bridge between two kinds LTAGs for a natural language in order to benefit from the capabilities of both kinds. To do so, an HMM was trained that links an elementary tree sequence of a source LTAG onto an elementary tree sequence of a target LTAG. Training was performed by using the standard HMM training algorithm called Baum–Welch. To lead the training algorithm to a better solution, the initial state of the HMM was also trained by a novel EM-based semi-supervised bootstrapping algorithm.The model was tested on two English LTAGs, XTAG (XTAG-Group, 2001) and MICA's grammar (Bangalore et al., 2009) as the target and source LTAGs, respectively. The empirical results confirm that the model can provide a satisfactory way for linking these LTAGs to share their capabilities together.","['Ali Basirat', 'Heshaam Faili']",August 2013,Computer Speech & Language,"['Tree adjoining grammar', 'LTAG', 'Hidden Markov model', 'XTAG', 'MICA']",Bridge the gap between statistical and hand-crafted grammars☆
458,"This paper proposes a two-stage feedforward neural network (FFNN) based approach for modeling fundamental frequency (F0) values of a sequence of syllables. In this study, (i) linguistic constraints represented by positional, contextual and phonological features, (ii) production constraints represented by articulatory features and (iii) linguistic relevance tilt parameters are proposed for predicting intonation patterns. In the first stage, tilt parameters are predicted using linguistic and production constraints. In the second stage, F0 values of the syllables are predicted using the tilt parameters predicted from the first stage, and basic linguistic and production constraints. The prediction performance of the neural network models is evaluated using objective measures such as average prediction error (μ), standard deviation (σ) and linear correlation coefficient (γX,Y). The prediction accuracy of the proposed two-stage FFNN model is compared with other statistical models such as Classification and Regression Tree (CART) and Linear Regression (LR) models. The prediction accuracy of the intonation models is also analyzed by conducting listening tests to evaluate the quality of synthesized speech obtained after incorporation of intonation models into the baseline system. From the evaluation, it is observed that prediction accuracy is better for two-stage FFNN models, compared to the other models.","['V. Ramu Reddy', 'K. Sreenivasa Rao']",August 2013,Computer Speech & Language,"['Intonation models', 'Prediction accuracy', 'Text-to-speech synthesis', 'Feedforward neural networks', 'Linguistic constraints', 'Production constraints', 'Positional', 'Contextual', 'Phonological', 'Articulatory', 'F0 of syllable', 'Tilt']",Two-stage intonation modeling using feedforward neural networks for syllable based text-to-speech synthesis☆
459,"Spoken European Portuguese (EP) is known to be difficult to understand for L2 learners, due to phenomena such as strong vowel reduction. In this paper, we present a method to automatically generate exercises aimed at improving listening comprehension skills in EP. Learners identify the words pronounced in real speech utterances. The exercises introduce two innovative aspects: using broadcast news videos for curriculum and automatically generating exercises with material updated on a daily basis. The videos are automatically transcribed by a speech recognition engine. A filtering chain, used to select appropriate sentences, was validated by a first survey comprised of both manually and automatically selected sentences. Both sets were assigned good to very good subjective quality scores. A second survey concerned the features of the exercise interface. Subjects with varying self-reported exposure to Portuguese as a second language tested several interfaces and functionalities and highlighted their preferred features. The results confirmed that the largest difficulty was the fast speech rate. All participants valued slowed-down audio and video documents, though this feature was more often used by the lowest proficiency subjects. The exercises were integrated into a Web platform where they are automatically updated daily. Though further evaluation is needed to find whether the platform affords skill acquisition, it is expected to be particularly valuable for distance learners who need opportunities to access authentic audio documents in EP.","['Thomas Pellegrini', 'Rui Correia', 'Isabel Trancoso', 'Jorge Baptista', 'Nuno Mamede', 'Maxine Eskenazi']",August 2013,Computer Speech & Language,"['Computer-Assisted Language Learning', 'Speech processing', 'Automatic generation of learning material', 'Listening comprehension']",ASR-based exercises for listening comprehension practice in European Portuguese☆
460,"A multi-point conference is an efficient and cost effective substitute for a face to face meeting. It involves three or more participants placed in separate locations, where each participant employs a single microphone and camera. The routing and processing of the audiovisual information is very demanding on the network. This raises a need for reducing the amount of information that flows through the system. One solution is to identify the dominant speaker and partially discard information originating from non-active participants. We propose a novel method for dominant speaker identification using speech activity information from time intervals of different lengths. The proposed method processes the audio signal of each participant independently and computes speech activity scores for the immediate, medium and long time-intervals. These scores are compared and the dominant speaker is identified. In comparison to other speaker selection methods, experimental results demonstrate reduction in the number of false speaker switches and improved robustness to transient audio interferences.","['Ilana Volfin', 'Israel Cohen']",June 2013,Computer Speech & Language,"['Speech processing', 'Videoconference', 'Dominant speaker identification', 'Acoustic signal detection', 'Acoustic noise', 'Transient noise']",Dominant speaker identification for multipoint videoconferencing☆☆☆
461,"Audio-visual speech recognition, or the combination of visual lip-reading with traditional acoustic speech recognition, has been previously shown to provide a considerable improvement over acoustic-only approaches in noisy environments, such as that present in an automotive cabin. The research presented in this paper will extend upon the established audio-visual speech recognition literature to show that further improvements in speech recognition accuracy can be obtained when multiple frontal or near-frontal views of a speaker's face are available. A series of visual speech recognition experiments using a four-stream visual synchronous hidden Markov model (SHMM) are conducted on the four-camera AVICAR automotive audio-visual speech database. We study the relative contribution between the side and central orientated cameras in improving visual speech recognition accuracy. Finally combination of the four visual streams with a single audio stream in a five-stream SHMM demonstrates a relative improvement of over 56% in word recognition accuracy when compared to the acoustic-only approach in the noisiest conditions of the AVICAR database.","['Rajitha Navarathna', 'David Dean', 'Sridha Sridharan', 'Patrick Lucey']",June 2013,Computer Speech & Language,"['AVASR', 'AVICAR database', 'Speech recognition', 'Multi-stream HMM', 'Automotive environment']",Multiple cameras for audio-visual speech recognition in an automotive environment☆
462,"State-of-the-art large vocabulary continuous speech recognition (LVCSR) systems often combine outputs from multiple sub-systems that may even be developed at different sites. Cross system adaptation, in which model adaptation is performed using the outputs from another sub-system, can be used as an alternative to hypothesis level combination schemes such as ROVER. Normally cross adaptation is only performed on the acoustic models. However, there are many other levels in LVCSR systems’ modelling hierarchy where complimentary features may be exploited, for example, the sub-word and the word level, to further improve cross adaptation based system combination. It is thus interesting to also cross adapt language models (LMs) to capture these additional useful features. In this paper cross adaptation is applied to three forms of language models, a multi-level LM that models both syllable and word sequences, a word level neural network LM, and the linear combination of the two. Significant error rate reductions of 4.0–7.1% relative were obtained over ROVER and acoustic model only cross adaptation when combining a range of Chinese LVCSR sub-systems used in the 2010 and 2011 DARPA GALE evaluations.","['X. Liu', 'M.J.F. Gales', 'P.C. Woodland']",June 2013,Computer Speech & Language,[],Language model cross adaptation for LVCSR system combination☆☆☆
463,"Traditional emotion models, when tagging single emotions in documents, often ignore the fact that most documents convey complex human emotions. In this paper, we join emotion analysis with topic models to find complex emotions in documents, as well as the intensity of the emotions, and study how the document emotions vary with topics. Hierarchical Bayesian networks are employed to generate the latent topic variables and emotion variables. On average, our model on single emotion classification outperforms the traditional supervised machine learning models such as SVM and Naive Bayes. The other model on the complex emotion classification also achieves promising results. We thoroughly analyze the impact of vocabulary quality and topic quantity to emotion and intensity prediction in our experiments. The distribution of topics such as Friend and Job are found to be sensitive to the documents’ emotions, which we call emotion topic variation in this paper. This reveals the deeper relationship between topics and emotions.","['Fuji Ren', 'Xin Kang']",June 2013,Computer Speech & Language,"['Emotion classification', 'Emotion topic model', 'Complex emotion', 'Hierarchical Bayesian network']",Employing hierarchical Bayesian networks in simple and complex emotion topic analysis☆
464,"Non-negative Tucker decomposition (NTD) is applied to unsupervised training of discrete density HMMs for the discovery of sequential patterns in data, for segmenting sequential data into patterns and for recognition of the discovered patterns in unseen data. Structure constraints are imposed on the NTD such that it shares its parameters with the HMM. Two training schemes are proposed: one uses NTD as a regularizer for the Baum–Welch (BW) training of the HMM, the other alternates between initializing the NTD with the BW output and vice versa. On the task of unsupervised spoken pattern discovery from the TIDIGITS database, both training schemes are observed to improve over BW training in terms of pattern purity, accuracy of the segmentation boundaries and accuracy for speech recognition. Furthermore, we experimentally observe that the alternative training of NTD and BW outperforms the NTD regularized BW, BW training and BW training with simulated annealing.","['Meng Sun', 'Hugo Van hamme']",June 2013,Computer Speech & Language,"['Non-negative Tucker decomposition', 'Hidden Markov models', 'Unsupervised training', 'Regularization', 'Speech recognition', 'Sequential pattern discovery', 'Vocabulary acquisition']",Joint training of non-negative Tucker decomposition and discrete density hidden Markov models
465,"Artificial talkers and speech synthesis systems have long been used as a means of understanding both speech production and speech perception. The development of an airway modulation model is described that simulates the time-varying changes of the glottis and vocal tract, as well as acoustic wave propagation, during speech production. The result is a type of artificial talker that can be used to study various aspects of how sound is generated by humans and how that sound is perceived by a listener. The primary components of the model are introduced and simulation of words and phrases are demonstrated.",['Brad H. Story'],June 2013,Computer Speech & Language,"['Vocal tract', 'Vocal folds', 'Modulation', 'Speech simulation', 'Speech synthesis']",Phrase-level speech simulation with an airway modulation model of speech production☆
466,"This paper proposes the use of Bayesian approaches with the cross likelihood ratio (CLR) as a criterion for speaker clustering within a speaker diarization system, using eigenvoice modelling techniques. The CLR has previously been shown to be an effective decision criterion for speaker clustering using Gaussian mixture models. Recently, eigenvoice modelling has become an increasingly popular technique, due to its ability to adequately represent a speaker based on sparse training data, as well as to provide an improved capture of differences in speaker characteristics. The integration of eigenvoice modelling into the CLR framework to capitalize on the advantage of both techniques has also been shown to be beneficial for the speaker clustering task. Building on that success, this paper proposes the use of Bayesian methods to compute the conditional probabilities in computing the CLR, thus effectively combining the eigenvoice-CLR framework with the advantages of a Bayesian approach to the diarization problem. Results obtained on the 2002 Rich Transcription (RT-02) Evaluation dataset show an improved clustering performance, resulting in a 33.5% relative improvement in the overall diarization error rate (DER) compared to the baseline system.","['David Wang', 'Robert Vogt', 'Sridha Sridharan']",June 2013,Computer Speech & Language,"['Eigenvoice modelling', 'Joint factor analysis', 'Cross likelihood ratio', 'Speaker clustering', 'Speaker diarization']",Eigenvoice modelling for cross likelihood ratio based speaker clustering: A Bayesian approach☆
467,"This paper describes a new algorithm for automatically detecting creak in speech signals. Detection is made by utilising two new acoustic parameters which are designed to characterise creaky excitations following previous evidence in the literature combined with new insights from observations in the current work. In particular the new method focuses on features in the Linear Prediction (LP) residual signal including the presence of secondary peaks as well as prominent impulse-like excitation peaks. These parameters are used as input features to a decision tree classifier for identifying creaky regions. The algorithm was evaluated on a range of read and conversational speech databases and was shown to clearly outperform the state-of-the-art. Further experiments involving degradations of the speech signal demonstrated robustness to both white and babble noise, providing better results than the state-of-the-art down to at least 20 dB signal to noise ratio.","['John Kane', 'Thomas Drugman', 'Christer Gobl']",June 2013,Computer Speech & Language,"['Creak', 'Creaky voice', 'Vocal fry', 'Glottal source', 'Glottal closure instant']",Improved automatic detection of creak
468,,"['Jon Barker', 'Emmanuel Vincent']",May 2013,Computer Speech & Language,[],PrefaceSpecial issue on speech separation and recognition in multisource environments
469,"Distant microphone speech recognition systems that operate with human-like robustness remain a distant goal. The key difficulty is that operating in everyday listening conditions entails processing a speech signal that is reverberantly mixed into a noise background composed of multiple competing sound sources. This paper describes a recent speech recognition evaluation that was designed to bring together researchers from multiple communities in order to foster novel approaches to this problem. The task was to identify keywords from sentences reverberantly mixed into audio backgrounds binaurally recorded in a busy domestic environment. The challenge was designed to model the essential difficulties of the multisource environment problem while remaining on a scale that would make it accessible to a wide audience. Compared to previous ASR evaluations a particular novelty of the task is that the utterances to be recognised were provided in a continuous audio background rather than as pre-segmented utterances thus allowing a range of background modelling techniques to be employed. The challenge attracted thirteen submissions. This paper describes the challenge problem, provides an overview of the systems that were entered and provides a comparison alongside both a baseline recognition system and human performance. The paper discusses insights gained from the challenge and lessons learnt for the design of future such evaluations.","['Jon Barker', 'Emmanuel Vincent', 'Ning Ma', 'Heidi Christensen', 'Phil Green']",May 2013,Computer Speech & Language,"['Speech recognition', 'Source separation', 'Noise robustness']",The PASCAL CHiME speech separation and recognition challenge☆
470,"The use of microphone arrays offers enhancements of speech signals recorded in meeting rooms and office spaces. A common solution for speech enhancement in realistic environments with ambient noise and multi-path propagation is the application of so-called beamforming techniques. Such beamforming algorithms enhance signals at the desired angle using constructive interference while attenuating signals coming from other directions by destructive interference. However, these techniques require as a priori the time difference of arrival information of the source. Therefore, the source localization and tracking algorithms are an integral part of such a system. The conventional localization algorithms deteriorate in realistic scenarios with multiple concurrent speakers. In contrast to conventional methods, the techniques presented in this paper make use of pitch information of speech signals in addition to the location information. This “position–pitch”-based algorithm pre-processes the speech signals by a multiband gammatone filterbank that is inspired from the auditory model of the human inner ear. The role of this gammatone filterbank is analyzed and discussed in details. For a robust localization of multiple concurrent speakers, a frequency-selective criterion is explored that is based on a study of the human neural system's use of correlations between adjacent sub-band frequencies. This frequency-selective criterion leads to improved localization performance. To further improve localization accuracy, an algorithm based on grouping of spectro-temporal regions formed by pitch cues is presented. All proposed speaker localization algorithms are tested using a multichannel database where multiple concurrent speakers are active. The real-world recordings were made with a 24-channel uniform circular microphone array using loudspeakers and human speakers under various acoustic environments including moving concurrent speaker scenarios. The proposed techniques produced a localization performance that was significantly better than the state-of-the-art baseline in the scenarios tested.","['Tania Habib', 'Harald Romsdorfer']",May 2013,Computer Speech & Language,"['Speaker localization', 'Auditory scene analysis', 'Array signal processing', 'Direction-of-arrival estimation']",Auditory inspired methods for localization of multiple concurrent speakers☆
471,"This paper presents a general framework for tracking the time differences of arrivals of multiple acoustic sources recorded by distributed microphone pairs. Tracking is based on a three-stage analysis. Complex-valued propagation models are extracted at different time instants and frequencies using either the independent component analysis or the phase of the cross-power spectrum evaluated at each microphone pair. In both cases, approximated densities of the propagation time delays are derived through the generalized state coherence transform. A sequential Bayesian tracking scheme with an integrated activity detection is finally implemented through disjoint particle filters based on a track-before-detect strategy. Experiments on both synthetic and real data recorded by two distributed microphone pairs show that the proposed framework can detect and track up to five sources simultaneously active in a reverberant environment.","['Alessio Brutti', 'Francesco Nesta']",May 2013,Computer Speech & Language,"['Acoustic source localization', 'Independent component analysis', 'Particle filtering']",Tracking of multidimensional TDOA for multiple sources with distributed microphone pairs☆
472,"Separating speech signals of multiple simultaneous talkers in a reverberant enclosure is known as the cocktail party problem. In real-time applications online solutions capable of separating the signals as they are observed are required in contrast to separating the signals offline after observation. Often a talker may move, which should also be considered by the separation system. This work proposes an online method for speaker detection, speaker direction tracking, and speech separation. The separation is based on multiple acoustic source tracking (MAST) using Bayesian filtering and time–frequency masking. Measurements from three room environments with varying amounts of reverberation using two different designs of microphone arrays are used to evaluate the capability of the method to separate up to four simultaneously active speakers. Separation of moving talkers is also considered. Results are compared to two reference methods: ideal binary masking (IBM) and oracle tracking (O-T). Simulations are used to evaluate the effect of number of microphones and their spacing.",['P. Pertilä'],May 2013,Computer Speech & Language,"['Blind source separation', 'Acoustic source tracking', 'Particle filtering', 'Time–frequency masking', 'Microphone arrays']",Online blind speech separation using multiple acoustic speaker tracking and time–frequency masking☆
473,"This paper proposes and describes a complete system for Blind Source Extraction (BSE). The goal is to extract a target signal source in order to recognize spoken commands uttered in reverberant and noisy environments, and acquired by a microphone array. The architecture of the BSE system is based on multiple stages: (a) TDOA estimation, (b) mixing system identification for the target source, (c) on-line semi-blind source separation and (d) source extraction. All the stages are effectively combined, allowing the estimation of the target signal with limited distortion.While a generalization of the BSE framework is described, here the proposed system is evaluated on the data provided for the CHiME Pascal 2011 competition, i.e. binaural recordings made in a real-world domestic environment. The CHiME mixtures are processed with the BSE and the recovered target signal is fed to a recognizer, which uses noise robust features based on Gammatone Frequency Cepstral Coefficients. Moreover, acoustic model adaptation is applied to further reduce the mismatch between training and testing data and improve the overall performance. A detailed comparison between different models and algorithmic settings is reported, showing that the approach is promising and the resulting system gives a significant reduction of the error rate.","['Francesco Nesta', 'Marco Matassoni']",May 2013,Computer Speech & Language,[],Blind source extraction for robust speech recognition in multisource noisy environments☆
474,"In this contribution, a novel two-channel acoustic front-end for robust automatic speech recognition in adverse acoustic environments with nonstationary interference and reverberation is proposed. From a MISO system perspective, a statistically optimum source signal extraction scheme based on the multichannel Wiener filter (MWF) is discussed for application in noisy and underdetermined scenarios. For free-field and diffuse noise conditions, this optimum scheme reduces to a Delay & Sum beamformer followed by a single-channel Wiener postfilter. Scenarios with multiple simultaneously interfering sources and background noise are usually modeled by a diffuse noise field. However, in reality, the free-field assumption is very weak because of the reverberant nature of acoustic environments. Therefore, we propose to estimate this simplified MWF solution in each frequency bin separately to cope with reverberation. We show that this approach can very efficiently be realized by the combination of a blocking matrix based on semi-blind source separation (‘directional BSS’), which provides a continuously updated reference of all undesired noise and interference components separated from the desired source and its reflections, and a single-channel Wiener postfilter. Moreover, it is shown, how the obtained reference signal of all undesired components can efficiently be used to realize the Wiener postfilter, and at the same time, generalizes well-known postfilter realizations. The proposed front-end and its integration into an automatic speech recognition (ASR) system are analyzed and evaluated in noisy living-room-like environments according to the PASCAL CHiME challenge. A comparison to a simplified front-end based on a free-field assumption shows that the introduced system substantially improves the speech quality and the recognition performance under the considered adverse conditions.","['Klaus Reindl', 'Yuanhang Zheng', 'Andreas Schwarz', 'Stefan Meier', 'Roland Maas', 'Armin Sehr', 'Walter Kellermann']",May 2013,Computer Speech & Language,"['Blind source extraction', 'Speech enhancement', 'Robust automatic speech recognition', 'PASCAL CHiME challenge']",A stereophonic acoustic signal extraction scheme for noisy and reverberant environments☆
475,"We introduce a new regularized nonnegative matrix factorization (NMF) method for supervised single-channel source separation (SCSS). We propose a new multi-objective cost function which includes the conventional divergence term for the NMF together with a prior likelihood term. The first term measures the divergence between the observed data and the multiplication of basis and gains matrices. The novel second term encourages the log-normalized gain vectors of the NMF solution to increase their likelihood under a prior Gaussian mixture model (GMM) which is used to encourage the gains to follow certain patterns. In this model, the parameters to be estimated are the basis vectors, the gain vectors and the parameters of the GMM prior. We introduce two different ways to train the model parameters, sequential training and joint training. In sequential training, after finding the basis and gains matrices, the gains matrix is then used to train the prior GMM in a separate step. In joint training, within each NMF iteration the basis matrix, the gains matrix and the prior GMM parameters are updated jointly using the proposed regularized NMF. The normalization of the gains makes the prior models energy independent, which is an advantage as compared to earlier proposals. In addition, GMM is a much richer prior than the previously considered alternatives such as conjugate priors which may not represent the distribution of the gains in the best possible way. In the separation stage after observing the mixed signal, we use the proposed regularized cost function with a combined basis and the GMM priors for all sources that were learned from training data for each source. Only the gain vectors are estimated from the mixed data by minimizing the joint cost function. We introduce novel update rules that solve the optimization problem efficiently for the new regularized NMF problem. This optimization is challenging due to using energy normalization and GMM for prior modeling, which makes the problem highly nonlinear and non-convex. The experimental results show that the introduced methods improve the performance of single channel source separation for speech separation and speech–music separation with different NMF divergence functions. The experimental results also show that, using the GMM prior gives better separation results than using the conjugate prior.","['Emad M. Grais', 'Hakan Erdogan']",May 2013,Computer Speech & Language,"['Nonnegative matrix factorization', 'Single-channel source separation', 'Gaussian mixture models', 'Trained prior models']",Regularized nonnegative matrix factorization using Gaussian mixture priors for supervised single channel source separation☆
476,"Speech recognition systems intended for everyday use must be able to cope with a large variety of noise types and levels, including highly non-stationary multi-source mixtures. This study applies spectral factorisation algorithms and long temporal context for separating speech and noise from mixed signals. To adapt the system to varying environments, noise models are acquired from the context, or learnt from the mixture itself without prior information. We also propose methods for reducing the size of the bases used for speech and noise modelling by 20–40 times for better practical applicability. We evaluate the performance of the methods both as a standalone classifier and as a signal-enhancing front-end for external recognisers. For the CHiME noisy speech corpus containing non-stationary multi-source household noises at signal-to-noise ratios ranging from +9 to −6 dB, we report average keyword recognition rates up to 87.8% using a single-stream sparse classification algorithm.","['Antti Hurmalainen', 'Jort F. Gemmeke', 'Tuomas Virtanen']",May 2013,Computer Speech & Language,"['Automatic speech recognition', 'Noise robustness', 'Non-stationary noise', 'Non-negative spectral factorisation', 'Exemplar-based']",Modelling non-stationary noise with spectral factorisation in automatic speech recognition☆
477,"This article proposes and evaluates various methods to integrate the concept of bidirectional Long Short-Term Memory (BLSTM) temporal context modeling into a system for automatic speech recognition (ASR) in noisy and reverberated environments. Building on recent advances in Long Short-Term Memory architectures for ASR, we design a novel front-end for context-sensitive Tandem feature extraction and show how the Connectionist Temporal Classification approach can be used as a BLSTM-based back-end, alternatively to Hidden Markov Models (HMM). We combine context-sensitive BLSTM-based feature generation and speech decoding techniques with source separation by convolutive non-negative matrix factorization. Applying our speaker adapted multi-stream HMM framework that processes MFCC features from NMF-enhanced speech as well as word predictions obtained via BLSTM networks and non-negative sparse classification (NSC), we obtain an average accuracy of 91.86% on the PASCAL CHiME Challenge task at signal-to-noise ratios ranging from −6 to 9 dB. To our knowledge, this is the best result ever reported for the CHiME Challenge task.","['Martin Wöllmer', 'Felix Weninger', 'Jürgen Geiger', 'Björn Schuller', 'Gerhard Rigoll']",May 2013,Computer Speech & Language,"['Automatic speech recognition', 'Long Short-Term Memory', 'Non-negative matrix factorization', 'Tandem feature extraction']",Noise robust ASR in reverberated multisource environments applying convolutive NMF and Long Short-Term Memory☆
478,"We present an automatic speech recognition system that uses a missing data approach to compensate for challenging environmental noise containing both additive and convolutive components. The unreliable and noise-corrupted (“missing”) components are identified using a Gaussian mixture model (GMM) classifier based on a diverse range of acoustic features. To perform speech recognition using the partially observed data, the missing components are substituted with clean speech estimates computed using both sparse imputation and cluster-based GMM imputation. Compared to two reference mask estimation techniques based on interaural level and time difference-pairs, the proposed missing data approach significantly improved the keyword accuracy rates in all signal-to-noise ratio conditions when evaluated on the CHiME reverberant multisource environment corpus. Of the imputation methods, cluster-based imputation was found to outperform sparse imputation. The highest keyword accuracy was achieved when the system was trained on imputed data, which made it more robust to possible imputation errors.","['Sami Keronen', 'Heikki Kallasjoki', 'Ulpu Remes', 'Guy J. Brown', 'Jort F. Gemmeke', 'Kalle J. Palomäki']",May 2013,Computer Speech & Language,"['Noise robust', 'Speech recognition', 'Missing data', 'Binaural', 'Multicondition', 'Imputation']",Mask estimation and imputation methods for missing data speech recognition in a multisource reverberant environment☆
479,"This paper addresses the problem of speech recognition in reverberant multisource noise conditions using distant binaural microphones. Our scheme employs a two-stage fragment decoding approach inspired by Bregman's account of auditory scene analysis, in which innate primitive grouping ‘rules’ are balanced by the role of learnt schema-driven processes. First, the acoustic mixture is split into local time-frequency fragments of individual sound sources using signal-level primitive grouping cues. Second, statistical models are employed to select fragments belonging to the sound source of interest, and the hypothesis-driven stage simultaneously searches for the most probable speech/background segmentation and the corresponding acoustic model state sequence. The paper reports recent advances in combining adaptive noise floor modelling and binaural localisation cues within this framework. By integrating signal-level grouping cues with acoustic models of the target sound source in a probabilistic framework, the system is able to simultaneously separate and recognise the sound of interest from the mixture, and derive significant recognition performance benefits from different grouping cue estimates despite their inherent unreliability in noisy conditions. Finally, the paper will show that missing data imputation can be applied via fragment decoding to allow reconstruction of a clean spectrogram that can be further processed and used as input to conventional ASR systems. The best performing system achieves an average keyword recognition accuracy of 85.83% on the PASCAL CHiME Challenge task.","['Ning Ma', 'Jon Barker', 'Heidi Christensen', 'Phil Green']",May 2013,Computer Speech & Language,"['Distant-microphone speech recognition', 'Auditory scene analysis', 'Binaural localisation', 'Noise robustness']",A hearing-inspired approach for distant-microphone speech recognition in the presence of multiple sources☆
480,"This paper presents a new approach for increasing the robustness of multi-channel automatic speech recognition in noisy and reverberant multi-source environments. The proposed method uses uncertainty propagation techniques to dynamically compensate the speech features and the acoustic models for the observation uncertainty determined at the beamforming stage. We present and analyze two methods that allow integrating classical multi-channel signal processing approaches like delay and sum beamformers or Zelinski-type Wiener filters, with uncertainty-of-observation techniques like uncertainty decoding or modified imputation. An analysis of the results on the PASCAL-CHiME task shows that this approach consistently outperforms conventional beamformers with a minimal increase in computational complexity. The use of dynamic compensation based on observation uncertainty also outperforms conventional static adaptation with no need of adaptation data.","['Ramón Fernandez Astudillo', 'Dorothea Kolossa', 'Alberto Abad', 'Steffen Zeiler', 'Rahim Saeidi', 'Pejman Mowlaee', 'João Paulo da Silva Neto', 'Rainer Martin']",May 2013,Computer Speech & Language,"['Robust speech recognition', 'Beamforming', 'Uncertainty decoding', 'Uncertainty propagation']",Integration of beamforming and uncertainty-of-observation techniques for robust ASR in multi-source environments☆
481,"Research on noise robust speech recognition has mainly focused on dealing with relatively stationary noise that may differ from the noise conditions in most living environments. In this paper, we introduce a recognition system that can recognize speech in the presence of multiple rapidly time-varying noise sources as found in a typical family living room. To deal with such severe noise conditions, our recognition system exploits all available information about speech and noise; that is spatial (directional), spectral and temporal information. This is realized with a model-based speech enhancement pre-processor, which consists of two complementary elements, a multi-channel speech–noise separation method that exploits spatial and spectral information, followed by a single channel enhancement algorithm that uses the long-term temporal characteristics of speech obtained from clean speech examples. Moreover, to compensate for any mismatch that may remain between the enhanced speech and the acoustic model, our system employs an adaptation technique that combines conventional maximum likelihood linear regression with the dynamic adaptive compensation of the variance of the Gaussians of the acoustic model. Our proposed system approaches human performance levels by greatly improving the audible quality of speech and substantially improving the keyword recognition accuracy.","['Marc Delcroix', 'Keisuke Kinoshita', 'Tomohiro Nakatani', 'Shoko Araki', 'Atsunori Ogawa', 'Takaaki Hori', 'Shinji Watanabe', 'Masakiyo Fujimoto', 'Takuya Yoshioka', 'Takanobu Oba', 'Yotaro Kubo', 'Mehrez Souden', 'Seong-Jun Hahm', 'Atsushi Nakamura']",May 2013,Computer Speech & Language,"['Robust ASR', 'Model-based speech enhancement', 'Example-based speech enhancement', 'Model adaptation', 'Dynamic variance adaptation']","Speech recognition in living rooms: Integrated speech enhancement and recognition system based on spatial, spectral and temporal modeling of sounds☆"
482,"We consider the problem of acoustic modeling of noisy speech data, where the uncertainty over the data is given by a Gaussian distribution. While this uncertainty has been exploited at the decoding stage via uncertainty decoding, its usage at the training stage remains limited to static model adaptation. We introduce a new expectation maximization (EM) based technique, which we call uncertainty training, that allows us to train Gaussian mixture models (GMMs) or hidden Markov models (HMMs) directly from noisy data with dynamic uncertainty. We evaluate the potential of this technique for a GMM-based speaker recognition task on speech data corrupted by real-world domestic background noise, using a state-of-the-art signal enhancement technique and various uncertainty estimation techniques as a front-end. Compared to conventional training, the proposed training algorithm results in 3–4% absolute improvement in speaker recognition accuracy by training from either matched, unmatched or multi-condition noisy data. This algorithm is also applicable with minor modifications to maximum a posteriori (MAP) or maximum likelihood linear regression (MLLR) acoustic model adaptation from noisy data and to other data than audio.","['Alexey Ozerov', 'Mathieu Lagrange', 'Emmanuel Vincent']",May 2013,Computer Speech & Language,"['Noisy data', 'Training', 'Uncertainty', 'Classification', 'Acoustic model', 'Gaussian mixture model', 'Hidden Markov model', 'Expectation–maximization']",Uncertainty-based learning of acoustic models from noisy data☆☆☆
483,"The development of high-performance statistical machine translation (SMT) systems is contingent on the availability of substantial, in-domain parallel training corpora. The latter, however, are expensive to produce due to the labor-intensive nature of manual translation. We propose to alleviate this problem with a novel, semi-supervised, batch-mode active learning strategy that attempts to maximize in-domain coverage by selecting sentences, which represent a balance between domain match, translation difficulty, and batch diversity. Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outperforms the random selection baseline, but also traditional active selection techniques based on dissimilarity to existing training data.","['Sankaranarayanan Ananthakrishnan', 'Rohit Prasad', 'David Stallard', 'Prem Natarajan']",February 2013,Computer Speech & Language,"['Statistical machine translation', 'Active learning', 'Semi-supervised learning', 'Resource-poor language pairs']",Batch-mode semi-supervised active learning for statistical machine translation☆
484,"This paper investigates a noise robust technique for automatic speech recognition which exploits hidden Markov modeling of stereo speech features from clean and noisy channels. The HMM trained this way, referred to as stereo HMM, has in each state a Gaussian mixture model (GMM) with a joint distribution of both clean and noisy speech features. Given the noisy speech input, the stereo HMM gives rise to a two-pass compensation and decoding process where MMSE denoising based on N-best hypotheses is first performed and followed by decoding the denoised speech in a reduced search space on lattice. Compared to the feature space GMM-based denoising approaches, the stereo HMM is advantageous as it has finer-grained noise compensation and makes use of information of the whole noisy feature sequence for the prediction of each individual clean feature. Experiments on large vocabulary spontaneous speech from speech-to-speech translation applications show that the proposed technique yields superior performance than its feature space counterpart in noisy conditions while still maintaining decent performance in clean conditions.","['Xiaodong Cui', 'Mohamed Afify', 'Yuqing Gao', 'Bowen Zhou']",February 2013,Computer Speech & Language,"['Automatic speech recognition', 'Noise robustness', 'Stereo data', 'Stereo HMM', 'Stochastic mapping']",Stereo hidden Markov modeling for noise robust speech recognition☆
485,In this paper we present results of unsupervised cross-lingual speaker adaptation applied to text-to-speech synthesis. The application of our research is the personalisation of speech-to-speech translation in which we employ a HMM statistical framework for both speech recognition and synthesis. This framework provides a logical mechanism to adapt synthesised speech output to the voice of the user by way of speech recognition. In this work we present results of several different unsupervised and cross-lingual adaptation approaches as well as an end-to-end speaker adaptive speech-to-speech translation system. Our experiments show that we can successfully apply speaker adaptation in both unsupervised and cross-lingual scenarios and our proposed algorithms seem to generalise well for several language pairs. We also discuss important future directions including the need for better evaluation metrics.,"['John Dines', 'Hui Liang', 'Lakshmi Saheer', 'Matthew Gibson', 'William Byrne', 'Keiichiro Oura', 'Keiichi Tokuda', 'Junichi Yamagishi', 'Simon King', 'Mirjam Wester', 'Teemu Hirsimäki', 'Reima Karhila', 'Mikko Kurimo']",February 2013,Computer Speech & Language,"['Speech-to-speech translation', 'Cross-lingual speaker adaptation', 'HMM-based speech synthesis', 'Speaker adaptation', 'Voice conversion']",Personalising speech-to-speech translation: Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis☆
486,"Concept classification has been used as a translation method and has shown notable benefits within the suite of speech-to-speech translation applications. However, the main bottleneck in achieving an acceptable performance with such classifiers is the cumbersome task of annotating large amounts of training data. Any attempt to develop a method to assist in, or to completely automate, data annotation needs a distance measure to compare sentences based on the concept they convey. Here, we introduce a new method of sentence comparison that is motivated from the translation point of view. In this method the imperfect translations produced by a phrase-based statistical machine translation system are used to compare the concepts of the source sentences. Three clustering methods are adapted to support the concept-base distance. These methods are applied to prepare groups of paraphrases and use them as training sets in concept classification tasks. The statistical machine translation is also used to enhance the training data for the classifier which is crucial when such data are sparse. Experiments show the effectiveness of the proposed methods.","['Emil Ettelaie', 'Panayiotis G. Georgiou', 'Shrikanth S. Narayanan']",February 2013,Computer Speech & Language,"['Speech to speech translation', 'Spoken language understanding', 'Concept classification']",Unsupervised data processing for classifier-based speech translator☆
487,"Globalization as well as international crises and disasters spur the need for cross-lingual verbal communication for myriad languages. This is reflected in ongoing intense research activity in the field of speech translation. However, the development of deployable speech translation systems still happens only for a handful of languages. Prohibitively high costs attached to the acquisition of sufficient amounts of suitable speech translation training data are one of the main reasons for this situation. A new language pair or domain is typically only considered for speech translation development after a major need for cross-lingual verbal communication just arose—justifying the high development costs. In such situations, communication has to rely on the help of interpreters, while massive data collections for system development are conducted in parallel. We propose an alternative to this time-consuming and costly parallel effort. By training speech translation directly on audio recordings of interpreter-mediated communication, we omit most of the manual transcription effort and all of the manual translation effort that characterizes traditional speech translation development.","['Matthias Paulik', 'Alex Waibel']",February 2013,Computer Speech & Language,"['Speech translation', 'Spoken language translation', 'Parallel speech']",Training speech translation from audio recordings of interpreter-mediated communication☆
488,"In this paper we present a speech-to-speech (S2S) translation system called the BBN TransTalk that enables two-way communication between speakers of English and speakers who do not understand or speak English. The BBN TransTalk has been configured for several languages including Iraqi Arabic, Pashto, Dari, Farsi, Malay, Indonesian, and Levantine Arabic. We describe the key components of our system: automatic speech recognition (ASR), machine translation (MT), text-to-speech (TTS), dialog manager, and the user interface (UI). In addition, we present novel techniques for overcoming specific challenges in developing high-performing S2S systems. For ASR, we present techniques for dealing with lack of pronunciation and linguistic resources and effective modeling of ambiguity in pronunciations of words in these languages. For MT, we describe techniques for dealing with data sparsity as well as modeling context. We also present and compare different user confirmation techniques for detecting errors that can cause the dialog to drift or stall.","['Rohit Prasad', 'Prem Natarajan', 'David Stallard', 'Shirin Saleem', 'Shankar Ananthakrishnan', 'Stavros Tsakalidis', 'Chia-lin Kao', 'Fred Choi', 'Ralf Meermeier', 'Mark Rawls', 'Jacob Devlin', 'Kriste Krstovski', 'Aaron Challenner']",February 2013,Computer Speech & Language,"['Speech-to-speech translation', 'Automatic speech recognition', 'Statistical machine translation', 'Dialog management']",BBN TransTalk: Robust multilingual two-way speech-to-speech translation for mobile platforms☆
489,"Conventional approaches to speech-to-speech (S2S) translation typically ignore key contextual information such as prosody, emphasis, discourse state in the translation process. Capturing and exploiting such contextual information is especially important in machine-mediated S2S translation as it can serve as a complementary knowledge source that can potentially aid the end users in improved understanding and disambiguation. In this work, we present a general framework for integrating rich contextual information in S2S translation. We present novel methodologies for integrating source side context in the form of dialog act (DA) tags, and target side context using prosodic word prominence. We demonstrate the integration of the DA tags in two different statistical translation frameworks, phrase-based translation and a bag-of-words lexical choice model. In addition to producing interpretable DA annotated target language translations, we also obtain significant improvements in terms of automatic evaluation metrics such as lexical selection accuracy and BLEU score. Our experiments also indicate that finer representation of dialog information such as yes–no questions, wh-questions and open questions are the most useful in improving translation quality. For target side enrichment, we employ factored translation models to integrate the assignment and transfer of prosodic word prominence (pitch accents) during translation. The factored translation models provide significant improvement in assignment of correct pitch accents to the target words in comparison with a post-processing approach. Our framework is suitable for integrating any word or utterance level contextual information that can be reliably detected (recognized) from speech and/or text.","['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangalore', 'Shrikanth Narayanan']",February 2013,Computer Speech & Language,"['Speech-to-speech translation', 'Rich context', 'Dialog acts', 'Prosody']",Enriching machine-mediated speech-to-speech translation using contextual information☆
490,"This paper outlines the first Asian network-based speech-to-speech translation system developed by the Asian Speech Translation Advanced Research (A-STAR) consortium. Eight research groups comprising the A-STAR members participated in the experiments, covering nine languages, i.e., eight Asian languages (Hindi, Indonesian, Japanese, Korean, Malay, Thai, Vietnamese, and Chinese) and English. Each A-STAR member contributed one or more of the following spoken language technologies: automatic speech recognition, machine translation, and text-to-speech through Web servers. The system was designed to translate common spoken utterances of travel conversations from a given source language into multiple target languages in order to facilitate multiparty travel conversations between people speaking different Asian languages. It covers travel expressions including proper nouns that are names of famous places or attractions in Asian countries. In this paper, we describe the issues of developing spoken language technologies for Asian languages, and discuss the difficulties involved in connecting different heterogeneous spoken language translation systems through Web servers. This paper also presents speech-translation results including subjective evaluation, from the first A-STAR field testing which was carried out in July 2009.","['Sakriani Sakti', 'Michael Paul', 'Andrew Finch', 'Shinsuke Sakai', 'Thang Tat Vu', 'Noriyuki Kimura', 'Chiori Hori', 'Eiichiro Sumita', 'Satoshi Nakamura', 'Jun Park', 'Chai Wutiwiwatchai', 'Bo Xu', 'Hammam Riza', 'Karunesh Arora', 'Chi Mai Luong', 'Haizhou Li']",February 2013,Computer Speech & Language,"['Speech-to-speech translation', 'Speech recognition', 'Machine translation', 'Text-to-speech', 'Asian languages']",A-STAR: Toward translating Asian spoken languages☆
491,"One of the most difficult challenges that military personnel face when operating in foreign countries is clear and successful communication with the local population. To address this issue, the Defense Advanced Research Projects Agency (DARPA) is funding academic institutions and industrial organizations through the Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program to develop practical machine translation systems. The goal of the TRANSTAC program is to demonstrate capabilities to rapidly develop and field free-form, two-way, speech-to-speech translation systems that enable speakers of different languages to communicate with one another in real-world tactical situations without an interpreter. Evaluations of these technologies are a significant part of the program and DARPA has asked the National Institute of Standards and Technology (NIST) to lead this effort. This article presents the experimental design of the TRANSTAC evaluations and the metrics, both quantitative and qualitative, that were used to comprehensively assess the systems’ performance.","['Gregory A. Sanders', 'Brian A. Weiss', 'Craig Schlenoff', 'Michelle P. Steves', 'Sherri Condon']",February 2013,Computer Speech & Language,"['Evaluation', 'Machine translation', 'NIST', 'Performance metrics', 'Bidirectional speech-to-speech translation', 'TRANSTAC']","Evaluation methodology and metrics employed to assess the TRANSTAC two-way, speech-to-speech translation systems☆"
492,"The study provides an empirical analysis of long-term user behavioral changes and varying user strategies during cross-lingual interaction using the multimodal speech-to-speech (S2S) translation system of USC/SAIL. The goal is to inform user adaptive designs of such systems. A 4-week medical-scenario-based study provides the basis for our analysis. The data analyzed includes user interviews, post-session surveys, and the extensive system logs that were post-processed and annotated. The annotations measured the meaning transfer rates using human evaluations and a scale defined here called the concept matching score.First, qualitative data analysis investigates user strategies in dealing with errors, such as repeat, rephrase, change topic, start over, and the participants’ self-reported longitudinal adaptation to errors. Post-session surveys explore participant experience with the system and point to a trend of user-perceived increased performance over time.The log data analysis provides further insightful results. Users chose to allow some degradation (84% of original concepts) of their intended meaning to proceed through the system, even after they observed potential errors in the visual output from the speech recognizer. The rejected utterances, on average, had only 25% of the original concepts. This user-filtered outcome, after the complete channel transfer through the S2S system, is that 91% of the successful turns result in transfer of at least half the intended concepts while 90% of the user rejected turns would have conveyed less than half the intended meaning.The multimodal interface results in 24% relative improvement in the confirmation mode and in 31% relative improvement in the choice mode compared to the speech-only modality. Analysis also showed that users of the multimodal interface temporally change their strategies by accepting more system-produced choices. This user behavior can expedite communication seeking an operating balance between user strategies and system performance factors. Lastly, user utterance length is analyzed. Longer utterances in general imply more information delivered per utterance but potentially at the cost of increased processing degradation. The analysis demonstrates that users reduce their utterance length after unsuccessful turns and increase it after successful turns and that there is a learning effect that increases this behavior over the duration of the study.","['JongHo Shin', 'Panayiotis G. Georgiou', 'Shrikanth Narayanan']",February 2013,Computer Speech & Language,"['Speech-to-speech', 'S2S', 'Speech translation', 'Longitudinal studies', 'User interfaces', 'HCI', 'User behaviors']",Enabling effective design of multimodal interfaces for speech-to-speech translation system: An empirical study of longitudinal user behaviors over time and user strategies for coping with errors☆
493,"In this paper, we investigate the task of translating spontaneous speech transcriptions by employing aligned movie subtitles in training a statistical machine translator (SMT). In contrast to the lexical-based dynamic time warping (DTW) approaches to bilingual subtitle alignment, we align subtitle documents using time-stamps. We show that subtitle time-stamps in two languages are often approximately linearly related, which can be exploited for extracting high-quality bilingual subtitle pairs. On a small tagged data-set, we achieve a performance improvement of 0.21 F-score points compared to traditional DTW alignment approach and 0.39 F-score points compared to a simple line-fitting approach. In addition, we achieve a performance gain of 4.88 BLEU score points in spontaneous speech translation experiments using the aligned subtitle data obtained by the proposed alignment approach compared to that obtained by the DTW based alignment approach demonstrating the merit of the time-stamps based subtitle alignment scheme.","['Andreas Tsiartas', 'Prasanta Ghosh', 'Panayiotis Georgiou', 'Shrikanth Narayanan']",February 2013,Computer Speech & Language,"['Movie subtitle alignment', 'Spontaneous speech translation']",High-quality bilingual subtitle document alignments with application to spontaneous speech translation☆
494,"This paper describes our recent improvements to IBM TRANSTAC speech-to-speech translation systems that address various issues arising from dealing with resource-constrained tasks, which include both limited amounts of linguistic resources and training data, as well as limited computational power on mobile platforms such as smartphones. We show how the proposed algorithms and methodologies can improve the performance of automatic speech recognition, statistical machine translation, and text-to-speech synthesis, while achieving low-latency two-way speech-to-speech translation on mobiles.","['Bowen Zhou', 'Xiaodong Cui', 'Songfang Huang', 'Martin Cmejrek', 'Wei Zhang', 'Jian Xue', 'Jia Cui', 'Bing Xiang', 'Gregg Daggett', 'Upendra Chaudhari', 'Sameer Maskey', 'Etienne Marcheret']",February 2013,Computer Speech & Language,"['Speech-to-speech translation', 'Speech recognition', 'Machine translation', 'Text-to-speech', 'Low-resourced languages', 'Mobiles']",The IBM speech-to-speech translation system for smartphone: Improvements for resource-constrained tasks☆
495,,"['Björn Schuller', 'Stefan Steidl', 'Anton Batliner']",January 2013,Computer Speech & Language,"['Computational paralinguistics', 'Age', 'Gender', 'Affect', 'Paralinguistic Challenge']",EditorialIntroduction to the special issue on Paralinguistics in Naturalistic Speech and Language
496,"Paralinguistic analysis is increasingly turning into a mainstream topic in speech and language processing. This article aims to provide a broad overview of the constantly growing field by defining the field, introducing typical applications, presenting exemplary resources, and sharing a unified view of the chain of processing. It then presents the first broader Paralinguistic Challenge organised at INTERSPEECH 2010 by the authors including a historical overview of the Challenge tasks of recognising age, gender, and affect, a summary of methods used by the participants, and their results. In addition, we present the new benchmark obtained by fusion of participants’ predictions and conclude by discussing ten recent and emerging trends in the analysis of paralinguistics in speech and language.","['Björn Schuller', 'Stefan Steidl', 'Anton Batliner', 'Felix Burkhardt', 'Laurence Devillers', 'Christian Müller', 'Shrikanth Narayanan']",January 2013,Computer Speech & Language,"['Paralinguistics', 'Age', 'Gender', 'Affect', 'Survey', 'Trends', 'Challenge']",Paralinguistics in speech and language—State-of-the-art and the challenge☆
497,"The search for vocal markers of emotion has been hampered by the difficulty of obtaining access to speech samples that represent authentic expressions of the speaker's felt emotions. The recent trend to privilege real-life, naturalistic speech tokens, often obtained by convenience sampling, encounters two major problems: (1) the assumption that speech recorded in the field or from the media is a direct, uncontrolled expression of the speaker's “true” feeling state is unrealistic, given the widespread use of expression control due to display rules and strategic concerns; (2) the use of convenience samples, often of rare events, can engender the neglect of minimal requirements for experimental control of important determinants. Conversely, the use of performance induction of affect allows systematic control of influence factors and shows that even subtle variations of task characteristics and appraisal can produce major paralinguistic differences. The disadvantage of this type of elicitation is that the emotional effects are often weak and vary greatly over individuals. In this paper, the results of a study comparing the effects of an established psychological mood induction technique (the Velten procedure) with a classic acting/portrayal approach on a set of major acoustic parameters are reported. The elicitation of positive/happy and negative/sad utterance through both tasks yields essentially the same differences in both tasks for energy, F0, spectral, and temporal parameters. In comparison, task differences have much less effect. As three different language groups were used, the important effects of the speaker's language, especially in interacting with task and emotion factors, can be demonstrated. It is suggested that enacting studies using professional mental imagery techniques are an important part of the available experimental paradigms, as they allow extensive experimental control and as the results seem to be comparable with other induction techniques. They are especially useful if the purpose of the research is to study the listener attribution of emotion from vocal cues, rather than the diagnosis of symptoms of “true” underlying emotions (which remains elusive and presents a major challenge for future research).",['Klaus R. Scherer'],January 2013,Computer Speech & Language,"['Vocal correlates of emotion', 'Emotion induction', 'Expression control', 'Acoustic parameters']",Vocal markers of emotion: Comparing induction and acting elicitation☆
498,"The paralinguistic information in a speech signal includes clues to the geographical and social background of the speaker. This paper is concerned with automatic extraction of this information from a short segment of speech. A state-of-the-art language identification (LID) system is applied to the problems of regional accent recognition for British English, and ethnic group recognition within a particular accent. We compare the results with human performance and, for accent recognition, the ‘text dependent’ ACCDIST accent recognition measure. For the 14 regional accents of British English in the ABI-1 corpus (good quality read speech), our LID system achieves a recognition accuracy of 89.6%, compared with 95.18% for our best ACCDIST-based system and 58.24% for human listeners. The “Voices across Birmingham” corpus contains significant amounts of telephone conversational speech for the two largest ethnic groups in the city of Birmingham (UK), namely the ‘Asian’ and ‘White’ communities. Our LID system distinguishes between these two groups with an accuracy of 96.51% compared with 90.24% for human listeners. Although direct comparison is difficult, it seems that our LID system performs much better on the standard 12 class NIST 2003 Language Recognition Evaluation task or the two class ethnic group recognition task than on the 14 class regional accent recognition task. We conclude that automatic accent recognition is a challenging task for speech technology, and speculate that the use of natural conversational speech may be advantageous for these types of paralinguistic task.","['A. Hanani', 'M.J. Russell', 'M.J. Carey']",January 2013,Computer Speech & Language,"['Accent identification', 'British English', 'Language identification', 'Gaussian Mixture Model', 'Support Vector Machine']",Human and computer recognition of regional accents and ethnic groups from British English speech☆
499,"In the last few years, the number of systems and devices that use voice based interaction has grown significantly. For a continued use of these systems, the interface must be reliable and pleasant in order to provide an optimal user experience. However there are currently very few studies that try to evaluate how pleasant is a voice from a perceptual point of view when the final application is a speech based interface. In this paper we present an objective definition for voice pleasantness based on the composition of a representative feature subset and a new automatic voice pleasantness classification and intensity estimation system. Our study is based on a database composed by European Portuguese female voices but the methodology can be extended to male voices or to other languages. In the objective performance evaluation the system achieved a 9.1% error rate for voice pleasantness classification and a 15.7% error rate for voice pleasantness intensity estimation.","['Luis Pinto-Coelho', 'Daniela Braga', 'Miguel Sales-Dias', 'Carmen Garcia-Mateo']",January 2013,Computer Speech & Language,"['Voice pleasantness', 'Subtle emotions', 'Perceptual speech analysis', 'Text-to-Speech synthesis']",On the development of an automatic voice pleasantness classification and intensity estimation system☆
500,"Automatically detecting human social intentions and attitudes from spoken conversation is an important task for speech processing and social computing. We describe a system for detecting interpersonal stance: whether a speaker is flirtatious, friendly, awkward, or assertive. We make use of a new spoken corpus of over 1000 4-min speed-dates. Participants rated themselves and their interlocutors for these interpersonal stances, allowing us to build detectors for style both as interpreted by the speaker and as perceived by the hearer. We use lexical, prosodic, and dialog features in an SVM classifier to detect very clear styles (the strongest 10% in each stance) with up to 75% accuracy on previously seen speakers (50% baseline) and up to 59% accuracy on new speakers (48% baseline). A feature analysis suggests that flirtation is marked by joint focus on the woman as a target of the conversation, awkwardness by decreased speaker involvement, and friendliness by a conversational style including other-directed laughter and appreciations. Our work has implications for our understanding of interpersonal stance, their linguistic expression, and their automatic extraction.","['Rajesh Ranganath', 'Dan Jurafsky', 'Daniel A. McFarland']",January 2013,Computer Speech & Language,"['Paralinguistics', 'Prosody', 'Emotion', 'Dating']","Detecting friendly, flirtatious, awkward, and assertive speech in speed-dates☆"
501,"We present the results of a study investigating the use of speech and language characteristics extracted from spontaneous spoken discourse to assess changes in cognitive function. Specifically, we investigated the use of automatic speech recognition technology to characterize spontaneous speech disfluency induced by topiramate, an anti-epileptic medication with language-related side-effects. We audio recorded spontaneous speech samples from 20 participants during several picture description tasks and analyzed the recordings automatically and manually to extract a range of spoken fluency measurements including speech discontinuities (e.g., filled pauses, false starts, and repetitions), silent pause duration, speaking rate and vowel lengthening. Our results indicate that some of these paralinguistic speech characteristics are (a) sensitive to the effects of topiramate, (b) are associated with topiramate concentrations in the blood, and (c) complement standard neuropsychological tests typically used to investigate cognitive effects of medications. This work demonstrates the use of computational linguistic tools to assess cognitive effects in a more sensitive, objective, and reproducible manner than is currently available with standard tests.","['Serguei V.S. Pakhomov', 'Susan E. Marino', 'Angela K. Birnbaum']",January 2013,Computer Speech & Language,"['Speech recognition', 'Cognitive assessment', 'Speech disfluency', 'Topiramate']",Quantification of speech disfluency as a marker of medication-induced cognitive impairment: An application of computerized speech analysis in neuropharmacology☆
502,"In this article we present an efficient approach to modeling the acoustic features for the tasks of recognizing various paralinguistic phenomena. Instead of the standard scheme of adapting the Universal Background Model (UBM), represented by the Gaussian Mixture Model (GMM), normally used to model the frame-level acoustic features, we propose to represent the UBM by building a monophone-based Hidden Markov Model (HMM). We present two approaches: transforming the monophone-based segmented HMM–UBM to a GMM–UBM and proceeding with the standard adaptation scheme, or to perform the adaptation directly on the HMM–UBM. Both approaches give superior results than the standard adaptation scheme (GMM–UBM) in both the emotion recognition task and the alcohol detection task. Furthermore, with the proposed method we were able to achieve better results than the current state-of-the-art systems in both tasks.","['R. Gajšek', 'F. Mihelič', 'S. Dobrišek']",January 2013,Computer Speech & Language,"['Emotion recognition', 'Intoxication recognition', 'Hidden Markov Models', 'Universal Background Model', 'Model adaptation']",Speaker state recognition using an HMM-based feature extraction method☆
503,"The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.","['Ming Li', 'Kyu J. Han', 'Shrikanth Narayanan']",January 2013,Computer Speech & Language,"['Age recognition', 'Gender recognition', 'Prosodic features', 'Pitch', 'Harmonic structure', 'Formant', 'Polynomial expansion', 'Maximum likelihood linear regression', 'UBM weight posterior probability supervectors', 'GMM', 'SVM', 'Sparse representation', 'Score level fusion']",Automatic speaker age and gender recognition using acoustic and prosodic level information fusion☆☆☆
504,"Traditional studies of speaker state focus primarily upon one-stage classification techniques using standard acoustic features. In this article, we investigate multiple novel features and approaches to two recent tasks in speaker state detection: level-of-interest (LOI) detection and intoxication detection. In the task of LOI prediction, we propose a novel Discriminative TFIDF feature to capture important lexical information and a novel Prosodic Event detection approach using AuToBI; we combine these with acoustic features for this task using a new multilevel multistream prediction feedback and similarity-based hierarchical fusion learning approach. Our experimental results outperform published results of all systems in the 2010 Interspeech Paralinguistic Challenge – Affect Subchallenge. In the intoxication detection task, we evaluate the performance of Prosodic Event-based, phone duration-based, phonotactic, and phonetic-spectral based approaches, finding that a combination of the phonotactic and phonetic-spectral approaches achieve significant improvement over the 2011 Interspeech Speaker State Challenge – Intoxication Subchallenge baseline. We discuss our results using these new features and approaches and their implications for future research.","['William Yang Wang', 'Fadi Biadsy', 'Andrew Rosenberg', 'Julia Hirschberg']",January 2013,Computer Speech & Language,"['Emotional speech', 'Paralinguistic', 'Speaker state']","Automatic detection of speaker state: Lexical, prosodic, and phonetic approaches to level-of-interest and intoxication classification"
505,"Automatic pitch stylization is an important resource for researchers working both on prosody and speech technologies. In order to be useful, the stylized F0 curve should contain the fewest possible number of control points while remaining, at the same time, close to the original curve from a perceptual point of view. Here, a pitch stylization algorithm aimed at finding the optimal balance between the number of employed control points and perceptual equality with respect to the original curve is presented. Rather than being defined by means of statistical closeness to the original F0 curve, the quality of the stylized curve is defined on the basis of a dynamic tonal perception model. The number of control points is optimized on the basis of previous results showing that the stylization can be more radical in those areas of the signal where tone perception is less accurate, i.e. in non-prominent areas. Perceptual tests show that, concerning the perceptual equality of the stylization, this approach performs as well as other reference ones, with the advantage of using a significantly lower number of control points. Although it is based on a theoretical background employing phonological units like syllables, the proposed, phonetic, approach does not require any preliminary segmentation or annotation step. It combines, instead, acoustic parameters related to syllabification and prominence detection into a single model which has been designed to be both integrated, in the sense that it does not introduce any pitfalls in the process, and dynamic, in the sense that it does not include rigid tonal perception thresholds.","['A. Origlia', 'G. Abete', 'F. Cutugno']",January 2013,Computer Speech & Language,"['Pitch stylization', 'Tonal perception', 'Pitch and energy interaction']",A dynamic tonal perception model for optimal pitch stylization☆
506,"We propose a novel universal acoustic characterization approach to spoken language recognition (LRE). The key idea is to describe any spoken language with a common set of fundamental units that can be defined “universally” across all spoken languages. In this study, speech attributes, such as manner and place of articulation, are chosen to form this unit inventory and used to build a set of language-universal attribute models with data-driven modeling techniques. The vector space modeling approach to LRE is adopted, where a spoken utterance is first decoded into a sequence of attributes independently of its language. Then, a feature vector is generated by using co-occurrence statistics of manner or place units, and the final LRE decision is implemented with a vector space language classifier. Several architectural configurations will be studied, and it will be shown that best performance is attained using a maximal figure-of-merit language classifier. Experimental evidence not only demonstrates the feasibility of the proposed techniques, but it also shows that the proposed technique attains comparable performance to standard approaches on the LRE tasks investigated in this work when the same experimental conditions are adopted.","['Sabato Marco Siniscalchi', 'Jeremy Reed', 'Torbjørn Svendsen', 'Chin-Hui Lee']",January 2013,Computer Speech & Language,"['Spoken language recognition', 'Vector space model', 'Latentsemantic analysis', 'Artificial neural network', 'Support vectormachine', 'Phonetic features']",Universal attribute characterization of spoken languages for automatic spoken language recognition☆
507,"The presented work covers the issue of applying neural networks to the recognition and categorization of non-fluent and fluent utterance records. Speech samples containing three types of stuttering episodes (blocks before words starting with plosives, syllable repetitions, and sound-initial prolongations) were applied. The proposed system, built with hierarchical neural network framework, was used and then evaluated with respect to its ability to recognize and classify disfluency types in stuttered speech. The purpose of the first network was to reduce the dimension of vector describing the input signals. The result of the analysis was the output matrix consisting of neurons winning in a particular time frame. This matrix was taken as an input for the next network. Various types of MLP networks were examined with respect to their ability to classify utterances correctly into two, non-fluent and fluent, groups. Good examination results were accomplished and classification correctness exceeded 84–100% depending on the disfluency type.","['Izabela Świetlicka', 'Wiesława Kuniszyk-Jóźkowiak', 'Elżbieta Smołka']",January 2013,Computer Speech & Language,"['Artificial neural network', 'Kohonen network', 'Multilayer Perceptron', 'Classification', 'Stuttering']",Hierarchical ANN system for stuttering identification☆
508,"This paper presents a model of incremental speech generation in practical conversational systems. The model allows a conversational system to incrementally interpret spoken input, while simultaneously planning, realising and self-monitoring the system response. If these processes are time consuming and result in a response delay, the system can automatically produce hesitations to retain the floor. While speaking, the system utilises hidden and overt self-corrections to accommodate revisions in the system. The model has been implemented in a general dialogue system framework. Using this framework, we have implemented a conversational game application. A Wizard-of-Oz experiment is presented, where the automatic speech recognizer is replaced by a Wizard who transcribes the spoken input. In this setting, the incremental model allows the system to start speaking while the user's utterance is being transcribed. In comparison to a non-incremental version of the same system, the incremental version has a shorter response time and is perceived as more efficient by the users.","['Gabriel Skantze', 'Anna Hjalmarsson']",January 2013,Computer Speech & Language,"['Conversational systems', 'Incremental processing', 'Speech generation', 'Wizard-of-Oz']",Towards incremental speech generation in conversational systems☆
509,"The dynamic use of voice qualities in spoken language can reveal useful information on a speakers attitude, mood and affective states. This information may be very desirable for a range of, both input and output, speech technology applications. However, voice quality annotation of speech signals may frequently produce far from consistent labeling. Groups of annotators may disagree on the perceived voice quality, but whom should one trust or is the truth somewhere in between? The current study looks first to describe a voice quality feature set that is suitable for differentiating voice qualities on a tense to breathy dimension. Further, the study looks to include these features as inputs to a fuzzy-input fuzzy-output support vector machine (F2SVM) algorithm, which is in turn capable of softly categorizing voice quality recordings. The F2SVM is compared in a thorough analysis to standard crisp approaches and shows promising results, while outperforming for example standard support vector machines with the sole difference being that the F2SVM approach receives fuzzy label information during training. Overall, it is possible to achieve accuracies of around 90% for both speaker dependent (cross validation) and speaker independent (leave one speaker out validation) experiments. Additionally, the approach using F2SVM performs at an accuracy of 82% for a cross corpus experiment (i.e. training and testing on entirely different recording conditions) in a frame-wise analysis and of around 97% after temporally integrating over full sentences. Furthermore, the output of fuzzy measures gave performances close to that of human annotators.","['Stefan Scherer', 'John Kane', 'Christer Gobl', 'Friedhelm Schwenker']",January 2013,Computer Speech & Language,"['Voice quality', 'Fuzzy-input fuzzy-output support vector machines', 'Fuzzy classification', 'LF-model', 'Cross corpus analysis']",Investigating fuzzy-input fuzzy-output support vector machines for robust voice quality classification☆
510,"This study focuses on automatic visual speech recognition in the presence of noise. The authors show that, when speech is produced in noisy environments, articulatory changes occur because of the Lombard effect; these changes are both audible and visible. The authors analyze the visual Lombard effect and its role in automatic visual- and audiovisual speech recognition. Experimental results using both English and Japanese data demonstrate the negative effect of the Lombard effect in the visual speech domain. Without considering this factor in designing a lip-reading system, the performance of the system decreases. This is very important in audiovisual speech automatic recognition in real noisy environments. In such a case, however, the recognition rates decrease because of the presence of acoustic noise and because of the Lombard effect. The authors also show that the performance of an audiovisual speech recognizer depends also on the visual Lombard effect and can be further improved when it is considered in designing such a system.","['Panikos Heracleous', 'Carlos T. Ishi', 'Miki Sato', 'Hiroshi Ishiguro', 'Norihiro Hagita']",January 2013,Computer Speech & Language,"['Lip-reading', 'Automatic speech recognition', 'Hidden Markov models (HMMs)', 'Fusion', 'Noise robustness']",Analysis of the visual Lombard effect and automatic recognition experiments☆
511,"Language models (LMs) are often constructed by building multiple individual component models that are combined using context independent interpolation weights. By tuning these weights, using either perplexity or discriminative approaches, it is possible to adapt LMs to a particular task. This paper investigates the use of context dependent weighting in both interpolation and test-time adaptation of language models. Depending on the previous word contexts, a discrete history weighting function is used to adjust the contribution from each component model. As this dramatically increases the number of parameters to estimate, robust weight estimation schemes are required. Several approaches are described in this paper. The first approach is based on MAP estimation where interpolation weights of lower order contexts are used as smoothing priors. The second approach uses training data to ensure robust estimation of LM interpolation weights. This can also serve as a smoothing prior for MAP adaptation. A normalized perplexity metric is proposed to handle the bias of the standard perplexity criterion to corpus size. A range of schemes to combine weight information obtained from training data and test data hypotheses are also proposed to improve robustness during context dependent LM adaptation. In addition, a minimum Bayes’ risk (MBR) based discriminative training scheme is also proposed. An efficient weighted finite state transducer (WFST) decoding algorithm for context dependent interpolation is also presented. The proposed technique was evaluated using a state-of-the-art Mandarin Chinese broadcast speech transcription task. Character error rate (CER) reductions up to 7.3% relative were obtained as well as consistent perplexity improvements.","['X. Liu', 'M.J.F. Gales', 'P.C. Woodland']",January 2013,Computer Speech & Language,[],Use of contexts in language model interpolation and adaptation☆
512,"One way of making speech recognisers more robust to noise is model compensation. Rather than enhancing the incoming observations, model compensation techniques modify a recogniser's state-conditional distributions so they model the speech in the target environment. Because the interaction between speech and noise is non-linear, even for Gaussian speech and noise the corrupted speech distribution has no closed form. Thus, model compensation methods approximate it with a parametric distribution, such as a Gaussian or a mixture of Gaussians. The impact of this approximation has never been quantified. This paper therefore introduces a non-parametric method to compute the likelihood of a corrupted speech observation. It uses sampling and, given speech and noise distributions and a mismatch function, is exact in the limit. It therefore gives a theoretical bound for model compensation. Though computing the likelihood is computationally expensive, the novel method enables a performance comparison based on the criterion that model compensation methods aim to minimise: the KL divergence to the ideal compensation. It gives the point where the Kullback–Leibler (KL) divergence is zero. This paper examines the performance of various compensation methods, such as vector Taylor series (VTS) and data-driven parallel model combination (DPMC). It shows that more accurate modelling than Gaussian-for-Gaussian compensation improves the performance of speech recognition.","['R.C. van Dalen', 'M.J.F. Gales']",January 2013,Computer Speech & Language,"['Speech recognition', 'Noise-robustness']",Importance sampling to compute likelihoods of noise-corrupted speech☆
513,"A conventional approach to noise robust speech recognition consists of employing a speech enhancement pre-processor prior to recognition. However, such a pre-processor usually introduces artifacts that limit recognition performance improvement. In this paper we discuss a framework for improving the interconnection between speech enhancement pre-processors and a recognizer. The framework relies on recent proposals for increasing robustness by replacing the point estimate of the enhanced features with a distribution with a dynamic (i.e. time varying) feature variance. We have recently proposed a model for the dynamic feature variance consisting of a dynamic feature variance root obtained from the pre-processor, which is multiplied by a weight representing the pre-processor uncertainty, and that uses adaptation data to optimize the pre-processor uncertainty weight. The formulation of the method is general and could be used with any speech enhancement pre-processor. However, we observed that in case of noise reduction based on spectral subtraction or related approaches, adaptation could fail because the proposed model is weak at representing well the actual dynamic feature variance. The dynamic feature variance changes according to the level of speech sound, which varies with the HMM states. Therefore, we propose improving the model by introducing HMM state dependency. We achieve this by using a cluster-based representation, i.e. the Gaussians of the acoustic model are grouped into clusters and a different pre-processor uncertainty weight is associated with each cluster. Experiments with various pre-processors and recognition tasks prove the generality of the proposed integration scheme and show that the proposed extension improves the performance with various speech enhancement pre-processors.","['Marc Delcroix', 'Shinji Watanabe', 'Tomohiro Nakatani', 'Atsushi Nakamura']",January 2013,Computer Speech & Language,"['Robust speech recognition', 'Variance compensation', 'Model adaptation', 'Speech enhancement']",Cluster-based dynamic variance adaptation for interconnecting speech enhancement pre-processor and speech recognizer☆
514,"This paper proposes a fast unsupervised acoustic model adaptation technique with efficient statistics accumulation for speech recognition. Conventional adaptation techniques accumulate the acoustic statistics based on a forward–backward algorithm or a Viterbi algorithm. Since both algorithms require a state sequence prior to statistic accumulation, the conventional techniques need time to determine the state sequence by transcribing the target speech in advance. Instead of pre-determining the state sequence, the proposed technique reduces the computation time by accumulating the statistics with state confidence within monophone per frame. It also rapidly selects the appropriate gender acoustic model before adaptation, and further increases the accuracy by employing a power term after adaptation. Recognition experiments using spontaneous speech show that the proposed technique reduces computation time by 57.3% while providing the same accuracy as the conventional adaptation technique.","['Satoshi Kobashikawa', 'Atsunori Ogawa', 'Taichi Asami', 'Yoshikazu Yamaguchi', 'Hirokazu Masataki', 'Satoshi Takahashi']",January 2013,Computer Speech & Language,"['68T10', '43.72.Ne', 'Speech recognition', 'Unsupervised adaptation', 'Context independent model', 'Gaussian mixture model']",Fast unsupervised adaptation based on efficient statistics accumulation using frame independent confidence within monophone states☆
515,"The performance of recent dereverberation methods for reverberant speech preprocessing prior to Automatic Speech Recognition (ASR) is compared for an extensive range of room and source-receiver configurations. It is shown that room acoustic parameters such as the clarity (C50) and the definition (D50) correlate well with the ASR results. When available, such room acoustic parameters can provide insight into reverberant speech ASR performance and potential improvement via dereverberation preprocessing. It is also shown that the application of a recent dereverberation method based on perceptual modelling can be used in the above context and achieve significant Phone Recognition (PR) improvement, especially under highly reverberant conditions.","['Alexandros Tsilfidis', 'Iosif Mporas', 'John Mourjopoulos', 'Nikos Fakotakis']",January 2013,Computer Speech & Language,"['Reverberant speech enhancement', 'Automatic Speech Recognition', 'Room acoustics']",Automatic speech recognition performance in different room acoustic environments with and without dereverberation preprocessing☆
516,,[],January 2013,Computer Speech & Language,[],Reviewer Acknowledgement
517,"The classification accuracy of text-based language identification depends on several factors, including the size of the text fragment to be identified, the amount of training data available, the classification features and algorithm employed, and the similarity of the languages to be identified. To date, no systematic study of these factors and their interactions has been published. We therefore investigate the effects of each of these factors and their relations on the performance of text-based language identification.Our study uses n-gram statistics as features for classification. In particular, we compare support vector machines, Naïve Bayesian and difference-in-frequency classifiers on different amounts of input text and various values of n for different amounts of training data. For a fixed value of n the support vector machines generally outperform the other classifiers, but the simpler classifiers are able to handle larger values of n. The additional computational complexity of training the support vector machine classifier may not be justified in light of importance of using a large value of n, except possibly for small sizes of the input window when limited training data is available.Our training and testing corpora consisted of text from the 11 official languages of South Africa. Within these languages distinct language families can be found. We find that it is much more difficult to discriminate languages within languages families than languages in different families. The overall accuracy on short input strings is low for this reason, but for input strings of 100 characters or more there is only a slight confusion within families and accuracies as high as 99.4% are achieved. For the smallest input strings studied here, which consist of 15 characters, the best accuracy achieved is only 83%, but when languages in different families are grouped together, this corresponds to a usable 95.1% accuracy.The relationship between the amount of training data and the accuracy achieved is found to depend on the window size: for the largest window (300 characters) about 400 000 characters are sufficient to achieve close-to-optimal accuracy, whereas improvements in accuracy are found even beyond 1.6 million characters of training data for smaller windows.Our study concludes that the correlation between the factors studied significantly affect classification accuracy; therefore, to assure credible and comparable results, these factors need to be controlled in any text-based language identification task.","['Gerrit Reinier Botha', 'Etienne Barnard']",October 2012,Computer Speech & Language,"['Text-based language identification', 'n-Gram statistics', 'Naïve Bayesian classification', 'Difference-in-frequency classification', 'Support vector machine']",Factors that affect the accuracy of text-based language identification☆
518,"Long organization names are often abbreviated in spoken Chinese, and abbreviated utterances cannot be recognized correctly if the abbreviations are not included in the recognition vocabulary. Therefore, it is very important to automatically generate and add abbreviations for organization names to the vocabulary. Generation of Chinese abbreviations is much more complex than English abbreviations which are mostly acronyms and truncations. In this paper, we propose a new hybrid method for automatically generating Chinese abbreviations and we perform vocabulary expansion using output of the abbreviation model for voice search. In our abbreviation modeling, we treat the abbreviation generation problem as a tagging problem and use conditional random fields (CRF) as the tagging tool, the output of which is then re-ranked by a length model and web information. In the vocabulary expansion, considering the multiple abbreviation phenomenon and limited coverage of the top-1 abbreviation candidate, we add top-10 candidates into the vocabulary. In our experiments, for the abbreviation modeling, we achieved a top-10 coverage of 88.3% with the proposed method. For the voice search using abbreviated utterances, we improved the full-name search accuracy from 16.9% to 79.2% by incorporating the top-10 abbreviation candidates to the vocabulary.","['Dong Yang', 'Yi-Cheng Pan', 'Sadaoki Furui']",October 2012,Computer Speech & Language,"['Automatic abbreviation generation', 'Vocabulary expansion', 'Voice search']",Vocabulary expansion through automatic abbreviation generation for Chinese voice search☆☆☆
519,"In this work, a first approach to a robust phoneme recognition task by means of a biologically inspired feature extraction method is presented. The proposed technique provides an approximation to the speech signal representation at the auditory cortical level. It is based on an optimal dictionary of atoms, estimated from auditory spectrograms, and the Matching Pursuit algorithm to approximate the cortical activations. This provides a sparse coding with intrinsic noise robustness, which can be therefore exploited when using the system in adverse environments. The recognition task consisted in the classification of a set of 5 easily confused English phonemes, in both clean and noisy conditions. Multilayer perceptrons were trained as classifiers and the performance was compared to other classic and robust parameterizations: the auditory spectrogram, a probabilistic optimum filtering on Mel frequency cepstral coefficients and the perceptual linear prediction coefficients. Results showed a significant improvement in the recognition rate of clean and noisy phonemes by the cortical representation over these other parameterizations.","['C. Martínez', 'J. Goddard', 'D. Milone', 'H. Rufiner']",October 2012,Computer Speech & Language,"['Robust phoneme recognition', 'Approximated auditory cortical representation', 'Sparse coding']",Bioinspired sparse spectro-temporal representation of speech for robust classification☆
520,"This paper presents a method for automatically transforming faithful transcripts or ASR results into clean transcripts for human consumption using a framework we label speaking style transformation (SST). We perform a detailed analysis of the types of corrections performed by human stenographers when creating clean transcripts, and propose a model that is able to handle the majority of the most common corrections. In particular, the proposed model uses a framework of monotonic statistical machine translation to perform not only the deletion of disfluencies and insertion of punctuation, but also correction of colloquial expressions, insertions of omitted words, and other transformations. We provide a detailed description of the model implementation in the weighted finite state transducer (WFST) framework. An evaluation of the proposed model on both faithful transcripts and speech recognition results of parliamentary and lecture speech demonstrates the effectiveness of the proposed model in performing the wide variety of corrections necessary for creating clean transcripts.","['Graham Neubig', 'Yuya Akita', 'Shinsuke Mori', 'Tatsuya Kawahara']",October 2012,Computer Speech & Language,"['Rich transcription', 'Speaking style transformation', 'Disfluency detection', 'Weighted finite state transducers', 'Monotonic machine translation']",A monotonic statistical machine translation approach to speaking style transformation☆
521,"Graphics processing units (GPUs) provide substantial processing power for little cost. We explore the application of GPUs to speech pattern processing, using language identification (LID) to demonstrate their benefits. Realization of the full potential of GPUs requires both effective coding of predetermined algorithms, and, if there is a choice, selection of the algorithm or technique for a specific function that is most able to exploit the GPU. We demonstrate these principles using the NIST LRE 2003 standard LID task, a batch processing task which involves the analysis of over 600 h of speech. We focus on two parts of the system, namely the acoustic classifier, which is based on a 2048 component Gaussian Mixture Model (GMM), and acoustic feature extraction. In the case of the latter we compare a conventional FFT-based analysis with IIR and FIR filter banks, both in terms of their ability to exploit the GPU architecture and LID performance. With no increase in error rate our GPU based system, with an FIR-based front-end, completes the NIST LRE 2003 task in 16 h, compared with 180 h for the conventional FFT-based system on a standard CPU (a speed up factor of more than 11). This includes a 61% decrease in front-end processing time. In the GPU implementation, front-end processing accounts for 8% and 10% of the total computing times during training and recognition, respectively. Hence the reduction in front-end processing achieved in the GPU implementation is significant.","['A. Hanani', 'M.J. Carey', 'M.J. Russell']",October 2012,Computer Speech & Language,"['Graphical processing units', 'Speech spectral analysis', 'Feature extraction', 'Language identification']",Language identification using multi-core processors☆
522,"We present a new generative model of natural language, the latent words language model. This model uses a latent variable for every word in a text that represents synonyms or related words in the given context. We develop novel methods to train this model and to find the expected value of these latent variables for a given unseen text. The learned word similarities help to reduce the sparseness problems of traditional n-gram language models. We show that the model significantly outperforms interpolated Kneser–Ney smoothing and class-based language models on three different corpora. Furthermore the latent variables are useful features for information extraction. We show that both for semantic role labeling and word sense disambiguation, the performance of a supervised classifier increases when incorporating these variables as extra features. This improvement is especially large when using only a small annotated corpus for training.","['Koen Deschacht', 'Jan De Belder', 'Marie-Francine Moens']",October 2012,Computer Speech & Language,"['Language model', 'Information extraction', 'Word sense disambiguation', 'Semantic role labeling']",The latent words language model☆
523,"In recent years, the use of morphological decomposition strategies for Arabic Automatic Speech Recognition (ASR) has become increasingly popular. Systems trained on morphologically decomposed data are often used in combination with standard word-based approaches, and they have been found to yield consistent performance improvements. The present article contributes to this ongoing research endeavour by exploring the use of the ‘Morphological Analysis and Disambiguation for Arabic’ (MADA) tools for this purpose. System integration issues concerning language modelling and dictionary construction, as well as the estimation of pronunciation probabilities, are discussed. In particular, a novel solution for morpheme-to-word conversion is presented which makes use of an N-gram Statistical Machine Translation (SMT) approach. System performance is investigated within a multi-pass adaptation/combination framework. All the systems described in this paper are evaluated on an Arabic large vocabulary speech recognition task which includes both Broadcast News and Broadcast Conversation test data. It is shown that the use of MADA-based systems, in combination with word-based systems, can reduce the Word Error Rates by up to 8.1% relative.","['F. Diehl', 'M.J.F. Gales', 'M. Tomalin', 'P.C. Woodland']",August 2012,Computer Speech & Language,"['Automatic Speech Recognition', 'Arabic', 'Morphology', 'Pronunciation probabilities']",Morphological decomposition in Arabic ASR systems☆
524,"Although Hidden Markov Models (HMMs) are still the mainstream approach towards speech recognition, their intrinsic limitations such as first-order Markov models in use or the assumption of independent and identically distributed frames lead to the extensive use of higher level linguistic information to produce satisfactory results. Therefore, researchers began investigating the incorporation of various discriminative techniques at the acoustical level to induce more discrimination between speech units. As is known, the k-nearest neighbour (k-NN) density estimation is discriminant by nature and is widely used in the pattern recognition field. However, its application to speech recognition has been limited to few experiments. In this paper, we introduce a new segmental k-NN-based phoneme recognition technique. In this approach, a group-delay-based method generates phoneme boundary hypotheses, and an approximate version of k-NN density estimation is used for the classification and scoring of variable-length segments. During the decoding, the construction of the phonetic graph starts from the best phoneme boundary setting and progresses through splitting and merging segments using the remaining boundary hypotheses and constraints such as phoneme duration and broad-class similarity information. To perform the k-NN search, we take advantage of a similarity search algorithm called Spatial Approximate Sample Hierarchy (SASH). One major advantage of the SASH algorithm is that its computational complexity is independent of the dimensionality of the data. This allows us to use high-dimensional feature vectors to represent phonemes. By using phonemes as units of speech, the search space is very limited and the decoding process fast. Evaluation of the proposed algorithm with the sole use of the best hypothesis for every segment and excluding phoneme transitional probabilities, context-based, and language model information results in an accuracy of 58.5% with correctness of 67.8% on the TIMIT test dataset.","['Ladan Golipour', 'Douglas O’Shaughnessy']",August 2012,Computer Speech & Language,"['Phoneme recognition', 'k-Nearest neighbour density estimation', 'Approximate similarity search algorithms']",A segmental non-parametric-based phoneme recognition approach at the acoustical level☆
525,"Automatic text summarization is an essential tool in this era of information overloading. In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. It is a direct system where no machine learning is involved. We use a two pass algorithm where in pass one, we produce a primary summary using Rhetorical Structure Theory (RST); this is followed by the second pass where we assign a score to each of the sentences in the primary summary. These scores will help us in generating the final summary. For the final output, sentences are selected with an objective of maximizing the overall score of the summary whose size should not exceed the user selected limit. We used Rouge to evaluate our system generated summaries of various lengths against those done by a (human) news editorial professional. Experiments on sample texts show our system to outperform some of the existing Arabic summarization systems including those that require machine learning.","['Aqil M. Azmi', 'Suha Al-Thanyyan']",August 2012,Computer Speech & Language,"['Arabic NLP', 'Rhetorical Structure Theory', 'Automatic text summarization', '0/1-Knapsack', 'Rouge']",A text summarizer for Arabic★
526,"We propose a two-stage phone duration modelling scheme, which can be applied for the improvement of prosody modelling in speech synthesis systems. This scheme builds on a number of independent feature constructors (FCs) employed in the first stage, and a phone duration model (PDM) which operates on an extended feature vector in the second stage. The feature vector, which acts as input to the first stage, consists of numerical and non-numerical linguistic features extracted from text. The extended feature vector is obtained by appending the phone duration predictions estimated by the FCs to the initial feature vector. Experiments on the American-English KED TIMIT and on the Modern Greek WCL-1 databases validated the advantage of the proposed two-stage scheme, improving prediction accuracy over the best individual predictor, and over a two-stage scheme which just fuses the first-stage outputs. Specifically, when compared to the best individual predictor, a relative reduction in the mean absolute error and the root mean square error of 3.9% and 3.9% on the KED TIMIT, and of 4.8% and 4.6% on the WCL-1 database, respectively, is observed.","['Alexandros Lazaridis', 'Todor Ganchev', 'Iosif Mporas', 'Evaggelos Dermatas', 'Nikos Fakotakis']",August 2012,Computer Speech & Language,"['Feature construction', 'Phone duration modelling', 'Statistical modelling', 'Text-to-speech synthesis']",Two-stage phone duration modelling with feature construction and feature vector extension for the needs of speech synthesis☆
527,"Glottal stop sounds in Amharic are produced due to abrupt closure of the glottis without any significant gesture in the accompanying articulatory organs in the vocal tract system. It is difficult to observe the features of the glottal stop through spectral analysis, as the spectral features emphasize mostly the features of the vocal tract system. In order to spot the glottal stop sounds in continuous speech, it is necessary to extract the features of the source of excitation also, which may require some non-spectral methods for analysis. In this paper the linear prediction (LP) residual is used as an approximation to the excitation source signal, and the excitation features are extracted from the LP residual using zero frequency filtering (ZFF). The glottal closure instants (GCIs) or epoch are identified from the ZFF signal. At each GCI, the cross-correlation coefficients of successive glottal cycles of the LP residual, the normalized jitter and the logarithm of the peak normalized excitation strength (LPNES) are calculated. Further, the parameters of Gaussian approximation models are derived from the distributions of the excitation parameters. These model parameters are used to identify the regions of the glottal stop sounds in continuous speech. For the database used in this study 92.89% of the glottal stop regions are identified correctly, with 8.50% false indications.","['Hussien Seid', 'B. Yegnanarayana', 'S. Rajendran']",August 2012,Computer Speech & Language,"['Amharic', 'Glottal stop', 'Acoustic characteristics', 'Non-spectral methods', 'LPNES', 'Normalized jitter', 'Creaky voice']",Spotting glottal stop in Amharic in continuous speech☆
528,"Although English pitch accent detection has been studied extensively, there relatively a few works explore Mandarin stress detection. Moreover, the comparison and analysis between Mandarin stress detection and English pitch accent detection have not been touched for such counterpart tasks. In this paper, we discuss Mandarin stress detection and compare it with English pitch accent detection. The contributions of the paper are two aspects: one is that we use classifier combination method to detect Mandarin stress and English pitch accent by using acoustic, lexical and syntactic evidence. Our proposed method achieves better performance on both the Mandarin prosodic annotation corpus—ASCCD and the English prosodic annotation corpus—Boston University Radio News Corpus (BURNC) when compared with the baseline system. We also verify our proposed method on other prosodic annotation corpus and continuous speech corpus. The other is the feature analysis. Duration, pitch, energy and intensity features are compared for Mandarin stress detection and English pitch accent detection. Based on the analysis of prosodic annotation corpora, we also verify some linguistic conclusions.","['Chongjia Ni', 'Wenju Liu', 'Bo Xu']",June 2012,Computer Speech & Language,"['Mandarin stress detection', 'Boosting classification and regression tree (CART)', 'Conditional random fields (CRFs)', 'Neural network (NN)', 'Support vector machine (SVM)']","From English pitch accent detection to Mandarin stress detection, where is the difference?☆"
529,"This paper describes a preprocessing module for improving the performance of a Spanish into Spanish Sign Language (Lengua de Signos Española: LSE) translation system when dealing with sparse training data. This preprocessing module replaces Spanish words with associated tags. The list with Spanish words (vocabulary) and associated tags used by this module is computed automatically considering those signs that show the highest probability of being the translation of every Spanish word. This automatic tag extraction has been compared to a manual strategy achieving almost the same improvement. In this analysis, several alternatives for dealing with non-relevant words have been studied. Non-relevant words are Spanish words not assigned to any sign. The preprocessing module has been incorporated into two well-known statistical translation architectures: a phrase-based system and a Statistical Finite State Transducer (SFST). This system has been developed for a specific application domain: the renewal of Identity Documents and Driver's License. In order to evaluate the system a parallel corpus made up of 4080 Spanish sentences and their LSE translation has been used. The evaluation results revealed a significant performance improvement when including this preprocessing module. In the phrase-based system, the proposed module has given rise to an increase in BLEU (Bilingual Evaluation Understudy) from 73.8% to 81.0% and an increase in the human evaluation score from 0.64 to 0.83. In the case of SFST, BLEU increased from 70.6% to 78.4% and the human evaluation score from 0.65 to 0.82.","['Verónica López-Ludeña', 'Rubén San-Segundo', 'Juan Manuel Montero', 'Ricardo Córdoba', 'Javier Ferreiros', 'José Manuel Pardo']",June 2012,Computer Speech & Language,"['Spanish Sign Language (LSE)', 'Statistical language translation', 'Syntactic-semantic information', 'Automatic tagging', 'Automatic categorization']",Automatic categorization for improving Spanish into Spanish Sign Language machine translation☆
530,"Reinforcement techniques have been successfully used to maximise the expected cumulative reward of statistical dialogue systems. Typically, reinforcement learning is used to estimate the parameters of a dialogue policy which selects the system's responses based on the inferred dialogue state. However, the inference of the dialogue state itself depends on a dialogue model which describes the expected behaviour of a user when interacting with the system. Ideally the parameters of this dialogue model should be also optimised to maximise the expected cumulative reward.This article presents two novel reinforcement algorithms for learning the parameters of a dialogue model. First, the Natural Belief Critic algorithm is designed to optimise the model parameters while the policy is kept fixed. This algorithm is suitable, for example, in systems using a handcrafted policy, perhaps prescribed by other design considerations. Second, the Natural Actor and Belief Critic algorithm jointly optimises both the model and the policy parameters. The algorithms are evaluated on a statistical dialogue system modelled as a Partially Observable Markov Decision Process in a tourist information domain. The evaluation is performed with a user simulator and with real users. The experiments indicate that model parameters estimated to maximise the expected reward function provide improved performance compared to the baseline handcrafted parameters.","['Filip Jurčíček', 'Blaise Thomson', 'Steve Young']",June 2012,Computer Speech & Language,"['Spoken dialogue systems', 'Reinforcement learning', 'POMDP', 'Dialogue management']",Reinforcement learning for parameter estimation in statistical spoken dialogue systems☆
531,"In this paper we present a statistical approach to question answering (QA). Our motivation is to build robust systems for many languages without the need for highly tuned linguistic modules. Consequently, word tokens and web data are used extensively but neither explicit linguistic knowledge nor annotated data is incorporated. A mathematical model for answer retrieval and answer classification is derived. Experiments are conducted by searching for answers in the AQUAINT corpus, as well as in web data. The redundancy inherent in web data outperforms retrieval from a fixed corpus, where there are typically relatively few answer occurrences for any given question. We participated with an implementation of this framework in the TREC 2006 QA evaluations, where we ranked 9th among 27 participants on the factoid task.","['Matthias H. Heie', 'Edward W.D. Whittaker', 'Sadaoki Furui']",June 2012,Computer Speech & Language,"['Question answering', 'Language modelling']",Question answering using statistical language modelling☆☆☆
532,"A novel approach for joint speaker identification and speech recognition is presented in this article. Unsupervised speaker tracking and automatic adaptation of the human–computer interface is achieved by the interaction of speaker identification, speech recognition and speaker adaptation for a limited number of recurring users. Together with a technique for efficient information retrieval a compact modeling of speech and speaker characteristics is presented. Applying speaker specific profiles allows speech recognition to take individual speech characteristics into consideration to achieve higher recognition rates. Speaker profiles are initialized and continuously adapted by a balanced strategy of short-term and long-term speaker adaptation combined with robust speaker identification. Different users can be tracked by the resulting self-learning speech controlled system. Only a very short enrollment of each speaker is required. Subsequent utterances are used for unsupervised adaptation resulting in continuously improved speech recognition rates. Additionally, the detection of unknown speakers is examined under the objective to avoid the requirement to train new speaker profiles explicitly. The speech controlled system presented here is suitable for in-car applications, e.g. speech controlled navigation, hands-free telephony or infotainment systems, on embedded devices. Results are presented for a subset of the SPEECON database. The results validate the benefit of the speaker adaptation scheme and the unified modeling in terms of speaker identification and speech recognition rates.","['Tobias Herbig', 'Franz Gerl', 'Wolfgang Minker']",June 2012,Computer Speech & Language,"['Speaker identification', 'Speech recognition', 'Speaker adaptation']",Self-learning speaker identification for enhanced speech recognition☆
533,"The training of state-of-the-art automatic speech recognition (ASR) systems requires huge relevant training corpora. The cost of such databases is high and remains a major limitation for the development of speech-enabled applications in particular contexts (e.g. low-density languages or specialized domains). On the other hand, a large amount of data can be found in news prompts, movie subtitles or scripts, etc. The use of such data as training corpus could provide a low-cost solution to the acoustic model estimation problem. Unfortunately, prior transcripts are seldom exact with respect to the content of the speech signal, and suffer from a lack of temporal information. This paper tackles the issue of prompt-based speech corpora improvement, by addressing the problems mentioned above. We propose a method allowing to locate accurate transcript segments in speech signals and automatically correct errors or lack of transcript surrounding these segments. This method relies on a new decoding strategy where the search algorithm is driven by the imperfect transcription of the input utterances. The experiments are conducted on the French language, by using the ESTER database and a set of records (and associated prompts) from RTBF (Radio Télévision Belge Francophone). The results demonstrate the effectiveness of the proposed approach, in terms of both error correction and text-to-speech alignment.","['Benjamin Lecouteux', 'Georges Linarès', 'Stanislas Oger']",April 2012,Computer Speech & Language,"['Speech processing', 'Acoustic model training', 'Text-to-speech alignment']",Integrating imperfect transcripts into speech recognition systems for building high-quality corpora☆
534,"Transcript-based topic segmentation of TV programs faces several difficulties arising from transcription errors, from the presence of potentially short segments and from the limited number of word repetitions to enforce lexical cohesion, i.e., lexical relations that exist within a text to provide a certain unity. To overcome these problems, we extend a probabilistic measure of lexical cohesion based on generalized probabilities with a unigram language model. On the one hand, confidence measures and semantic relations are considered as additional sources of information. On the other hand, language model interpolation techniques are investigated for better language model estimation. Experimental topic segmentation results are presented on two corpora with distinct characteristics, composed respectively of broadcast news and reports on current affairs. Significant improvements are obtained on both corpora, demonstrating the effectiveness of the extended lexical cohesion measure for spoken TV contents, as well as its genericity over different programs.","['Camille Guinaudeau', 'Guillaume Gravier', 'Pascale Sébillot']",April 2012,Computer Speech & Language,"['Topic segmentation', 'Lexical cohesion', 'Confidence measures', 'Semantic relations', 'Language model interpolation', 'TV broadcasts']","Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation☆"
535,"The task of reviewing scientific publications and keeping up with the literature in a particular domain is extremely time-consuming. Extraction and exploration of methodological information, in particular, requires systematic understanding of the literature, but in many cases is performed within a limited context of publications that can be manually reviewed by an individual or group. Automated methodology identification could provide an opportunity for systematic retrieval of relevant documents and for exploring developments within a given discipline. In this paper we present a system for the identification of methodology mentions in scientific publications in the area of natural language processing, and in particular in automatic terminology recognition. The system comprises two major layers: the first layer is an automatic identification of methodological sentences; the second layer highlights methodological phrases (segments). Each mention is categorised in four semantic categories: Task, Method, Resource/Feature and Implementation. Extraction and classification of the segments is formalised as a sequence tagging problem and four separate phrase-based Conditional Random Fields are used to accomplish the task. The system has been evaluated on a manually annotated corpus comprising 45 full text articles. The results for the segment level annotation show an F-measure of 53% for identification of Task and Method mentions (with 70% precision), whereas the F-measures for Resource/Feature and Implementation identification were 61% (with 67% precision) and 75% (with 86% precision) respectively. At the document-level, an F-measure of 72% (with 81% precision) for Task mentions, 60% (with 81% precision) for Method mentions, 74% (with 78% precision) for the Resource/Feature and 79% (with 81% precision) for the Implementation categories have been achieved. We provide a detailed analysis of errors and explore the impact that the particular groups of features have on the extraction of methodological segments.","['Aleksandar Kovačević', 'Zora Konjović', 'Branko Milosavljević', 'Goran Nenadic']",April 2012,Computer Speech & Language,"['Information extraction', 'Methodology mining', 'Conditional Random Fields', 'Automatic terminology mining']",Mining methodologies from NLP publications: A case study in automatic terminology recognition☆
536,,[],April 2012,Computer Speech & Language,[],Reviewer Acknowledgement 2010
537,"Evaluation and optimization of automatic speech recognition (ASR) and parsing systems are often done separately. In the context of spoken language processing, however, these problems may be explored jointly via a reranking architecture. In this work, the effects of reranking for word error rate (WER) or reranking for the Sparseval parse-quality measure are examined in conversational speech recognition, while considering the impact of automatic segmentation. Under a WER criterion, the results indicate that the parse language model alone provides little benefit over a large n-gram model, but adding non-local syntactic features leads to improved performance. Under a Sparseval criterion, it is shown that including alternative word-sequence hypotheses has a much greater impact on parse accuracy than including alternate parse hypotheses. In both cases, the biggest performance improvements are obtained with high quality sentence segmentations. Qualitative analyses show that parse features help recover pronouns and improve recognition of main verbs.","['Jeremy G. Kahn', 'Mari Ostendorf']",January 2012,Computer Speech & Language,"['Parsing', 'Automatic speech recognition', 'Conversational speech', 'Sentence segmentation']",Joint reranking of parsing and word recognition with automatic segmentation☆
538,"Source-tract decomposition (or glottal flow estimation) is one of the basic problems of speech processing. For this, several techniques have been proposed in the literature. However, studies comparing different approaches are almost nonexistent. Besides, experiments have been systematically performed either on synthetic speech or on sustained vowels. In this study we compare three of the main representative state-of-the-art methods of glottal flow estimation: closed-phase inverse filtering, iterative and adaptive inverse filtering, and mixed-phase decomposition. These techniques are first submitted to an objective assessment test on synthetic speech signals. Their sensitivity to various factors affecting the estimation quality, as well as their robustness to noise are studied. In a second experiment, their ability to label voice quality (tensed, modal, soft) is studied on a large corpus of real connected speech. It is shown that changes of voice quality are reflected by significant modifications in glottal feature distributions. Techniques based on the mixed-phase decomposition and on a closed-phase inverse filtering process turn out to give the best results on both clean synthetic and real speech signals. On the other hand, iterative and adaptive inverse filtering is recommended in noisy environments for its high robustness.","['Thomas Drugman', 'Baris Bozkurt', 'Thierry Dutoit']",January 2012,Computer Speech & Language,"['Source-tract separation', 'Glottal flow estimation', 'Inverse filtering', 'Mixed-phase decomposition', 'Voice quality']",A comparative study of glottal source estimation techniques☆
539,"Constrained Maximum Likelihood Linear Regression (CMLLR) is a speaker adaptation method for speech recognition that can be realized as a feature-space transformation. In its original form it does not work well when the amount of speech available for adaptation is less than about 5 s, because of the difficulty of robustly estimating the parameters of the transformation matrix. In this paper we describe a basis representation of the CMLLR transformation matrix, in which the variation between speakers is concentrated in the leading coefficients. When adapting to a speaker, we can select a variable number of coefficients to estimate depending on the amount of adaptation data available, and assign a zero value to the remaining coefficients. We obtain improved performance when the amount of adaptation data is limited, while retaining the same asymptotic performance as conventional CMLLR. We demonstrate that our method performs better than the popular existing approaches, and is more efficient than conventional CMLLR estimation.","['Daniel Povey', 'Kaisheng Yao']",January 2012,Computer Speech & Language,"['Speech recognition', 'Speaker adaptation']",A basis representation of constrained MLLR transforms for robust adaptation☆
540,"We present an overview of the data collection and transcription efforts for the COnversational Speech In Noisy Environments (COSINE) corpus. The corpus is a set of multi-party conversations recorded in real world environments with background noise. It can be used to train noise-robust speech recognition systems or develop speech de-noising algorithms. We explain the motivation for creating such a corpus, and describe the resulting audio recordings and transcriptions that comprise the corpus. These high quality recordings were captured in situ on a custom wearable recording system, whose design and construction is also described. On separate synchronized audio channels, seven-channel audio is captured with a 4-channel far-field microphone array, along with a close-talking, a monophonic far-field, and a throat microphone. This corpus thus creates many possibilities for speech algorithm research.","['Alex Stupakov', 'Evan Hanusa', 'Deepak Vijaywargi', 'Dieter Fox', 'Jeff Bilmes']",January 2012,Computer Speech & Language,"['Speech recognition', 'Microphone arrays', 'Multi-party corpora', 'Multi-microphone', 'Portable recording', 'Noise-robust speech recognition']","The design and collection of COSINE, a multi-microphone in situ speech corpus recorded in noisy environments☆"
541,"A novel updating method for Probabilistic Latent Semantic Analysis (PLSA), called Recursive PLSA (RPLSA), is proposed. The updating of conditional probabilities is derived from first principles for both the asymmetric and the symmetric PLSA formulations. The performance of RPLSA for both formulations is compared to that of the PLSA folding-in, the PLSA rerun from the breakpoint, and well-known LSA updating methods, such as the singular value decomposition (SVD) folding-in and the SVD-updating. The experimental results demonstrate that the RPLSA outperforms the other updating methods under study with respect to the maximization of the average log-likelihood and the minimization of the average absolute error between the probabilities estimated by the updating methods and those derived by applying the non-adaptive PLSA from scratch. A comparison in terms of CPU run time is conducted as well. Finally, in document clustering using the Adjusted Rand index, it is demonstrated that the clusters generated by the RPLSA are: (a) similar to those generated by the PLSA applied from scratch; (b) closer to the ground truth than those created by the other PLSA or LSA updating methods.","['N. Bassiou', 'C. Kotropoulos']",October 2011,Computer Speech & Language,"['PLSA', 'PLSA updating', 'Document clustering', 'Information retrieval', 'Adjusted Rand', 'Expectation Maximization']",RPLSA: A novel updating scheme for Probabilistic Latent Semantic Analysis☆
542,"In this paper, we develop an approach called syntax-based reordering (SBR) to handling the fundamental problem of word ordering for statistical machine translation (SMT). We propose to alleviate the word order challenge including morpho-syntactical and statistical information in the context of a pre-translation reordering framework aimed at capturing short- and long-distance word distortion dependencies. We examine the proposed approach from the theoretical and experimental points of view discussing and analyzing its advantages and limitations in comparison with some of the state-of-the-art reordering methods.In the final part of the paper, we describe the results of applying the syntax-based model to translation tasks with a great need for reordering (Chinese-to-English and Arabic-to-English). The experiments are carried out on standard phrase-based and alternative N-gram-based SMT systems. We first investigate sparse training data scenarios, in which the translation and reordering models are trained on a sparse bilingual data, then scaling the method to a large training set and demonstrating that the improvement in terms of translation quality is maintained.","['Maxim Khalilov', 'José A.R. Fonollosa']",October 2011,Computer Speech & Language,"['Statistical machine translation', 'Word reordering', 'Natural language processing', 'Computational linguistics']",Syntax-based reordering for statistical machine translation☆
543,"We consider the problem of using speech processing to characterize an aggregate of voice data, in contrast to inferences about individual voice cuts. We derive simple turn-taking models from speaker activity detection output on the Switchboard-1 corpus. These can be used to cluster speakers into turn-taking ‘styles.’ Demographic fields and turn-taking behavior prove to be statistically dependent, thus observed speaker activity improves estimates of the demographics of held-out data. Finally, we use turn-taking style to estimate speaker influence.","['John Grothendieck', 'Allen L. Gorin', 'Nash Borges']",October 2011,Computer Speech & Language,"['Speech activity detection', 'Turn-taking behavior']",Social correlates of turn-taking style☆
544,"In this paper we describe a method that can be used for Minimum Bayes Risk (MBR) decoding for speech recognition. Our algorithm can take as input either a single lattice, or multiple lattices for system combination. It has similar functionality to the widely used Consensus method, but has a clearer theoretical basis and appears to give better results both for MBR decoding and system combination. Many different approximations have been described to solve the MBR decoding problem, which is very difficult from an optimization point of view. Our proposed method solves the problem through a novel forward–backward recursion on the lattice, not requiring time markings. We prove that our algorithm iteratively improves a bound on the Bayes risk.","['Haihua Xu', 'Daniel Povey', 'Lidia Mangu', 'Jie Zhu']",October 2011,Computer Speech & Language,"['Speech recognition', 'Minimum Bayes Risk decoding']",Minimum Bayes Risk decoding and system combination based on a recursion for edit distance
545,"Audio pattern classification represents a particular statistical classification task and includes, for example, speaker recognition, language recognition, emotion recognition, speech recognition and, recently, video genre classification. The feature being used in all these tasks is generally based on a short-term cepstral representation. The cepstral vectors contain at the same time useful information and nuisance variability, which are difficult to separate in this domain. Recently, in the context of GMM-based recognizers, a novel approach using a Factor Analysis (FA) paradigm has been proposed for decomposing the target model into a useful information component and a session variability component. This approach is called Joint Factor Analysis (JFA), since it models jointly the nuisance variability and the useful information, using the FA statistical method. The JFA approach has even been combined with Support Vector Machines, known for their discriminative power. In this article, we successfully apply this paradigm to three automatic audio processing applications: speaker verification, language recognition and video genre classification. This is done by applying the same process and using the same free software toolkit. We will show that this approach allows for a relative error reduction of over 50% in all the aforementioned audio processing tasks.","['Driss Matrouf', 'Florian Verdet', 'Mickaël Rouvier', 'Jean-François Bonastre', 'Georges Linarès']",July 2011,Computer Speech & Language,"['Speaker', 'Language', 'Video genre', 'Session variability', 'Nuisance variability', 'Joint Factor Analysis', 'SVM']",Modeling nuisance variabilities with factor analysis for GMM-based audio pattern classification☆
546,"The lexical items like and well can serve as discourse markers (DMs), but can also play numerous other roles, such as verb or adverb. Identifying the occurrences that function as DMs is an important step for language understanding by computers. In this study, automatic classifiers using lexical, prosodic/positional and sociolinguistic features are trained over transcribed dialogues, manually annotated with DM information. The resulting classifiers improve state-of-the-art performance of DM identification, at about 90% recall and 79% precision for like (84.5% accuracy, κ = 0.69), and 99% recall and 98% precision for well (97.5% accuracy, κ = 0.88). Automatic feature analysis shows that lexical collocations are the most reliable indicators, followed by prosodic/positional features, while sociolinguistic features are marginally useful for the identification of DM like and not useful for well. The differentiated processing of each type of DM improves classification accuracy, suggesting that these types should be treated individually.","['Andrei Popescu-Belis', 'Sandrine Zufferey']",July 2011,Computer Speech & Language,"['Discourse markers', 'Discourse marker identification', 'Statistical classifiers', 'Lexical features', 'Prosodic features']",Automatic identification of discourse markers in dialogues: An in-depth study of like and well☆
547,"In recent years, the use of Multi-Layer Perceptron (MLP) derived acoustic features has become increasingly popular in automatic speech recognition systems. These features are typically used in combination with standard short-term spectral-based features, and have been found to yield consistent performance improvements. However there are a number of design decisions and issues associated with the use of MLP features for state-of-the-art speech recognition systems. Two modifications to the standard training/adaptation procedures are described in this work. First, the paper examines how MLP features, and the associated acoustic models, can be trained efficiently on large training corpora using discriminative training techniques. An approach that combines multiple individual MLPs is proposed, and this reduces the time needed to train MLPs on large amounts of data. In addition, to further speed up discriminative training, a lattice re-use method is proposed. The paper also examines how systems with MLP features can be adapted to a particular speakers, or acoustic environments. In contrast to previous work (where standard HMM adaptation schemes are used), linear input network adaptation is investigated. System performance is investigated within a multi-pass adaptation/combination framework. This allows the performance gains of individual techniques to be evaluated at various stages, as well as the impact in combination with other sub-systems. All the approaches considered in this paper are evaluated on an Arabic large vocabulary speech recognition task which includes both Broadcast News and Broadcast Conversation test data.","['J. Park', 'F. Diehl', 'M.J.F. Gales', 'M. Tomalin', 'P.C. Woodland']",July 2011,Computer Speech & Language,"['Automatic speech recognition', 'MLP feature', 'Acoustic modelling', 'Speaker adaptation']",The efficient incorporation of MLP features into automatic speech recognition systems
548,"We present the Vocal Joystick engine, a real-time software library which can be used to map non-linguistic vocalizations into realizable continuous control signals. The system is designed to strike a balance between low latency and accurate recognition while simultaneously taking advantage of the rich complexity of sounds producible by the human vocal tract. By developing a modular, cross-platform library, we aim to provide a robust but simple means of incorporating such controls into any application, thereby producing a new form of accessible technology. This is demonstrated by the various applications that so far have used the Vocal Joystick engine. Unlike previous discussions of parts of the Vocal Joystick, this paper presents a detailed view of the inner workings of the current version of the engine.","['Jonathan Malkin', 'Xiao Li', 'Susumu Harada', 'James Landay', 'Jeff Bilmes']",July 2011,Computer Speech & Language,"['Speech', 'Speech recognition', 'Vocal control', 'Toolkit', 'Human–computer interaction', 'Assistive technology']",The Vocal Joystick Engine v1.0
549,"The recognition of the emotional state of speakers is a multi-disciplinary research area that has received great interest over the last years. One of the most important goals is to improve the voice-based human–machine interactions. Several works on this domain use the prosodic features or the spectrum characteristics of speech signal, with neural networks, Gaussian mixtures and other standard classifiers. Usually, there is no acoustic interpretation of types of errors in the results. In this paper, the spectral characteristics of emotional signals are used in order to group emotions based on acoustic rather than psychological considerations. Standard classifiers based on Gaussian Mixture Models, Hidden Markov Models and Multilayer Perceptron are tested. These classifiers have been evaluated with different configurations and input features, in order to design a new hierarchical method for emotion classification. The proposed multiple feature hierarchical method for seven emotions, based on spectral and prosodic information, improves the performance over the standard classifiers and the fixed features.","['Enrique M. Albornoz', 'Diego H. Milone', 'Hugo L. Rufiner']",July 2011,Computer Speech & Language,"['Emotion recognition', 'Spectral information', 'Hierarchical classifiers', 'Hidden Markov Model', 'Multilayer Perceptron']",Spoken emotion recognition using hierarchical classifiers☆
550,"Automatic speech recognition (ASR) in reverberant environments is still a challenging task. In this study, we propose a robust feature-extraction method on the basis of the normalization of the sub-band temporal modulation envelopes (TMEs). The sub-band TMEs were extracted using a series of constant bandwidth band-pass filters with Hilbert transforms followed by low-pass filtering. Based on these TMEs, the modulation spectrums in both clean and reverberation spaces are transformed to a reference space by using modulation transfer functions (MTFs), wherein the MTFs are estimated as the measure of the modulation transfer effect on the sub-band TMEs between the clean, reverberation, and reference spaces. By using the MTFs on the modulation spectrum, it is supposed that the difference on the modulation spectrum caused by the difference of the recording environments is removed. Based on the normalized modulation spectrum, inverse Fourier transform was conducted to restore the sub-band TMEs by retaining their original phase information. We tested the proposed method on speech recognition experiments in a reverberant room with differing speaker to microphone distance (SMD). For comparison, the recognition performance of using the traditional Mel frequency cepstral coefficients with mean and variance normalization was used as the baseline. The experimental results showed that by averaging the results for SMDs from 50 cm to 400 cm, we obtained a 44.96% relative improvement by only using sub-band TME processing, and obtained a further 15.68% relative improvement by performing the normalization on the modulation spectrum of the sub-band TMEs. In all, we obtained a 53.59% relative improvement, which was better than using other temporal filtering and normalization methods.","['Xugang Lu', 'Masashi Unoki', 'Satoshi Nakamura']",July 2011,Computer Speech & Language,"['01.30.−y', 'Speech reverberation', 'Temporal modulation', 'Sub-band temporal modulation envelope', 'Automatic speech recognition']",Sub-band temporal modulation envelopes and their normalization for automatic speech recognition in reverberant environments☆
551,"In this paper we propose a new method for utilising phase information by complementing it with traditional magnitude-only spectral subtraction speech enhancement through complex spectrum subtraction (CSS). The proposed approach has the following advantages over traditional magnitude-only spectral subtraction: (a) it introduces complementary information to the enhancement algorithm; (b) it reduces the total number of algorithmic parameters; and (c) is designed for improving clean speech magnitude spectra and is therefore suitable for both automatic speech recognition (ASR) and speech perception applications. Oracle-based ASR experiments verify this approach, showing an average of 20% relative word accuracy improvements when accurate estimates of the phase spectrum are available. Based on sinusoidal analysis and assuming stationarity between observations (which is shown to be better approximated as the frame rate is increased), this paper also proposes a novel method for acquiring the phase information called Phase Estimation via Delay Projection (PEDEP). Further oracle ASR experiments validate the potential for the proposed PEDEP technique in ideal conditions. Realistic implementation of CSS with PEDEP shows performance comparable to state of the art spectral subtraction techniques in a range of 15–20 dB signal-to-noise ratio environments. These results clearly demonstrate the potential for using phase spectra in spectral subtractive enhancement applications, and at the same time highlight the need for deriving more accurate phase estimates in a wider range of noise conditions.","['Tristan Kleinschmidt', 'Sridha Sridharan', 'Michael Mason']",July 2011,Computer Speech & Language,"['Speech enhancement', 'Spectral subtraction', 'Robust speech recognition', 'Phase spectrum']",The use of phase in complex spectrum subtraction for robust speech recognition☆
552,"As interactive voice response systems become more prevalent and provide increasingly more complex functionality, it becomes clear that the challenges facing such systems are not solely in their synthesis and recognition capabilities. Issues such as the coordination of turn exchanges between system and user also play an important role in system usability. In particular, both systems and users have difficulty determining when the other is taking or relinquishing the turn. In this paper, we seek to identify turn-taking cues correlated with human–human turn exchanges which are automatically computable. We compare the presence of potential prosodic, acoustic, and lexico-syntactic turn-yielding cues in prosodic phrases preceding turn changes (smooth switches) vs. turn retentions (holds) vs. backchannels in the Columbia Games Corpus, a large corpus of task-oriented dialogues, to determine which features reliably distinguish between these three. We identify seven turn-yielding cues, all of which can be extracted automatically, for future use in turn generation and recognition in interactive voice response (IVR) systems. Testing Duncan’s (1972) hypothesis that these turn-yielding cues are linearly correlated with the occurrence of turn-taking attempts, we further demonstrate that, the greater the number of turn-yielding cues that are present, the greater the likelihood that a turn change will occur. We also identify six cues that precede backchannels, which will also be useful for IVR backchannel generation and recognition; these cues correlate with backchannel occurrence in a quadratic manner. We find similar results for overlapping and for non-overlapping speech.","['Agustín Gravano', 'Julia Hirschberg']",July 2011,Computer Speech & Language,"['Dialogue', 'Turn-taking', 'IVR systems', 'Prosody']",Turn-taking cues in task-oriented dialogue☆
553,,"['Animesh Mukherjee', 'Monojit Choudhury', 'Samer Hassan', 'Smaranda Muresan']",July 2011,Computer Speech & Language,[],EditorialNetwork based models of cognitive and social dynamics of human languages
554,"Archaeological excavations in the sites of the Indus Valley civilization (2500–1900 BCE) in Pakistan and northwestern India have unearthed a large number of artifacts with inscriptions made up of hundreds of distinct signs. To date, there is no generally accepted decipherment of these sign sequences, and there have been suggestions that the signs could be non-linguistic. Here we apply complex network analysis techniques to a database of available Indus inscriptions, with the aim of detecting patterns indicative of syntactic organization. Our results show the presence of patterns, e.g., recursive structures in the segmentation trees of the sequences, that suggest the existence of a grammar underlying these inscriptions.","['Sitabhra Sinha', 'Ashraf Md Izhar', 'Raj Kumar Pan', 'Bryan Kenneth Wells']",July 2011,Computer Speech & Language,"['Linguistic corpus', 'Indus civilization', 'Network analysis']",Network analysis of a corpus of undeciphered Indus civilization inscriptions indicates syntactic organization
555,"How much can we infer about the pronunciation of a language – past or present – by observing which words its speakers rhyme? This paper explores the connection between pronunciation and network structure in sets of rhymes. We consider the rhyme graphs corresponding to rhyming corpora, where nodes are words and edges are observed rhymes. We describe the graph G corresponding to a corpus of ∼ 12000 rhymes from English poetry written c. 1900, and find a close correspondence between graph structure and pronunciation: most connected components show community structure that reflects the distinction between full and half rhymes. We build classifiers for predicting which components correspond to full rhymes, using a set of spectral and non-spectral features. Feature selection gives a small number (1–5) of spectral features, with accuracy and F-measure of ∼90%, reflecting that positive components are essentially those without any good partition. We partition components of G via maximum modularity, giving a new graph, G′, in which the “quality” of components, by several measures, is much higher than in G. We discuss how rhyme graphs could be used for historical pronunciation reconstruction.",['Morgan Sonderegger'],July 2011,Computer Speech & Language,"['Rhymes', 'Graph theory', 'Complex networks', 'Poetry', 'Phonology', 'English']",Applications of graph theory to an English rhyming corpus
556,"A Markov chain analysis of a network generated by the matrix of lexical distances allows for representing complex relationships between different languages in a language family geometrically, in terms of distances and angles. The fully automated method for construction of language taxonomy is tested on a sample of fifty languages of the Indo-European language group and applied to a sample of fifty languages of the Austronesian language group. The Anatolian and Kurgan hypotheses of the Indo-European origin and the ‘express train’ model of the Polynesian origin are thoroughly discussed.","['Ph. Blanchard', 'F. Petroni', 'M. Serva', 'D. Volchenkov']",July 2011,Computer Speech & Language,"['Language taxonomy', 'Lexicostatistic data analysis', 'Indo-European and Polynesian origins']",Geometric representations of language taxonomies
557,"In this study we use bipartite spectral graph partitioning to simultaneously cluster varieties and identify their most distinctive linguistic features in Dutch dialect data. While clustering geographical varieties with respect to their features, e.g. pronunciation, is not new, the simultaneous identification of the features which give rise to the geographical clustering presents novel opportunities in dialectometry. Earlier methods aggregated sound differences and clustered on the basis of aggregate differences. The determination of the significant features which co-vary with cluster membership was carried out on a post hoc basis. Bipartite spectral graph clustering simultaneously seeks groups of individual features which are strongly associated, even while seeking groups of sites which share subsets of these same features. We show that the application of this method results in clear and sensible geographical groupings and discuss and analyze the importance of the concomitant features.","['Martijn Wieling', 'John Nerbonne']",July 2011,Computer Speech & Language,"['Bipartite spectral graph partitioning', 'Clustering', 'Sound correspondences', 'Dialectometry', 'Dialectology', 'Language variation']",Bipartite spectral graph partitioning for clustering dialect varieties and detecting their linguistic features☆
558,"In this article, we test a variant of the Sapir-Whorf Hypothesis in the area of complex network theory. This is done by analyzing social ontologies as a new resource for automatic language classification. Our method is to solely explore structural features of social ontologies in order to predict family resemblances of languages used by the corresponding communities to build these ontologies. This approach is based on a reformulation of the Sapir-Whorf Hypothesis in terms of distributed cognition. Starting from a corpus of 160 Wikipedia-based social ontologies, we test our variant of the Sapir-Whorf Hypothesis by several experiments, and find out that we outperform the corresponding baselines. All in all, the article develops an approach to classify linguistic networks of tens of thousands of vertices by exploring a small range of mathematically well-established topological indices.","['Alexander Mehler', 'Olga Pustylnikov', 'Nils Diewald']",July 2011,Computer Speech & Language,"['Sapir-Whorf Hypothesis', 'Linguistic networks', 'Automatic language classification', 'Social ontologies', 'Quantitative network analysis']",Geography of social ontologies: Testing a variant of the Sapir-Whorf Hypothesis in the context of Wikipedia
559,,[],April 2011,Computer Speech & Language,[],Editorial
560,,"['Yorick Wilks', 'Roberta Catizone', 'Simon Worgan', 'Markku Turunen']",April 2011,Computer Speech & Language,"['Dialogue systems', 'Human–computer interaction', 'Dialogue management', 'Dialogue architectures', 'Emotion detection']",ReviewSome background on dialogue management and conversational speech for dialogue systems
561,"This paper describes an initial prototype of the Companions project (www.companions-project.org): the Senior Companion (SC), designed to be a platform to display novel approaches to:(1)The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an ASR phase.(2)The conversion of the input to RDF form to allow the generation of new facts from existing ones, under the control of a Dialogue Manager (DM), that also has access to stored knowledge and knowledge accessed in real time from the web, all in RDF form.(3)A DM expressed as a stack and network virtual machine that models mixed initiative in dialogue control.(4)A tuned dialogue act detector based on corpus evidence.The prototype platform was evaluated, and we describe this; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning. We describe preliminary studies and results for these, in particular a novel approach to enabling reinforcement learning for open dialogue systems through the detection of emotion in the speech signal and its deployment as a form of a learned DM, at a higher level than the DM virtual machine and able to direct the SC's responses to a more emotionally appropriate part of its repertoire.","['Yorick Wilks', 'Roberta Catizone', 'Simon Worgan', 'Alexiei Dingli', 'Roger Moore', 'Debora Field', 'Weiwei Cheng']",April 2011,Computer Speech & Language,"['Dialogue', 'Human–computer interaction', 'Dialogue management', 'ASR and emotion']",A prototype for a conversational companion for reminiscing about images
562,"There are several factors that influence communicative behavior, such as gender, personality or culture. As virtual agents interact in a more and more human-like manner, their behavior should be dependent on social factors as well. Culture is a phenomenon that affects one’s behavior without one realizing it. Behavior is thus sometimes perceived as inappropriate because there is no awareness of the cultural gap. Thus, we think cultural background should also influence the communication behavior of virtual agents. Behavioral differences are sometimes easy to recognize by humans but still hard to describe formally, to enable integration into a system that automatically generates culture-specific behavior. In our work, we focus on culture-related differences in the domain of casual Small Talk. Our model of culture-related differences in Small Talk behavior is based on findings described in the literature as well as on a video corpus that was recorded in Germany and Japan. In a validation study, we provide initial evidence that our simulation of culture-specific Small Talk with virtual agents is perceived differently by human observers. We thus implemented a system that automatically generates culture-specific Small Talk dialogs for virtual agents.","['Birgit Endrass', 'Matthias Rehm', 'Elisabeth André']",April 2011,Computer Speech & Language,"['Virtual agents', 'Culture', 'Small Talk', 'Behavior planning', 'Language generation']",Planning Small Talk behavior with cultural influences for multiagent systems
563,"A common task for spoken dialog systems (SDS) is to help users select a suitable option (e.g., flight, hotel, and restaurant) from the set of options available. As the number of options increases, the system must have strategies for generating summaries that enable the user to browse the option space efficiently and successfully. In the user-model based summarize and refine approach (UMSR, Demberg and Moore, 2006), options are clustered to maximize utility with respect to a user model, and linguistic devices such as discourse cues and adverbials are used to highlight the trade-offs among the presented items. In a Wizard-of-Oz experiment, we show that the UMSR approach leads to improvements in task success, efficiency, and user satisfaction compared to an approach that clusters the available options to maximize coverage of the domain (Polifroni et al., 2003). In both a laboratory experiment and a web-based experimental paradigm employing the Amazon Mechanical Turk platform, we show that the discourse cues in UMSR summaries help users compare different options and choose between options, even though they do not improve verbatim recall. This effect was observed for both written and spoken stimuli.","['Andi K. Winterboer', 'Martin I. Tietze', 'Maria K. Wolters', 'Johanna D. Moore']",April 2011,Computer Speech & Language,"['Information presentation', 'Spoken dialog systems', 'User modeling', 'Discourse markers']",The user model-based summarize and refine approach improves information presentation in spoken dialog systems
564,"Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and fitness. This paper describes how such multimodal conversational Companions can be implemented to support their owners in various pervasive and mobile settings. We present concrete system architectures, virtual, physical and mobile multimodal interfaces, and interaction management techniques for such Companions. In particular how knowledge representation and separation of low-level interaction modelling from high-level reasoning at the domain level makes it possible to implement distributed, but still coherent, interaction with Companions. The distribution is enabled by using a dialogue plan to communicate information from domain level planner to dialogue management and from there to a separate mobile interface. The model enables each part of the system to handle the same information from its own perspective without containing overlapping logic, and makes it possible to separate task-specific and conversational dialogue management from each other. In addition to technical descriptions, results from the first evaluations of the Companions interfaces are presented.","['Markku Turunen', 'Jaakko Hakulinen', 'Olov Ståhl', 'Björn Gambäck', 'Preben Hansen', 'Mari C. Rodríguez Gancedo', 'Raúl Santos de la Cámara', 'Cameron Smith', 'Daniel Charlton', 'Marc Cavazza']",April 2011,Computer Speech & Language,"['Companions', 'Embodied conversational agents', 'Conversational spoken dialogue systems', 'Mobile interfaces', 'Cognitive modelling', 'Dialogue management']",Multimodal and mobile conversational Health and Fitness Companions
565,"This paper argues that the problems of dialogue management (DM) and Natural Language Generation (NLG) in dialogue systems are closely related and can be fruitfully treated statistically, in a joint optimisation framework such as that provided by Reinforcement Learning (RL). We first review recent results and methods in automatic learning of dialogue management strategies for spoken and multimodal dialogue systems, and then show how these techniques can also be used for the related problem of Natural Language Generation. This approach promises a number of theoretical and practical benefits such as fine-grained adaptation, generalisation, and automatic (global) optimisation, and we compare it to related work in statistical/trainable NLG. A demonstration of the proposed approach is then developed, showing combined DM and NLG policy learning for adaptive information presentation decisions. A joint DM and NLG policy learned in the framework shows a statistically significant 27% relative increase in reward over a baseline policy, which is learned in the same way only without the joint optimisation. We thereby show that that NLG problems can be approached statistically, in combination with dialogue management decisions, and we show how to jointly optimise NLG and DM using Reinforcement Learning.",['Oliver Lemon'],April 2011,Computer Speech & Language,"['Dialogue systems', 'Natural Language Generation', 'Reinforcement Learning']",Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation
566,"This paper studies the multifunctionality of dialogue utterances, i.e. the phenomenon that utterances in dialogue often have more than one communicative function. It is argued that this phenomenon can be explained by analyzing the participation in dialogue as involving the performance of several types of activity in parallel, relating to different dimensions of communication. The multifunctionality of dialogue utterances is studied by (1) redefining the notion of ‘utterance’ in a rigorous manner (calling the revised notion ‘functional segment’), and (2) empirically investigating the multifunctionality of functional segments in a corpus of dialogues, annotated with a rich, multidimensional annotation schema. It is shown that, when communicative functions are assigned to functional segments, thereby eliminating every form of segmentation-related multifunctionality, an average multifunctionality is found between 1.8 and 3.6, depending on what is considered to count as a segment's communicative function. Moreover, a good understanding of the nature of the relations among the various multiple functions that a segment may have, and of the relations between functional segments and other units in dialogue segmentation, opens the way for defining a multidimensional computational update semantics for dialogue interpretation.",['Harry Bunt'],April 2011,Computer Speech & Language,"['Multifunctionality', 'Dialogue acts', 'Communicative function', 'Dialogue segmentation', 'Functional segment', 'Dynamic Interpretation Theory']",Multifunctionality in dialogue
567,"This paper presents an automated scoring system which grades students’ English writing tests. The system provides a score and diagnostic feedback to students without human’s efforts. Target users are Korean students in junior high schools who learn English as a second language. The system takes a single English sentence as its input. Dealing with a single sentence as an input has some advantages on comparing the input with the answers given by human teachers and giving detailed feedback to the students. The system was developed and tested with the real test data collected through English tests given to third grade students in junior high school. Scoring requires two steps of the process. The first process is analyzing the input sentence in order to detect possible errors, such as spelling errors and syntactic errors. The second process is comparing the input sentence with given answers to identify the differences as errors. To evaluate the performance of the system, the output produced by the system is compared with the result provided by human raters. The score agreement value between a human rater and the system is quite close to the value between two human raters.","['Kong Joo Lee', 'Yong-Seok Choi', 'Jee Eun Kim']",April 2011,Computer Speech & Language,"['Automated scoring system', 'Natural language analysis', 'English sentences', 'Human rater', 'Error production rules', 'Intra-sentential error', 'Inter-sentential error']",Building an automated English sentence evaluation system for students learning English as a second language
568,"Ranking functions are an important component of information retrieval systems. Recently there has been a surge of research in the field of “learning to rank”, which aims at using labeled training data and machine learning algorithms to construct reliable ranking functions. Machine learning methods such as neural networks, support vector machines, and least squares have been successfully applied to ranking problems, and some are already being deployed in commercial search engines.Despite these successes, most algorithms to date construct ranking functions in a supervised learning setting, which assume that relevance labels are provided by human annotators prior to training the ranking function. Such methods may perform poorly when human relevance judgments are not available for a wide range of queries. In this paper, we examine whether additional unlabeled data, which is easy to obtain, can be used to improve supervised algorithms. In particular, we investigate the transductive setting, where the unlabeled data is equivalent to the test data.We propose a simple yet flexible transductive meta-algorithm: the key idea is to adapt the training procedure to each test list after observing the documents that need to be ranked. We investigate two instantiations of this general framework: The Feature Generation approach is based on discovering more salient features from the unlabeled test data and training a ranker on this test-dependent feature-set. The importance weighting approach is based on ideas in the domain adaptation literature, and works by re-weighting the training data to match the statistics of each test list. We demonstrate that both approaches improve over supervised algorithms on the TREC and OHSUMED tasks from the LETOR dataset.","['Kevin Duh', 'Katrin Kirchhoff']",April 2011,Computer Speech & Language,"['Information retrieval', 'Ranking algorithms', 'Semi-supervised Learning']",Semi-supervised ranking for document retrieval
569,"This paper presents a description and evaluation of SpeechRaterSM, a system for automated scoring of non-native speakers’ spoken English proficiency, based on tasks which elicit spontaneous monologues on particular topics. This system builds on much previous work in the automated scoring of test responses, but differs from previous work in that the highly unpredictable nature of the responses to this task type makes the challenge of accurate scoring much more difficult.SpeechRater uses a three-stage architecture. Responses are first processed by a filtering model to ensure that no exceptional conditions exist which might prevent them from being scored by SpeechRater. Responses not filtered out at this stage are then processed by the scoring model to estimate the proficiency rating which a human might assign to them, on the basis of features related to fluency, pronunciation, vocabulary diversity, and grammar. Finally, an aggregation model combines an examinee’s scores for multiple items to calculate a total score, as well as an interval in which the examinee’s score is predicted to reside with high confidence.SpeechRater’s current level of accuracy and construct representation have been deemed sufficient for low-stakes practice exercises, and it has been used in a practice exam for the TOEFL since late 2006. In such a practice environment, it offers a number of advantages compared to human raters, including system load management, and the facilitation of immediate feedback to students. However, it must be acknowledged that SpeechRater presently fails to measure many important aspects of speaking proficiency (such as intonation and appropriateness of topic development), and its agreement with human ratings of proficiency does not yet approach the level of agreement between two human raters.","['Derrick Higgins', 'Xiaoming Xi', 'Klaus Zechner', 'David Williamson']",April 2011,Computer Speech & Language,"['Language testing', 'English speaking proficiency', 'Automated scoring', 'Constructed response scoring', 'Speech recognition']",A three-stage approach to the automated scoring of spontaneous spoken responses
570,"This paper proposes a novel user intention simulation method which is data-driven but can integrate diverse user discourse knowledge to simulate various types of user behaviors. A method of data-driven user intention modeling based on logistic regression is introduced in the Markov logic framework. Human dialog knowledge is designed into two layers, domain and discourse knowledge, and integrated with the data-driven model in generation time. Three types of user knowledge, i.e., cooperative, corrective and self-directing, are designed and integrated to generate behaviors of corresponding user-types. In experiments to investigate the patterns of simulated users, the approach successfully generated cooperative, corrective and self-directing user intention patterns.","['Sangkeun Jung', 'Cheongjae Lee', 'Kyungduk Kim', 'Donghyeon Lee', 'Gary Geunbae Lee']",April 2011,Computer Speech & Language,"['User simulation', 'Dialog simulation', 'User intention simulation', 'Data-driven', 'Hybrid approach', 'Markov logic', 'Spoken dialog system', 'Dialog system']",Hybrid user intention modeling to diversify dialog simulations
571,"This paper presents an extended study on the implementation of support vector machine (SVM) based speaker verification in systems that employ continuous progressive model adaptation using the weight-based factor analysis model. The weight-based factor analysis model compensates for session variations in unsupervised scenarios by incorporating trial confidence measures in the general statistics used in the inter-session variability modelling process. Employing weight-based factor analysis in Gaussian mixture models (GMMs) was recently found to provide significant performance gains to unsupervised classification. Further improvements in performance were found through the integration of SVM-based classification in the system by means of GMM supervectors.This study focuses particularly on the way in which a client is represented in the SVM kernel space using single and multiple target supervectors. Experimental results indicate that training client SVMs using a single target supervector maximises performance while exhibiting a certain robustness to the inclusion of impostor training data in the model. Furthermore, the inclusion of low-scoring target trials in the adaptation process is investigated where they were found to significantly aid performance.","['Mitchell McLaren', 'Driss Matrouf', 'Robbie Vogt', 'Jean-Francois Bonastre']",April 2011,Computer Speech & Language,"['Speaker verification', 'Factor analysis', 'Gaussian mixture model (GMM)', 'Support vector machine (SVM)', 'Unsupervised adaptation']",Applying SVMs and weight-based factor analysis to unsupervised adaptation for speaker verification☆
572,"This paper describes a comprehensive usability evaluation of an automated telephone banking system which employs text-to-speech (TTS) synthesis in offering additional detail on customers’ account transactions. The paper describes a series of four experiments in which TTS was employed to offer an extra level of detail to recent transactions listings within an established banking service which otherwise uses recorded speech from a professional recording artist. Results from the experiments show that participants welcome the added value of TTS in being able to provide additional detail on their account transactions, but that TTS should be used minimally in the service.","['Hazel Morton', 'Nancie Gunson', 'Diarmid Marshall', 'Fergus McInnes', 'Andrea Ayres', 'Mervyn Jack']",April 2011,Computer Speech & Language,"['Text-to-speech (TTS)', 'Usability', 'Dialogue design', 'Automated telephony']",Usability assessment of text-to-speech synthesis for additional detail in an automated telephone banking system
573,"Automatic speech recognition (ASR) has become a valuable tool in large document production environments like medical dictation. While manual post-processing is still needed for correcting speech recognition errors and for creating documents which adhere to various stylistic and formatting conventions, a large part of the document production process is carried out by the ASR system. For improving the quality of the system output, knowledge about the multi-layered relationship between the dictated texts and the final documents is required. Thus, typical speech-recognition errors can be avoided, and proper style and formatting can be anticipated in the ASR part of the document production process. Yet – while vast amounts of recognition results and manually edited final reports are constantly being produced – the error-free literal transcripts of the actually dictated texts are a scarce and costly resource because they have to be created by manually transcribing the audio files.To obtain large corpora of literal transcripts for medical dictation, we propose a method for automatically reconstructing them from draft speech-recognition transcripts plus the corresponding final medical reports. The main innovative aspect of our method is the combination of two independent knowledge sources: phonetic information for the identification of speech-recognition errors and semantic information for detecting post-editing concerning format and style. Speech recognition results and final reports are first aligned, then properly matched based on semantic and phonetic similarity, and finally categorised and selectively combined into a reconstruction hypothesis. This method can be used for various applications in language technology, e.g., adaptation for ASR, document production, or generally for the development of parallel text corpora of non-literal text resources. In an experimental evaluation, which also includes an assessment of the quality of the reconstructed transcripts compared to manual transcriptions, the described method results in a relative word error rate reduction of 7.74% after retraining the standard language model with reconstructed transcripts.","['Stefan Petrik', 'Christina Drexel', 'Leo Fessler', 'Jeremy Jancsary', 'Alexandra Klein', 'Gernot Kubin', 'Johannes Matiasek', 'Franz Pernkopf', 'Harald Trost']",April 2011,Computer Speech & Language,"['Automatic transcription', 'Semantics', 'Phonetics', 'Automatic speech recognition', 'Dictation']",Semantic and phonetic automatic reconstruction of medical dictations
574,"Sentiment classification is used to identify whether the opinion expressed in a document is positive or negative. In this paper, we present an approach to do documentary-level sentiment classification by modeling description of topical terms. The motivation of this work stems from the observation that the global document classification will benefit greatly by examining the way of a topical term to give opinion in its local sentence context. Two sentence-level sentiment description models, namely positive and negative Topical Term Description Models, are constructed for each topical term. When analyzing a document, the Topical Term Description Models generate divergence to support the classification of its sentiment at the sentence-level which in turn can be used to decide the whole document classification collectively. The results of the experiments prove that our proposed method is effective. It is also shown that our results are comparable to the state-of-art results on a publicly available movie review corpus and a Chinese digital product review corpus. This is quite encouraging to us and motivates us to have further investigation on the development of a more effective topical term related description model in the future.","['Yi Hu', 'Wenjie Li']",April 2011,Computer Speech & Language,"['Sentiment classification', 'Topical term', 'Topical Term Description Model', 'Maximum spanning tree']",Document sentiment classification by exploring description model of topical terms
575,"We describe a new approach to speech recognition, in which all Hidden Markov Model (HMM) states share the same Gaussian Mixture Model (GMM) structure with the same number of Gaussians in each state. The model is defined by vectors associated with each state with a dimension of, say, 50, together with a global mapping from this vector space to the space of parameters of the GMM. This model appears to give better results than a conventional model, and the extra structure offers many new opportunities for modeling innovations while maintaining compatibility with most standard techniques.","['Daniel Povey', 'Lukáš Burget', 'Mohit Agarwal', 'Pinar Akyazi', 'Feng Kai', 'Arnab Ghoshal', 'Ondřej Glembek', 'Nagendra Goel', 'Martin Karafiát', 'Ariya Rastrow', 'Richard C. Rose', 'Petr Schwarz', 'Samuel Thomas']",April 2011,Computer Speech & Language,"['Speech recognition', 'Gaussian Mixture Model', 'Subspace Gaussian Mixture Model']",The subspace Gaussian mixture model—A structured model for speech recognition
576,"In a real environment, acoustic and language features often vary depending on the speakers, speaking styles and topic changes. To accommodate these changes, speech recognition approaches that include the incremental tracking of changing environments have attracted attention. This paper proposes a topic tracking language model that can adaptively track changes in topics based on current text information and previously estimated topic models in an on-line manner. The proposed model is applied to language model adaptation in speech recognition. We use the MIT OpenCourseWare corpus and Corpus of Spontaneous Japanese in speech recognition experiments, and show the effectiveness of the proposed method.","['Shinji Watanabe', 'Tomoharu Iwata', 'Takaaki Hori', 'Atsushi Sako', 'Yasuo Ariki']",April 2011,Computer Speech & Language,"['Language model', 'Latent topic model', 'Topic tracking', 'On-line algorithm', 'Speech recognition']",Topic tracking language model for speech recognition
577,"An effective way to increase noise robustness in automatic speech recognition is to label the noisy speech features as either reliable or unreliable (‘missing’), and replace (‘impute’) the missing ones by clean speech estimates. Conventional imputation techniques employ parametric models and impute the missing features on a frame-by-frame basis. At low SNRs, frame-based imputation techniques fail because many time frames contain few, if any, reliable features. In previous work, we introduced an exemplar-based method, dubbed sparse imputation, which can impute missing features using reliable features from neighbouring frames. We achieved substantial gains in performance at low SNRs for a connected digit recognition task. In this work, we investigate whether the exemplar-based approach can be generalised to a large vocabulary task.Experiments on artificially corrupted speech show that sparse imputation substantially outperforms a conventional imputation technique when the ideal ‘oracle’ reliability of features is used. With error-prone estimates of feature reliability, sparse imputation performance is comparable to our baseline imputation technique in the cleanest conditions, and substantially better at lower SNRs. With noisy speech recorded in realistic noise conditions, sparse imputation performs slightly worse than our baseline imputation technique in the cleanest conditions, but substantially better in the noisier conditions.","['Jort Florent Gemmeke', 'Bert Cranen', 'Ulpu Remes']",April 2011,Computer Speech & Language,"['Missing data techniques', 'Noise robustness', 'Automatic speech recognition', 'Sparse imputation']",Sparse imputation for large vocabulary noise robust ASR
578,,"['Laurence Devillers', 'Nick Campbell']",January 2011,Computer Speech & Language,[],EditorialSpecial issue of computer speech and language on “affective speech in real-life interactions”
579,"In this article, we describe and interpret a set of acoustic and linguistic features that characterise emotional/emotion-related user states – confined to the one database processed: four classes in a German corpus of children interacting with a pet robot. To this end, we collected a very large feature vector consisting of more than 4000 features extracted at different sites. We performed extensive feature selection (Sequential Forward Floating Search) for seven acoustic and four linguistic types of features, ending up in a small number of ‘most important’ features which we try to interpret by discussing the impact of different feature and extraction types. We establish different measures of impact and discuss the mutual influence of acoustics and linguistics.","['Anton Batliner', 'Stefan Steidl', 'Björn Schuller', 'Dino Seppi', 'Thurid Vogt', 'Johannes Wagner', 'Laurence Devillers', 'Laurence Vidrascu', 'Vered Aharonson', 'Loic Kessous', 'Noam Amir']",January 2011,Computer Speech & Language,"['Feature types', 'Feature selection', 'Automatic classification', 'Emotion']",Whodunnit – Searching for the most important feature types signalling emotion-related user states in speech
580,"The automatic recognition of user’s communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. For dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. This paper addresses the problem of automatically detecting “frustration”, “politeness”, and “neutral” attitudes from a child’s speech communication cues, elicited in spontaneous dialog interactions with computer characters. Several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. The study is based on a Wizard-of-Oz dialog corpus of 103 children, 7–14 years of age, playing a voice activated computer game. Three-way classification experiments, as well as, pairwise classification between polite vs. others and frustrated vs. others were performed. Experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness, whereas context and acoustic features perform best for frustration detection. Furthermore, the fusion of acoustic, lexical and contextual information provided significantly better classification results. Results also showed that classification performance varies with age and gender. Specifically, for the “politeness” detection task, higher classification accuracy was achieved for females and 10–11 years-olds, compared to males and other age groups, respectively.","['Serdar Yildirim', 'Shrikanth Narayanan', 'Alexandros Potamianos']",January 2011,Computer Speech & Language,"['Emotion recognition', 'Spoken dialog systems', 'Children speech', 'Spontaneous speech', 'Natural emotions', 'Child–computer interaction', 'Feature extraction']",Detecting emotional state of a child in a conversational computer game☆
581,"Affective states and their non-verbal expressions are an important aspect of human reasoning, communication and social life. Automated recognition of affective states can be integrated into a wide variety of applications for various fields. Therefore, it is of interest to design systems that can infer the affective states of speakers from the non-verbal expressions in speech, occurring in real scenarios. This paper presents such a system and the framework for its design and validation. The framework defines a representation method that comprises a set of affective-state groups or archetypes that often appear in everyday life. The inference system is designed to infer combinations of affective states that can occur simultaneously and whose level of expression can change over time. The framework considers also the validation and generalisation of the system. The system was built of 36 independent pair-wise comparison machines, with average accuracy (tenfold cross-validation) of 75%. The accumulated inference system yielded total accuracy of 83% and recognised combinations for different nuances within the affective-state groups. In addition to the ability to recognise these affective-state groups, the inference system was applied to characterisation of a very large variety of affective state concepts (549 concepts) as combinations of the affective-state groups. The system was also applied to annotation of affective states that were naturally evoked during sustained human–computer interactions and multi-modal analysis of the interactions, to new speakers and to a different language, with no additional training. The system provides a powerful tool for recognition, characterisation, annotation (interpretation) and analysis of affective states. In addition, the results inferred from speech in both English and Hebrew, indicate that the vocal expressions of complex affective states such as thinking, certainty and interest transcend language boundaries.",['Tal Sobol-Shikler'],January 2011,Computer Speech & Language,"['Affective computing', 'Affect recognition', 'Cognition', 'Emotions', 'Human perception', 'Intelligent systems', 'Machine learning', 'Multi-label inference', 'Multi-modal analysis', 'Multi-modal database', 'Speech analysis', 'Speech corpora']",Automatic inference of complex affective states
582,"The present paper aims at filling the lack that currently exists with respect to databases containing emotional manifestations. Emotions, such as strong emotions, are indeed difficult to collect in real-life. They occur during contexts, which are generally unpredictable, and some of them such as anger are less frequent in public life than in private. Even though such emotions are not so present in existing databases, the need for applications, which target them (crisis management, surveillance, strategic intelligence, etc.), and the need for emotional recordings is even more acute. We propose here to use fictional media to compensate for the difficulty of collecting strong emotions. Emotions in realistic fictions are portrayed by skilled actors in interpersonal interactions. The mise-en-scene of the actors tends to stir genuine emotions. In addition, fiction offers an overall view of emotional manifestations in various real-life contexts: face-to-face interactions, phone calls, interviews, emotional event reporting vs. in situ emotional manifestations. A fear-type emotion recognition system has been developed, that is based on acoustic models learnt from the fiction corpus. This paper aims at providing an in-depth analysis of the various factors that may influence the system behaviour: the annotation issue and the acoustic features behaviour. These two aspects emphasize the main feature of fiction: the variety of the emotional manifestations and of their context.","['C. Clavel', 'I. Vasilescu', 'L. Devillers']",January 2011,Computer Speech & Language,"['Fiction corpus', 'Affective speech', 'Multi-level annotation scheme', 'Acostic features', 'Unvoiced speech']",Fiction support for realistic portrayals of fear-type emotional manifestations
583,"The majority of previous studies on vocal expression have been conducted on posed expressions. In contrast, we utilized a large corpus of authentic affective speech recorded from real-life voice controlled telephone services. Listeners rated a selection of 200 utterances from this corpus with regard to level of perceived irritation, resignation, neutrality, and emotion intensity. The selected utterances came from 64 different speakers who each provided both neutral and affective stimuli. All utterances were further automatically analyzed regarding a comprehensive set of acoustic measures related to F0, intensity, formants, voice source, and temporal characteristics of speech. Results first showed that several significant acoustic differences were found between utterances classified as neutral and utterances classified as irritated or resigned using a within-persons design. Second, listeners’ ratings on each scale were associated with several acoustic measures. In general the acoustic correlates of irritation, resignation, and emotion intensity were similar to previous findings obtained with posed expressions, though the effect sizes were smaller for the authentic expressions. Third, automatic classification (using LDA classifiers both with and without speaker adaptation) of irritation, resignation, and neutral performed at a level comparable to human performance, though human listeners and machines did not necessarily classify individual utterances similarly. Fourth, clearly perceived exemplars of irritation and resignation were rare in our corpus. These findings were discussed in relation to future research.","['Petri Laukka', 'Daniel Neiberg', 'Mimmi Forsell', 'Inger Karlsson', 'Kjell Elenius']",January 2011,Computer Speech & Language,"['Acoustic features', 'Automatic speech classification', 'Emotion recognition', 'Human–computer interaction', 'Spontaneous speech']",Expression of affect in spontaneous speech: Acoustic correlates and automatic detection of irritation and resignation
584,"We describe the design and evaluation of two different dynamic student uncertainty adaptations in wizarded versions of a spoken dialogue tutoring system. The two adaptive systems adapt to each student turn based on its uncertainty, after an unseen human “wizard” performs speech recognition and natural language understanding and annotates the turn for uncertainty. The design of our two uncertainty adaptations is based on a hypothesis in the literature that uncertainty is an “opportunity to learn”; both adaptations use additional substantive content to respond to uncertain turns, but the two adaptations vary in the complexity of these responses. The evaluation of our two uncertainty adaptations represents one of the first controlled experiments to investigate whether substantive dynamic responses to student affect can significantly improve performance in computer tutors. To our knowledge we are the first study to show that dynamically responding to uncertainty can significantly improve learning during computer tutoring. We also highlight our ongoing evaluation of our uncertainty-adaptive systems with respect to other important performance metrics, and we discuss how our corpus can be used by the wider computer speech and language community as a linguistic resource supporting further research on effective affect-adaptive spoken dialogue systems in general.","['Kate Forbes-Riley', 'Diane Litman']",January 2011,Computer Speech & Language,"['Affective states in spontaneous data', 'Spoken dialogue tutoring system', 'Automatic affect adaptation', 'Wizard of Oz experimental design', 'Collection and annotation of realistic, representative, and publicly']",Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system
585,"A real-time trained system for robust speaker verification is proposed. This system was developed using a relative space of reference speakers, also referred to as anchor models. The real-time training aspect of the system is based on this relative space’s intriguing features and properties. The relative space concept uses relative speaker representation rather than an absolute representation, by comparing the speaker to a set of well-trained reference speakers. The advantage of this approach is that instead of estimating numerous parameters of an absolute model for a speaker, only a few parameters of a model relative to a number of anchor models are estimated. In order to optimize the performance of the proposed system, several techniques were assessed for possible implementation in various blocks of the system. As a result, the best performance was achieved where normalized vector’s mutual angle with the Minimum normalization method was applied to speaker verification in conjunction with an orthogonal relative space of virtual reference speakers. In this case, an Equal Error Rate (EER) of 0.12% on 400 test samples of 100 speakers was obtained. In addition to assessment under normal conditions, the developed speaker verification system was also evaluated under abnormal conditions where noisy or telephonic speech sequence contamination was present. Experiments conducted in this case demonstrated that, in most cases, this system outperforms absolute space based systems even with shortened training speech sequences. Another major contribution of this research is the development of a more complex speaker verification system capable of tackling abnormal conditions more effectively. In this case, other interesting features of the relative space approach were employed. For this purpose, a novel enrichment method was developed to construct a relative space of anchor models trained to tackle noise. The results of the experiments conducted in this part of the research demonstrated an excellent ability of this approach to tackle abnormal conditions. Compared to absolute space based system, applying this method in relative space led to lower error rates of speaker verification in all cases even with low SNR values.","['Ali Sadeghi Naini', 'M. Mehdi Homayounpour', 'Abbas Samani']",October 2010,Computer Speech & Language,"['Speaker verification', 'Robust', 'Noisy condition', 'Real-time training', 'Relative space', 'Absolute space', 'Anchor models', 'Reference speakers', 'Eigenspace', 'Normalization', 'Orthogonal']",A real-time trained system for robust speaker verification using relative space of anchor models
586,"This paper describes a statistically motivated framework for performing real-time dialogue state updates and policy learning in a spoken dialogue system. The framework is based on the partially observable Markov decision process (POMDP), which provides a well-founded, statistical model of spoken dialogue management. However, exact belief state updates in a POMDP model are computationally intractable so approximate methods must be used. This paper presents a tractable method based on the loopy belief propagation algorithm. Various simplifications are made, which improve the efficiency significantly compared to the original algorithm as well as compared to other POMDP-based dialogue state updating approaches. A second contribution of this paper is a method for learning in spoken dialogue systems which uses a component-based policy with the episodic Natural Actor Critic algorithm.The framework proposed in this paper was tested on both simulations and in a user trial. Both indicated that using Bayesian updates of the dialogue state significantly outperforms traditional definitions of the dialogue state. Policy learning worked effectively and the learned policy outperformed all others on simulations. In user trials the learned policy was also competitive, although its optimality was less conclusive. Overall, the Bayesian update of dialogue state framework was shown to be a feasible and effective approach to building real-world POMDP-based dialogue systems.","['Blaise Thomson', 'Steve Young']",October 2010,Computer Speech & Language,"['Dialogue systems', 'Robustness', 'POMDP', 'Reinforcement learning']",Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems
587,"Recently, discriminative training (DT) methods have achieved tremendous progress in automatic speech recognition (ASR). In this survey article, all mainstream DT methods in speech recognition are reviewed from both theoretical and practical perspectives. From the theoretical aspect, many effective discriminative learning criteria in ASR are first introduced and then a unifying view is presented to elucidate the relationship among these popular DT criteria originally proposed from different viewpoints. Next, some key optimization methods used to optimize these criteria are summarized and their convergence properties are discussed. Moreover, as some recent advances, a novel discriminative learning framework is introduced as a general scheme to formulate discriminative training of HMMs for ASR, from which a variety of new DT methods can be developed. In addition, some important implementation issues regarding how to conduct DT for large vocabulary ASR are also discussed from a more practical aspect, such as efficient implementation of discriminative training on word graphs and effective optimization of complex DT objective functions in high-dimensionality space, and so on. Finally, this paper is summarized and concluded with some possible future research directions for this area. As a technical survey, all DT techniques and ideas are reviewed and discussed in this paper from high level without involving too much technical detail and experimental result.",['Hui Jiang'],October 2010,Computer Speech & Language,[],Discriminative training of HMMs for automatic speech recognition: A survey
588,"Spoken dialog systems have difficulty selecting which action to take in a given situation because recognition and understanding errors are prevalent due to noise and unexpected inputs. To solve this problem, this paper presents a hybrid approach to improving robustness of the dialog manager by using agenda-based and example-based dialog modeling. This approach can exploit n-best hypotheses to determine the current dialog state in the dialog manager and keep track of the dialog state using a discourse interpretation algorithm based on an agenda graph and a focus stack. Given the agenda graph and multiple recognition hypotheses, the system can predict the next action to maximize multi-level score functions and trigger error recovery strategies to handle exceptional cases due to misunderstandings or unexpected focus shifts. The proposed method was tested by developing a spoken dialog system for a building guidance domain in an intelligent service robot. This system was then evaluated by simulated and real users. The experimental results show that our approach can effectively develop robust dialog management for spoken dialog systems.","['Cheongjae Lee', 'Sangkeun Jung', 'Kyungduk Kim', 'Gary Geunbae Lee']",October 2010,Computer Speech & Language,"['Example-based dialog modeling', 'Agenda-based dialog management', 'Robust dialog management', 'Error handling']",Hybrid approach to robust dialog management using agenda and dialog examples
589,"A gain factor adapted by both the intra-frame masking properties of the human auditory system and the inter-frame SNR variation is proposed to enhance a speech signal corrupted by additive noise. In this article we employ an averaging factor, varying with time–frequency, to improve the estimate of the a priori SNR. In turn, this SNR estimate is utilized to adapt a gain factor for speech enhancement. This gain factor reduces the spectral variation over successive frames, so the effect of musical residual noise is mitigated. In addition, the simultaneous masking property of the human ears is also employed to adapt the gain factor. Imperceptive residual noise with energy below the noise masking threshold is retained, resulting in a reduction of speech distortion. Experimental results show that the proposed scheme can efficiently reduce the effect of musical residual noise.","['Ching-Ta Lu', 'Kun-Fu Tseng']",October 2010,Computer Speech & Language,"['Speech enhancement', 'Perceptual', 'SNR variation', 'Spectral subtraction', 'Colored noise']",A gain factor adapted by masking property and SNR variation for speech enhancement in colored-noise corruptions
590,"Discriminative classifiers are a popular approach to solving classification problems. However, one of the problems with these approaches, in particular kernel based classifiers such as support vector machines (SVMs), is that they are hard to adapt to mismatches between the training and test data. This paper describes a scheme for overcoming this problem for speech recognition in noise by adapting the kernel rather than the SVM decision boundary. Generative kernels, defined using generative models, are one type of kernel that allows SVMs to handle sequence data. By compensating the parameters of the generative models for each noise condition noise-specific generative kernels can be obtained. These can be used to train a noise-independent SVM on a range of noise conditions, which can then be used with a test-set noise kernel for classification. The noise-specific kernels used in this paper are based on Vector Taylor Series (VTS) model-based compensation. VTS allows all the model parameters to be compensated and the background noise to be estimated in a maximum likelihood fashion. A brief discussion of VTS, and the optimisation of the mismatch function representing the impact of noise on the clean speech, is also included. Experiments using these VTS-based test-set noise kernels were run on the AURORA 2 continuous digit task. The proposed SVM rescoring scheme yields large gains in performance over the VTS compensated models.","['M.J.F. Gales', 'F. Flego']",October 2010,Computer Speech & Language,"['Speech recognition', 'Noise robustness', 'Support vector machines', 'Generative kernels']",Discriminative classifiers with adaptive kernels for noise robust speech recognition
591,"Many automatic speech recognition (ASR) systems rely on the sole pronunciation dictionaries and language models to take into account information about language. Implicitly, morphology and syntax are to a certain extent embedded in the language models but the richness of such linguistic knowledge is not exploited. This paper studies the use of morpho-syntactic (MS) information in a post-processing stage of an ASR system, by reordering N-best lists. Each sentence hypothesis is first part-of-speech tagged. A morpho-syntactic score is computed over the tag sequence with a long-span language model and combined to the acoustic and word-level language model scores. This new sentence-level score is finally used to rescore N-best lists by reranking or consensus. Experiments on a French broadcast news task show that morpho-syntactic knowledge improves the word error rate and confidence measures. In particular, it was observed that the errors corrected are not only agreement errors and errors on short grammatical words but also other errors on lexical words where the hypothesized lemma was modified.","['Stéphane Huet', 'Guillaume Gravier', 'Pascale Sébillot']",October 2010,Computer Speech & Language,"['Speech recognition', 'Morpho-syntax', 'Tagging', 'Confidence measure']",Morpho-syntactic post-processing of N-best lists for improved French automatic speech recognition☆
592,"We present graphical model based methodology that enhances a speech recognizer with information about syllabic segmentations. The segmentations are specified by locations of syllable nuclei, and the graphical models are able to consider these locations as “soft” information. The graphs give improved discrimination between speech and noise when compared to a baseline model. When using locations derived from oracle information an overall improvement is shown, and when the oracle syllable nuclei are augmented with information about lexical stress the methods give additional improvements over locations alone.","['Chris D. Bartels', 'Jeff A. Bilmes']",October 2010,Computer Speech & Language,"['Speech recognition', 'Graphical models', 'Dynamic Bayesian networks', 'Syllables']",Graphical models for integrating syllabic information
593,"Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp–Rabin hash families are not pairwise independent.","['Daniel Lemire', 'Owen Kaser']",October 2010,Computer Speech & Language,"['Rolling hashing', 'Rabin–Karp hashing', 'Hashing strings']","Recursive n-gram hashing is pairwise independent, at best"
594,"This paper provides an introduction to the acoustic–phonetic structure of English regional accents and presents a signal processing method for the modeling and transformation of the acoustic correlates of English accents for example from British English to American English. The focus of this paper is on the modeling of intonation and duration correlates of accents as the modeling of formants is described in previous papers (Yan et al., 2007, Vaseghi et al., 2009). The intonation correlates of accents are modeled with the statistics of a set of broad features of the pitch contour. The statistical models of phoneme durations and word speaking rates are obtained from automatic segmentation of word/phoneme boundaries of speech databases. A contribution of this paper is the use of accent synthesis for comparative evaluation of the causal effects of the acoustic correlates of accent. The differences between the acoustics–phonetic realizations of British Received Pronunciation (RP), Broad Australian (BAU) and General American (GenAm) English accents are modeled and used in an accent transformation and synthesis method for evaluation of the influence of formant, pitch and duration on conveying accents.","['Qin Yan', 'Saeed Vaseghi']",October 2010,Computer Speech & Language,"['Accent', 'Formant', 'HMMs', 'Pitch', 'Intonation', 'Duration']",Modeling and synthesis of English regional accents with pitch and duration correlates
595,"Weblogs are increasingly popular modes of communication and they are frequently used as mediums for emotional expression in the ever changing online world. This work uses blogs as object and data source for Chinese emotional expression analysis. First, a textual emotional expression space model is described, and based on this model, a relatively fine-grained annotation scheme is proposed for manual annotation of an emotion corpus. In document and paragraph levels, emotion category, emotion intensity, topic word and topic sentence are annotated. In sentence level, emotion category, emotion intensity, emotional keyword and phrase, degree word, negative word, conjunction, rhetoric, punctuation, objective or subjective, and emotion polarity are annotated. Then, using this corpus, we explore these linguistic expressions that indicate emotion in Chinese, and present a detailed data analysis on them, involving mixed emotions, independent emotion, emotion transfer, and analysis on words and rhetorics for emotional expression.","['Changqin Quan', 'Fuji Ren']",October 2010,Computer Speech & Language,"['Emotion analysis', 'Weblogs', 'Corpus annotation', 'Natural language processing']",A blog emotion corpus for emotional expression analysis in Chinese
596,"This paper reports on the work done on vocabulary and language model daily adaptation for a European Portuguese broadcast news transcription system. The proposed adaptation framework takes into consideration European Portuguese language characteristics, such as its high level of inflection and complex verbal system.A multi-pass speech recognition framework using contemporary written texts available daily on the Web is proposed. It uses morpho-syntactic knowledge (part-of-speech information) about an in-domain training corpus for daily selection of an optimal vocabulary. Using an information retrieval engine and the ASR hypotheses as query material, relevant documents are extracted from a dynamic and large-size dataset to generate a story-based language model. When applied to a daily and live closed-captioning system of live TV broadcasts, it was shown to be effective, with a relative reduction of out-of-vocabulary word rate (69%) and WER (12.0%) when compared to the results obtained by the baseline system with the same vocabulary size.","['Ciro Martins', 'António Teixeira', 'João Neto']",October 2010,Computer Speech & Language,"['Vocabulary selection', 'Language modeling', 'Information retrieval techniques', 'Automatic speech recognition (ASR)', 'Broadcast news transcription']",Dynamic language modeling for European Portuguese☆
597,,"['Rodrigo Capobianco Guido', 'José Carlos Pereira', 'Jan Frans Willem Slaets']",July 2010,Computer Speech & Language,[],EditorialEmergent artificial intelligence approaches for pattern recognition in speech and language processing
598,"We propose a unified global entropy reduction maximization (GERM) framework for active learning and semi-supervised learning for speech recognition. Active learning aims to select a limited subset of utterances for transcribing from a large amount of un-transcribed utterances, while semi-supervised learning addresses the problem of selecting right transcriptions for un-transcribed utterances, so that the accuracy of the automatic speech recognition system can be maximized. We show that both the traditional confidence-based active learning and semi-supervised learning approaches can be improved by maximizing the lattice entropy reduction over the whole dataset. We introduce our criterion and framework, show how the criterion can be simplified and approximated, and describe how these approaches can be combined. We demonstrate the effectiveness of our new framework and algorithm with directory assistance data collected under the real usage scenarios and show that our GERM based active learning and semi-supervised learning algorithms consistently outperform the confidence-based counterparts by a significant margin. Using our new active learning algorithm cuts the number of utterances needed for transcribing by 50% to achieve the same recognition accuracy obtained using the confidence-based active learning approach, and by 60% compared to the random sampling approach. Using our new semi-supervised algorithm we can determine the cutoff point in determining which utterance-transcription pair to use in a principled way by demonstrating that the point it finds is very close to the achievable peak point.","['Dong Yu', 'Balakrishnan Varadarajan', 'Li Deng', 'Alex Acero']",July 2010,Computer Speech & Language,"['Active learning', 'Semi-supervised learning', 'Acoustic model', 'Entropy reduction', 'Confidence', 'Lattice', 'Collective information']",Active learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion
599,"A new method for the recognition of spoken emotions is presented based on features of the glottal airflow signal. Its effectiveness is tested on the new optimum path classifier (OPF) as well as on six other previously established classification methods that included the Gaussian mixture model (GMM), support vector machine (SVM), artificial neural networks – multi layer perceptron (ANN-MLP), k-nearest neighbor rule (k-NN), Bayesian classifier (BC) and the C4.5 decision tree. The speech database used in this work was collected in an anechoic environment with ten speakers (5 M and 5 F) each speaking ten sentences in four different emotions: Happy, Angry, Sad, and Neutral. The glottal waveform was extracted from fluent speech via inverse filtering. The investigated features included the glottal symmetry and MFCC vectors of various lengths both for the glottal and the corresponding speech signal. Experimental results indicate that best performance is obtained for the glottal-only features with SVM and OPF generally providing the highest recognition rates, while for GMM or the combination of glottal and speech features performance was relatively inferior. For this text dependent, multi speaker task the top performing classifiers achieved perfect recognition rates for the case of 6th order glottal MFCCs.","['Alexander I. Iliev', 'Michael S. Scordilis', 'João P. Papa', 'Alexandre X. Falcão']",July 2010,Computer Speech & Language,"['Emotion recognition', 'Glottal analysis', 'Speech analysis', 'Optimum-path forest']",Spoken emotion recognition through optimum-path forest classification using glottal features
600,"In this paper we introduce an evidential reasoning based framework for weighted combination of classifiers for word sense disambiguation (WSD). Within this framework, we propose a new way of defining adaptively weights of individual classifiers based on ambiguity measures associated with their decisions with respect to each particular pattern under classification, where the ambiguity measure is defined by Shannon’s entropy. We then apply the discounting-and-combination scheme in Dempster–Shafer theory of evidence to derive a consensus decision for the classification task at hand. Experimentally, we conduct two scenarios of combining classifiers with the discussed method of weighting. In the first scenario, each individual classifier corresponds to a well-known learning algorithm and all of them use the same representation of context regarding the target word to be disambiguated, while in the second scenario the same learning algorithm applied to individual classifiers but each of them uses a distinct representation of the target word. These experimental scenarios are tested on English lexical samples of Senseval-2 and Senseval-3 resulting in an improvement in overall accuracy.","['Van-Nam Huynh', 'Tri Thanh Nguyen', 'Cuong Anh Le']",July 2010,Computer Speech & Language,"['Computational linguistics', 'Classifier combination', 'Word sense disambiguation', 'Dempster’s rule of combination', 'Entropy']",Adaptively entropy-based weighting classifiers in combination using Dempster–Shafer theory for word sense disambiguation☆
601,"The basic goal of the voice conversion system is to modify the speaker-specific characteristics, keeping the message and the environmental information contained in the speech signal intact. Speaker characteristics reflect in speech at different levels, such as, the shape of the glottal pulse (excitation source characteristics), the shape of the vocal tract (vocal tract system characteristics) and the long-term features (suprasegmental or prosodic characteristics). In this paper, we are proposing neural network models for developing mapping functions at each level. The features used for developing the mapping functions are extracted using pitch synchronous analysis. Pitch synchronous analysis provides the estimation of accurate vocal tract parameters, by analyzing the speech signal independently in each pitch period without influenced by the adjacent pitch cycles. In this work, the instants of significant excitation are used as pitch markers to perform the pitch synchronous analysis. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the linear prediction (LP) residual of speech signals by using the property of average group-delay of minimum phase signals. In this paper, line spectral frequencies (LSFs) are used for representing the vocal tract characteristics, and for developing its associated mapping function. LP residual of the speech signal is viewed as excitation source, and the residual samples around the instant of glottal closure are used for mapping. Prosodic parameters at syllable and phrase levels are used for deriving the mapping function. Source and system level mapping functions are derived pitch synchronously, and the incorporation of target prosodic parameters is performed pitch synchronously using instants of significant excitation. The performance of the voice conversion system is evaluated using listening tests. The prediction accuracy of the mapping functions (neural network models) used at different levels in the proposed voice conversion system is further evaluated using objective measures such as deviation (Di), root mean square error (μRMSE) and correlation coefficient (γX,Y). The proposed approach (i.e., mapping and modification of parameters using pitch synchronous approach) used for voice conversion is shown to be performed better compared to the earlier method (mapping the vocal tract parameters using block processing) proposed by the author.",['K. Sreenivasa Rao'],July 2010,Computer Speech & Language,"['Mapping function', 'Feedforward neural network (FFNN)', 'Pitch contour', 'Excitation source', 'LP residual', 'Instants of significant excitation (epochs)', 'Prosody characteristics', 'Duration and energy patterns', 'Glottal closure', 'Voice conversion', 'Objective measures', 'Mean opinion score (MOS)', 'ABX test']",Voice conversion by mapping the speaker-specific features using pitch synchronous approach
602,"Meeting summarization provides a concise and informative summary for the lengthy meetings and is an effective tool for efficient information access. In this paper, we focus on extractive summarization, where salient sentences are selected from the meeting transcripts to form a summary. We adopt a supervised learning approach for this task and use a classifier to determine whether to select a sentence in the summary based on a rich set of features. We address two important problems associated with this supervised classification approach. First we propose different sampling methods to deal with the imbalanced data problem for this task where the summary sentences are the minority class. Second, in order to account for human disagreement for summary annotation, we reframe the extractive summarization task using a regression scheme instead of binary classification. We evaluate our approaches using the ICSI meeting corpus on both the human transcripts and speech recognition output, and show performance improvement using different sampling methods and regression model.","['Shasha Xie', 'Yang Liu']",July 2010,Computer Speech & Language,"['Meeting summarization', 'Imbalanced data', 'Sampling', 'Regression']",Improving supervised learning for meeting summarization using sampling and regression
603,"The voice activity detectors (VADs) based on statistical models have shown impressive performances especially when fairly precise statistical models are employed. Moreover, the accuracy of the VAD utilizing statistical models can be significantly improved when machine-learning techniques are adopted to provide prior knowledge for speech characteristics. In the first part of this paper, we introduce a more accurate and flexible statistical model, the generalized gamma distribution (GΓD) as a new model in the VAD based on the likelihood ratio test. In practice, parameter estimation algorithm based on maximum likelihood principle is also presented. Experimental results show that the VAD algorithm implemented based on GΓD outperform those adopting the conventional Laplacian and Gamma distributions. In the second part of this paper, we introduce machine learning techniques such as a minimum classification error (MCE) and support vector machine (SVM) to exploit automatically prior knowledge obtained from the speech database, which can enhance the performance of the VAD. Firstly, we present a discriminative weight training method based on the MCE criterion. In this approach, the VAD decision rule becomes the geometric mean of optimally weighted likelihood ratios. Secondly, the SVM-based approach is introduced to assist the VAD based on statistical models. In this algorithm, the SVM efficiently classifies the input signal into two classes which are voice active and voice inactive regions with nonlinear boundary. Experimental results show that these training-based approaches can effectively enhance the performance of the VAD.","['Jong Won Shin', 'Joon-Hyuk Chang', 'Nam Soo Kim']",July 2010,Computer Speech & Language,"['Voice activity detection', 'Statistical modeling', 'Machine learning', 'Prior knowledge', 'Likelihood ratio test', 'Generalized gamma', 'Minimum classification error', 'Support vector machine', 'A posteriori SNR', 'A priori SNR', 'Predicted SNR']",Voice activity detection based on statistical models and machine learning approaches
604,"This paper proposes an improved voice activity detection (VAD) algorithm using wavelet and support vector machine (SVM) for European Telecommunication Standards Institution (ETSI) adaptive multi-rate (AMR) narrow-band (NB) and wide-band (WB) speech codecs. First, based on the wavelet transform, the original IIR filter bank and pitch/tone detector are implemented, respectively, via the wavelet filter bank and the wavelet-based pitch/tone detection algorithm. The wavelet filter bank can divide input speech signal into several frequency bands so that the signal power level at each sub-band can be calculated. In addition, the background noise level can be estimated in each sub-band by using the wavelet de-noising method. The wavelet filter bank is also derived to detect correlated complex signals like music. Then the proposed algorithm can apply SVM to train an optimized non-linear VAD decision rule involving the sub-band power, noise level, pitch period, tone flag, and complex signals warning flag of input speech signals. By the use of the trained SVM, the proposed VAD algorithm can produce more accurate detection results. Various experimental results carried out from the Aurora speech database with different noise conditions show that the proposed algorithm gives considerable VAD performances superior to the AMR-NB VAD Options 1 and 2, and AMR-WB VAD.","['Shi-Huang Chen', 'Rodrigo Capobianco Guido', 'Trieu-Kien Truong', 'Yaotsu Chang']",July 2010,Computer Speech & Language,"['Voice activity detection (VAD)', 'AMR-NB', 'AMR-WB', 'Wavelet transform', 'Support vector machine (SVM)']",Improved voice activity detection algorithm using wavelet and support vector machine
605,"Text prediction was initially proposed to help people with a low text composition speed to enhance their message composition. After the important advancements obtained in the last years, text prediction methods may nowadays benefit anyone trying to input text messages or commands, if they are adequately integrated within the user interface of the application. Diverse text prediction methods are based in different statistic and linguistic properties of natural languages. Hence, they are very dependent on the language concerned. In order to discuss general issues of text prediction it is necessary to propose abstract descriptions of the methods used. In this paper a number of models applied to text prediction are presented. Some of them are oriented to low-inflected languages while others are for high-inflected languages. All these models have been implemented and their results are compared. Presented models may be useful for future discussion. Finally, some comments related to the comparison of previously published results are also done.","['Nestor Garay-Vitoria', 'Julio Abascal']",April 2010,Computer Speech & Language,"['Text prediction', 'Anticipative interfaces', 'Prediction models', 'Communication speed enhancement', 'Prediction measures']",Modelling text prediction systems in low- and high-inflected languages
606,"The cascading appearance-based (CAB) feature extraction technique has established itself as the state-of-the-art in extracting dynamic visual speech features for speech recognition. In this paper, we will focus on investigating the effectiveness of this technique for the related speaker verification application. By investigating the speaker verification ability of each stage of the cascade we will demonstrate that the same steps taken to reduce static speaker and environmental information for the visual speech recognition application also provide similar improvements for visual speaker recognition. A further study is conducted comparing synchronous HMM (SHMM) based fusion of CAB visual features and traditional perceptual linear predictive (PLP) acoustic features to show that higher complexity inherit in the SHMM approach does not appear to provide any improvement in the final audio–visual speaker verification system over simpler utterance level score fusion.","['David Dean', 'Sridha Sridharan']",April 2010,Computer Speech & Language,"['Audio-visual speaker recognition', 'Cascading appearance-based features', 'Synchronous hidden Markov models']",Dynamic visual features for audio–visual speaker verification☆
607,This paper explains how Partially Observable Markov Decision Processes (POMDPs) can provide a principled mathematical framework for modelling the inherent uncertainty in spoken dialogue systems. It briefly summarises the basic mathematics and explains why exact optimisation is intractable. It then describes in some detail a form of approximation called the Hidden Information State model which does scale and which can be used to build practical systems. A prototype HIS system for the tourist information domain is evaluated and compared with a baseline MDP system using both user simulations and a live user trial. The results give strong support to the central contention that the POMDP-based framework is both a tractable and powerful approach to building more robust spoken dialogue systems.,"['Steve Young', 'Milica Gašić', 'Simon Keizer', 'François Mairesse', 'Jost Schatzmann', 'Blaise Thomson', 'Kai Yu']",April 2010,Computer Speech & Language,"['Statistical dialogue systems', 'POMDP', 'Hidden Information State model']",The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management
608,"An initiative conflict is an overlap of speech in which both conversants try to steer the conversation in different directions. In this paper, we investigate how conversants in human-human dialogue deal with such conflicts. First, in investigating why initiative conflicts occur, we find that the offsets of the utterances involved in initiative conflicts tend to be very short, and that initiative conflicts seem more likely to occur when one of the conversants has an urgent conversational goal. These findings strongly suggest that initiative conflicts are unintentional collisions and that conversants try to prevent them from even occurring, unless there is an urgent reason. Second, in investigating how initiative conflicts are resolved, we find that the overlaps tend to last less than two syllables, that volume correlates with who wins initiative conflicts, and that for longer overlaps, the volume of the winner increases in the second half of the overlaps. These findings strongly suggest that initiative conflicts are quickly resolved through an interactive process, using volume as one of the devices. Third, we find that after an initiative conflict is resolved, the winner sometimes repeats the words involved in the overlap; and this happens more when the overlap is more likely to interfere with the other conversant’s understanding. These findings will help us build next-generation mixed-initiative spoken dialogue systems that are natural and efficient to use.","['Fan Yang', 'Peter A. Heeman']",April 2010,Computer Speech & Language,"['Initiative', 'Initiative conflict', 'Turn-taking']",Initiative conflicts in task-oriented dialogue
609,"The automatic recognition of dialogue act is a task of crucial importance for the processing of natural language dialogue at discourse level. It is also one of the most challenging problems as most often the dialogue act is not expressed directly in speaker’s utterance. In this paper, a new cue-based model for dialogue act recognition is presented. The model is, essentially, a dynamic Bayesian network induced from manually annotated dialogue corpus via dynamic Bayesian machine learning algorithms. Furthermore, the dynamic Bayesian network’s random variables are constituted from sets of lexical cues selected automatically by means of a variable length genetic algorithm, developed specifically for this purpose. To evaluate the proposed approaches of design, three stages of experiments have been conducted. In the initial stage, the dynamic Bayesian network model is constructed using sets of lexical cues selected manually from the dialogue corpus. The model is evaluated against two previously proposed models and the results confirm the potentiality of dynamic Bayesian networks for dialogue act recognition. In the second stage, the developed variable length genetic algorithm is used to select different sets of lexical cues to constitute the dynamic Bayesian networks’ random variables. The developed approach is evaluated against some of the previously used ranking approaches and the results provide experimental evidences on its ability to avoid the drawbacks of the ranking approaches. In the third stage, the dynamic Bayesian networks model is constructed using random variables constituted from the sets of lexical cues generated in the second stage and the results confirm the effectiveness of the proposed approaches for designing dialogue act recognition model.","['Anwar Ali Yahya', 'Ramlan Mahmod', 'Abd Rahman Ramli']",April 2010,Computer Speech & Language,"['Dialogue act recognition', 'Dynamic Bayesian networks', 'Variable length genetic algorithm', 'Lexical cues selection']",Dynamic Bayesian networks and variable length genetic algorithm for designing cue-based model for dialogue act recognition
610,"This paper investigates the unique pharyngeal and uvular consonants of Arabic from the point of view of automatic speech recognition (ASR). Comparisons of the recognition error rates for these phonemes are analyzed in five experiments that involve different combinations of native and non-native Arabic speakers. The most three confusing consonants for every investigated consonant are discussed. All experiments use the Hidden Markov Model Toolkit (HTK) and the Language Data Consortium (LDC) WestPoint Modern Standard Arabic (MSA) database. Results confirm that these Arabic distinct consonants are a major source of difficulty for Arabic ASR. While the recognition rate for certain of these unique consonants such as /ℏ/ can drop below 35% when uttered by non-native speakers, there is advantage to include non-native speakers in ASR. Besides, regional differences in pronunciation of MSA by native Arabic speakers require the attention of Arabic ASR research.","['Yousef Ajami Alotaibi', 'Ghulam Muhammad']",April 2010,Computer Speech & Language,"['Arabic', 'Foreign accents', 'HMMs', 'Pharyngeal', 'Uvular', 'Speech recognition']",Study on pharyngeal and uvular consonants in foreign accented Arabic for ASR
611,"This paper addresses modeling user behavior in interactions between two people who do not share a common spoken language and communicate with the aid of an automated bidirectional speech translation system. These interaction settings are complex. The translation machine attempts to bridge the language gap by mediating the verbal communication, noting however that the technology may not be always perfect. In a step toward understanding user behavior in this mediated communication scenario, usability data from doctor–patient dialogs involving a two way English–Persian speech translation system are analyzed. We specifically consider user behavior in light of potential uncertainty in the communication between the interlocutors. We analyze the Retry (Repeat and Rephrase) versus Accept behaviors in the mediated verbal channel and as a result identify three user types – Accommodating, Normal and Picky, and propose a dynamic Bayesian network model of user behavior. To validate the model, we performed offline and online experiments. The experimental results using offline data show that correct user type is clearly identified as a user keeps his/her consistent behavior in a given interaction condition. In the online experiment, agent feedback was presented to users according to the user types. We show high user satisfaction and interaction efficiency in the analysis of user interview, video data, questionnaire and log data.","['JongHo Shin', 'Panayiotis G. Georgiou', 'Shrikanth Narayanan']",April 2010,Computer Speech & Language,"['Speech to speech translation', 'User modeling', 'Bayesian reasoning', 'Spoken dialog systems', 'Agent', 'Feedback', 'Objective and subjective measures', 'Machine mediated communication', 'Cross-lingual interactions']",Towards modeling user behavior in interactions mediated through an automated bidirectional speech translation system
612,"Nowadays the applications in multimedia domain require that the Speech/Music classifier has many other merits in addition to the accuracy, such as short-time delay and low complexity. Here, we endeavor to form a Speech/Music classifier by using different data mining methods. The main contributions of this paper are to obtain a system by analyzing the inherent validity of diverse features extracted from the audio, building a hierarchical structure of oblique decision trees (HODT) to maintain optimal performances, and applying a novel context-based state transform (ST) strategy to refine the classification results. The proposed algorithm is evaluated by a set of 5–11 min 702 audio files, which are made from 54 speech or music files according to different Signal-to-Noise Ratio (SNR) levels and diverse noise types. The experiment results show that our proposed classifier outperforms AMR-WB+ by achieving 97.9% and 95.9% in classification rate at the 10 ms frame level in pure and high SNR (> = 20 dB) environment, respectively. The post-processing ST strategy further enhances the system performance, particularly at low SNR circumstances (10 dB), with 5.6% up in the accuracy rate. In addition, the complexity of the proposed system is lower than 1WMOPS which make it easily adaptable to many scenarios.","['Qiong Wu', 'Qin Yan', 'Haojiang Deng', 'Jinlin Wang']",April 2010,Computer Speech & Language,"['Real-time discrimination', 'Optimal feature subset', 'Hierarchical oblique decision tree', 'State transform strategy']",A combination of data mining method with decision trees building for Speech/Music discrimination
613,"In the present work we study the appropriateness of a number of linear and non-linear regression methods, employed on the task of speech segmentation, for combining multiple phonetic boundary predictions which are obtained through various segmentation engines. The proposed fusion schemes are independent of the implementation of the individual segmentation engines as well as from their number. In order to illustrate the practical significance of the proposed approach, we employ 112 speech segmentation engines based on hidden Markov models (HMMs), which differ in the setup of the HMMs and in the speech parameterization techniques they employ. Specifically we relied on sixteen different HMMs setups and on seven speech parameterization techniques, four of which are recent and their performance on the speech segmentation task have not been evaluated yet. In the evaluation experiments we contrast the performance of the proposed fusion schemes for phonetic boundary predictions against some recently reported methods. Throughout this comparison, on the established for the phonetic segmentation task TIMIT database, we demonstrate that the support vector regression scheme is capable of achieving more accurate predictions, when compared to other fusion schemes reported so far.","['Iosif Mporas', 'Todor Ganchev', 'Nikos Fakotakis']",April 2010,Computer Speech & Language,"['Speech segmentation', 'Regression fusion', 'Hidden Markov models']",Speech segmentation using regression fusion of boundary predictions
614,"There are many speech and language processing problems which require cascaded classification tasks. While model adaptation has been shown to be useful in isolated speech and language processing tasks, it is not clear what constitutes system adaptation for such complex systems. This paper studies the following questions: In cases where a sequence of classification tasks is employed, how important is to adapt the earlier or latter systems? Is the performance improvement obtained in the earlier stages via adaptation carried on to later stages in cases where the later stages perform adaptation using similar data and/or methods? In this study, as part of a larger scale multiparty meeting understanding system, we analyze various methods for adapting dialog act segmentation and tagging models trained on conversational telephone speech (CTS) to meeting style conversations. We investigate the effect of using adapted and unadapted models for dialog act segmentation with those of tagging, showing the effect of model adaptation for cascaded classification tasks. Our results indicate that we can achieve significantly better dialog act segmentation and tagging by adapting the out-of-domain models, especially when the amount of in-domain data is limited. Experimental results show that it is more effective to adapt the models in the latter classification tasks, in our case dialog act tagging, when dealing with a sequence of cascaded classification tasks.","['Umit Guz', 'Gokhan Tur', 'Dilek Hakkani-Tür', 'Sébastien Cuendet']",April 2010,Computer Speech & Language,"['Model adaptation', 'Dialog act segmentation', 'Dialog act tagging', 'Meetings processing']",Cascaded model adaptation for dialog act segmentation and tagging
615,"We are addressing the novel problem of jointly evaluating multiple speech patterns for automatic speech recognition and training. We propose solutions based on both the non-parametric dynamic time warping (DTW) algorithm, and the parametric hidden Markov model (HMM). We show that a hybrid approach is quite effective for the application of noisy speech recognition. We extend the concept to HMM training wherein some patterns may be noisy or distorted. Utilizing the concept of “virtual pattern” developed for joint evaluation, we propose selective iterative training of HMMs. Evaluating these algorithms for burst/transient noisy speech and isolated word recognition, significant improvement in recognition accuracy is obtained using the new algorithms over those which do not utilize the joint evaluation strategy.","['Nishanth Ulhas Nair', 'T.V. Sreenivas']",April 2010,Computer Speech & Language,"['Joint recognition', 'Joint decoding', 'Dynamic time warping', 'Viterbi algorithm', 'Hidden Markov models', 'Multi pattern analysis', 'Robust speech recognition', 'Selective HMM training', 'Multi pattern dynamic time warping']",Joint evaluation of multiple speech patterns for speech recognition and training
616,"This paper addresses the problem of parameterization for speech/music discrimination. The current successful parameterization based on cepstral coefficients uses the Fourier transformation (FT), which is well adapted for stationary signals. In order to take into account the non-stationarity of music/speech signals, this work proposes to study wavelet-based signal decomposition instead of FT. Three wavelet families and several numbers of vanishing moments have been evaluated. Different types of energy, calculated for each frequency band obtained from wavelet decomposition, are studied. Static, dynamic and long-term parameters were evaluated. The proposed parameterization are integrated into two class/non-class classifiers: one for speech/non-speech, one for music/non-music. Different experiments on realistic corpora, including different styles of speech and music (Broadcast News, Entertainment, Scheirer), illustrate the performance of the proposed parameterization, especially for music/non-music discrimination. Our parameterization yielded a significant reduction of the error rate. More than 30% relative improvement was obtained for the envisaged tasks compared to MFCC parameterization.","['E. Didiot', 'I. Illina', 'D. Fohr', 'O. Mella']",April 2010,Computer Speech & Language,"['Speech/music discrimination', 'Segmentation', 'Wavelets', 'Static parameters', 'Dynamic parameters', 'Long-term parameters']",A wavelet-based parameterization for speech/music discrimination
617,"In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems. We model the task of spoken language understanding as a two-stage classification problem. Firstly, the topic classifier is used to identify the topic of an input utterance. Secondly, with the restriction of the recognized target topic, the slot classifiers are trained to extract the corresponding slot-value pairs. It is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language. More importantly, it allows that weakly supervised strategies are employed for training the two kinds of classifiers, which could significantly reduce the number of labeled sentences. We investigated active learning and naive self-training for the two kinds of classifiers. Also, we propose a practical method for bootstrapping topic-dependent slot classifiers from a small amount of labeled sentences. Experiments have been conducted in the context of the Chinese public transportation information inquiry domain and the English DARPA Communicator domain. The experimental results show the effectiveness of our proposed SLU framework and demonstrate the possibility to reduce human labeling efforts significantly.","['Wei-Lin Wu', 'Ru-Zhan Lu', 'Jian-Yong Duan', 'Hui Liu', 'Feng Gao', 'Yu-Quan Chen']",April 2010,Computer Speech & Language,"['Spoken language understanding', 'Spoken dialogue system', 'Topic classification', 'Active learning', 'Self-training', 'Bootstrapping']",Spoken language understanding using weakly supervised learning☆
618,"Collocations are linguistic phenomena that occur when two or more words appear together more often than by chance and whose meaning often cannot be inferred from the meanings of its parts. As collocations have found many applications in the fields of natural language processing, information retrieval, and text mining, extracting them from large corpora has been the focus of many studies over the past few years. In this paper, we introduce the notion of an extension pattern, a formalization of the idea of extending lexical association measures (AMs) defined for bigrams. An extension pattern provides a measure-independent way of extending AMs for extracting collocations of arbitrary length. We define different extension patterns and compare them on a task of extracting collocations from a newspaper corpus. We show that the stopword-sensitive extension patterns we propose outperform other extensions, which indicates that AMs could benefit by taking into account linguistic information about an n-gram’s part-of-speech pattern.","['Saša Petrović', 'Jan Šnajder', 'Bojana Dalbelo Bašić']",April 2010,Computer Speech & Language,"['Collocations', 'Multi-word expressions', 'Collocation extraction', 'Lexical association measures']",Extending lexical association measures for collocation extraction
619,"We describe an evaluation of spoken dialogue strategies designed using hierarchical reinforcement learning agents. The dialogue strategies were learnt in a simulated environment and tested in a laboratory setting with 32 users. These dialogues were used to evaluate three types of machine dialogue behaviour: hand-coded, fully-learnt and semi-learnt. These experiments also served to evaluate the realism of simulated dialogues using two proposed metrics contrasted with ‘Precision-Recall’. The learnt dialogue behaviours used the Semi-Markov Decision Process (SMDP) model, and we report the first evaluation of this model in a realistic conversational environment. Experimental results in the travel planning domain provide evidence to support the following claims: (a) hierarchical semi-learnt dialogue agents are a better alternative (with higher overall performance) than deterministic or fully-learnt behaviour; (b) spoken dialogue strategies learnt with highly coherent user behaviour and conservative recognition error rates (keyword error rate of 20%) can outperform a reasonable hand-coded strategy; and (c) hierarchical reinforcement learning dialogue agents are feasible and promising for the (semi) automatic design of optimized dialogue behaviours in larger-scale systems.","['Heriberto Cuayáhuitl', 'Steve Renals', 'Oliver Lemon', 'Hiroshi Shimodaira']",April 2010,Computer Speech & Language,"['Spoken dialogue systems', 'Hierarchical reinforcement learning', 'Human–machine dialogue simulation', 'Dialogue strategies', 'System evaluation']",Evaluation of a hierarchical reinforcement learning spoken dialogue system
620,"Robust speech recognition in everyday conditions requires the solution to a number of challenging problems, not least the ability to handle multiple sound sources. The specific case of speech recognition in the presence of a competing talker has been studied for several decades, resulting in a number of quite distinct algorithmic solutions whose focus ranges from modeling both target and competing speech to speech separation using auditory grouping principles. The purpose of the monaural speech separation and recognition challenge was to permit a large-scale comparison of techniques for the competing talker problem. The task was to identify keywords in sentences spoken by a target talker when mixed into a single channel with a background talker speaking similar sentences. Ten independent sets of results were contributed, alongside a baseline recognition system. Performance was evaluated using common training and test data and common metrics. Listeners’ performance in the same task was also measured. This paper describes the challenge problem, compares the performance of the contributed algorithms, and discusses the factors which distinguish the systems. One highlight of the comparison was the finding that several systems achieved near-human performance in some conditions, and one out-performed listeners overall.","['Martin Cooke', 'John R. Hershey', 'Steven J. Rennie']",January 2010,Computer Speech & Language,"['Speech recognition', 'Speech separation', 'Speaker identification', 'Simultaneous speech', 'Auditory scene analysis', 'Noise robustness']",Monaural speech separation and recognition challenge
621,"We present a system for model-based source separation for use on single channel speech mixtures where the precise source characteristics are not known a priori. The sources are modeled using hidden Markov models (HMM) and separated using factorial HMM methods. Without prior speaker models for the sources in the mixture it is difficult to exactly resolve the individual sources because there is no way to determine which state corresponds to which source at any point in time. This is solved to a small extent by the temporal constraints provided by the Markov models, but permutations between sources remains a significant problem. We overcome this by adapting the models to match the sources in the mixture. We do this by representing the space of speaker variation with a parametric signal model-based on the eigenvoice technique for rapid speaker adaptation. We present an algorithm to infer the characteristics of the sources present in a mixture, allowing for significantly improved separation performance over that obtained using unadapted source models. The algorithm is evaluated on the task defined in the 2006 Speech Separation Challenge [Cooke, M.P., Lee, T.-W., 2008. The 2006 Speech Separation Challenge. Computer Speech and Language] and compared with separation using source-dependent models. Although performance is not as good as with speaker-dependent models, we show that the system based on model adaptation is able to generalize better to held out speakers.","['Ron J. Weiss', 'Daniel P.W. Ellis']",January 2010,Computer Speech & Language,"['Source separation', 'Model adaptation', 'Eigenvoice']",Speech separation using speaker-adapted eigenvoice speech models
622,"Robustness is one of the most important topics for automatic speech recognition (ASR) in practical applications. Monaural speech separation based on computational auditory scene analysis (CASA) offers a solution to this problem. In this paper, a novel system is presented to separate the monaural speech of two talkers. Gaussian mixture models (GMMs) and vector quantizers (VQs) are used to learn the grouping cues on isolated clean data for each speaker. Given an utterance, speaker identification is firstly performed to identify the two speakers presented in the utterance, then the factorial-max vector quantization model (MAXVQ) is used to infer the mask signals and finally the utterance of the target speaker is resynthesized in the CASA framework. Recognition results on the 2006 speech separation challenge corpus prove that this proposed system can improve the robustness of ASR significantly.","['Peng Li', 'Yong Guan', 'Shijin Wang', 'Bo Xu', 'Wenju Liu']",January 2010,Computer Speech & Language,"['Monaural speech separation', 'Computational auditory scene analysis (CASA)', 'Factorial-max vector quantization (MAXVQ)', 'Automatic speech recognition (ASR)']",Monaural speech separation based on MAXVQ and CASA for robust speech recognition
623,"We present a system that can separate and recognize the simultaneous speech of two people recorded in a single channel. Applied to the monaural speech separation and recognition challenge, the system out-performed all other participants – including human listeners – with an overall recognition error rate of 21.6%, compared to the human error rate of 22.3%. The system consists of a speaker recognizer, a model-based speech separation module, and a speech recognizer. For the separation models we explored a range of speech models that incorporate different levels of constraints on temporal dynamics to help infer the source speech signals. The system achieves its best performance when the model of temporal dynamics closely captures the grammatical constraints of the task. For inference, we compare a 2-D Viterbi algorithm and two loopy belief-propagation algorithms. We show how belief-propagation reduces the complexity of temporal inference from exponential to linear in the number of sources and the size of the language model. The best belief-propagation method results in nearly the same recognition error rate as exact inference.","['John R. Hershey', 'Steven J. Rennie', 'Peder A. Olsen', 'Trausti T. Kristjansson']",January 2010,Computer Speech & Language,"['Factorial hidden Markov model', 'Speech separation', 'Algonquin', 'Multiple talker speaker identification', 'Speaker-dependent labeling']",Super-human multi-talker speech recognition: A graphical modeling approach
624,"This paper considers the separation and recognition of overlapped speech sentences assuming single-channel observation. A system based on a combination of several different techniques is proposed. The system uses a missing-feature approach for improving crosstalk/noise robustness, a Wiener filter for speech enhancement, hidden Markov models for speech reconstruction, and speaker-dependent/-independent modeling for speaker and speech recognition. We develop the system on the Speech Separation Challenge database, involving a task of separating and recognizing two mixing sentences without assuming advanced knowledge about the identity of the speakers nor about the signal-to-noise ratio. The paper is an extended version of a previous conference paper submitted for the challenge.","['Ji Ming', 'Timothy J. Hazen', 'James R. Glass']",January 2010,Computer Speech & Language,"['Speech enhancement', 'Speaker modeling', 'Speech recognition', 'Missing-feature theory', 'Posterior union model']","Combining missing-feature theory, speech enhancement, and speaker-dependent/-independent modeling for speech separation"
625,"A conventional automatic speech recognizer does not perform well in the presence of multiple sound sources, while human listeners are able to segregate and recognize a signal of interest through auditory scene analysis. We present a computational auditory scene analysis system for separating and recognizing target speech in the presence of competing speech or noise. We estimate, in two stages, the ideal binary time–frequency (T–F) mask which retains the mixture in a local T–F unit if and only if the target is stronger than the interference within the unit. In the first stage, we use harmonicity to segregate the voiced portions of individual sources in each time frame based on multipitch tracking. Additionally, unvoiced portions are segmented based on an onset/offset analysis. In the second stage, speaker characteristics are used to group the T–F units across time frames. The resulting masks are used in an uncertainty decoding framework for automatic speech recognition. We evaluate our system on a speech separation challenge and show that our system yields substantial improvement over the baseline performance.","['Yang Shao', 'Soundararajan Srinivasan', 'Zhaozhang Jin', 'DeLiang Wang']",January 2010,Computer Speech & Language,"['Speech segregation', 'Computational Auditory Scene Analysis', 'Binary time–frequency mask', 'Robust speech recognition', 'Uncertainty decoding']",A computational auditory scene analysis system for speech segregation and robust speech recognition
626,"This paper addresses the problem of recognising speech in the presence of a competing speaker. We review a speech fragment decoding technique that treats segregation and recognition as coupled problems. Data-driven techniques are used to segment a spectro-temporal representation into a set of fragments, such that each fragment is dominated by one or other of the speech sources. A speech fragment decoder is used which employs missing data techniques and clean speech models to simultaneously search for the set of fragments and the word sequence that best matches the target speaker model. The paper investigates the performance of the system on a recognition task employing artificially mixed target and masker speech utterances. The fragment decoder produces significantly lower error rates than a conventional recogniser, and mimics the pattern of human performance that is produced by the interplay between energetic and informational masking. However, at around 0 dB the performance is generally quite poor. An analysis of the errors shows that a large number of target/masker confusions are being made. The paper presents a novel fragment-based speaker identification approach that allows the target speaker to be reliably identified across a wide range of SNRs. This component is combined with the recognition system to produce significant improvements. When the target and masker utterance have the same gender, the recognition system has a performance at 0 dB equal to that of humans; in other conditions the error rate is roughly twice the human error rate.","['Jon Barker', 'Ning Ma', 'André Coy', 'Martin Cooke']",January 2010,Computer Speech & Language,"['Speech recognition', 'Speech separation', 'Speaker identification', 'Simultaneous speech', 'Auditory scene analysis', 'Noise robustness']",Speech fragment decoding techniques for simultaneous speaker identification and speech recognition
627,,[],January 2010,Computer Speech & Language,[],List of reviewers
628,"Prosody is an important cue for identifying dialog acts. In this paper, we show that modeling the sequence of acoustic–prosodic values as n-gram features with a maximum entropy model for dialog act (DA) tagging can perform better than conventional approaches that use coarse representation of the prosodic contour through summative statistics of the prosodic contour. The proposed scheme for exploiting prosody results in an absolute improvement of 8.7% over the use of most other widely used representations of acoustic correlates of prosody. The proposed scheme is discriminative and exploits context in the form of lexical, syntactic and prosodic cues from preceding discourse segments. Such a decoding scheme facilitates online DA tagging and offers robustness in the decoding process, unlike greedy decoding schemes that can potentially propagate errors. Our approach is different from traditional DA systems that use the entire conversation for offline dialog act decoding with the aid of a discourse model. In contrast, we use only static features and approximate the previous dialog act tags in terms of lexical, syntactic and prosodic information extracted from previous utterances. Experiments on the Switchboard-DAMSL corpus, using only lexical, syntactic and prosodic cues from three previous utterances, yield a DA tagging accuracy of 72% compared to the best case scenario with accurate knowledge of previous DA tags (oracle), which results in 74% accuracy.","['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangalore', 'Shrikanth Narayanan']",October 2009,Computer Speech & Language,"['Dialog act tagging', 'Prosodic cues', 'Acoustic correlates of prosody', 'Maximum entropy modeling', 'Discourse context']","Combining lexical, syntactic and prosodic cues for improved online dialog act tagging"
629,"This paper presents a data-driven Korean grapheme-to-phoneme conversion method including alignment, rule extraction, and rule pruning procedures. Novel rule extraction and pruning techniques are introduced to effectively handle the exceptional pronunciation of speech databases. The performances with the full rules and the reduced rules are 99.22% and 99.11% phoneme level accuracy, respectively. Compared with state-of-the-art previous works, the experimental results show that our method is very promising.","['Jinsik Lee', 'Gary Geunbae Lee']",October 2009,Computer Speech & Language,"['Grapheme-to-phoneme conversion', 'Letter-to-sound rules', 'Korean TTS systems']",A data-driven grapheme-to-phoneme conversion method using dynamic contextual converting rules for Korean TTS systems☆
630,"We investigate whether accent identification is more effective for English utterances embedded in a different language as part of a mixed code than for English utterances that are part of a monolingual dialogue. Our focus is on Xhosa and Zulu, two South African languages for which code-mixing with English is very common. In order to carry out our investigation, we extract English utterances from mixed-code Xhosa and Zulu speech corpora, as well as comparable utterances from an English-only corpus by Xhosa and Zulu mother-tongue speakers. Experiments using automatic accent identification systems show that identification is substantially more accurate for the utterances originating from the mixed-code speech. These findings are supported by a corresponding set of perceptual experiments in which human subjects were asked to identify the accents of recorded utterances. We conclude that accent identification is more successful for these utterances because accents are more pronounced for English embedded in mother-tongue speech than for English spoken as part of a monolingual dialogue by non-native speakers. Furthermore we find that this is true for human listeners as well as for automatic identification systems.","['Thomas Niesler', 'Febe de Wet']",October 2009,Computer Speech & Language,"['Accent identification', 'Code-mixing']",The effect of code-mixing on accent identification accuracy☆
631,"Linguistic rules have been assumed to be the best technique for determining the syllabification of unknown words. This has recently been challenged for the English language where data-driven algorithms have been shown to outperform rule-based methods. It may be possible, however, that data-driven methods are only better for languages with complex syllable structures. In this study, three rule-based automatic syllabification systems and two data-driven automatic syllabification systems (Syllabification by Analogy and the Look-Up Procedure) are compared on a language with lower syllabic complexity – Italian. Comparing the performance using a lexicon containing 44,720 words, the best data-driven algorithm (Syllabification by Analogy) achieved 97.70% word accuracy while the best rule set correctly syllabified 89.77% words. These results show that data-driven methods can also outperform rule-based methods on Italian syllabification, a language of low syllabic complexity.","['Connie R. Adsett', 'Yannick Marchand', 'Vlado Kes˘elj']",October 2009,Computer Speech & Language,"['Syllabification', 'Italian language', 'Rule-based systems', 'Data-driven methods', 'Analogy']",Syllabification rules versus data-driven methods in a language with low syllabic complexity: The case of Italian
632,"This paper proposes a semi-supervised learning method for semantic relation extraction between named entities. Given a small amount of labeled data, it benefits much from a large amount of unlabeled data by first bootstrapping a moderate number of weighted support vectors from all the available data through a co-training procedure on top of support vector machines (SVM) with feature projection and then applying a label propagation (LP) algorithm via the bootstrapped support vectors and the remaining hard unlabeled instances after SVM bootstrapping to classify unseen instances. Evaluation on the ACE RDC corpora shows that our method can integrate the advantages of both SVM bootstrapping and label propagation. It shows that our LP algorithm via the bootstrapped support vectors and hard unlabeled instances significantly outperforms the normal LP algorithm via all the available data without SVM bootstrapping. Moreover, our LP algorithm can significantly reduce the computational burden, especially when a large amount of labeled and unlabeled data is taken into consideration.","['Zhou GuoDong', 'Qian LongHua', 'Zhu QiaoMing']",October 2009,Computer Speech & Language,"['Semi-supervised learning', 'Semantic relation extraction', 'Bootstrapped support vectors', 'SVM bootstrapping', 'Label propagation']",Label propagation via bootstrapped support vectors for semantic relation extraction between named entities
633,"This paper proposes a novel integrated dialog simulation technique for evaluating spoken dialog systems. A data-driven user simulation technique for simulating user intention and utterance is introduced. A novel user intention modeling and generating method is proposed that uses a linear-chain conditional random field, and a two-phase data-driven domain-specific user utterance simulation method and a linguistic knowledge-based ASR channel simulation method are also presented. Evaluation metrics are introduced to measure the quality of user simulation at intention and utterance. Experiments using these techniques were carried out to evaluate the performance and behavior of dialog systems designed for car navigation dialogs and a building guide robot, and it turned out that our approach was easy to set up and showed similar tendencies to real human users.","['Sangkeun Jung', 'Cheongjae Lee', 'Kyungduk Kim', 'Minwoo Jeong', 'Gary Geunbae Lee']",October 2009,Computer Speech & Language,"['User simulation', 'Dialog simulation', 'Evaluation', 'Data-driven', 'Spoken dialog system', 'Dialog system']",Data-driven user simulation for automated evaluation of spoken dialog systems
634,"In large vocabulary continuous speech recognition (LVCSR) the acoustic model computations often account for the largest processing overhead. Our weighted finite state transducer (WFST) based decoding engine can utilize a commodity graphics processing unit (GPU) to perform the acoustic computations to move this burden off the main processor. In this paper we describe our new GPU scheme that can achieve a very substantial improvement in recognition speed whilst incurring no reduction in recognition accuracy. We evaluate the GPU technique on a large vocabulary spontaneous speech recognition task using a set of acoustic models with varying complexity and the results consistently show by using the GPU it is possible to reduce the recognition time with largest improvements occurring in systems with large numbers of Gaussians. For the systems which achieve the best accuracy we obtained between 2.5 and 3 times speed-ups. The faster decoding times translate to reductions in space, power and hardware costs by only requiring standard hardware that is already widely installed.","['Paul R. Dixon', 'Tasuku Oonishi', 'Sadaoki Furui']",October 2009,Computer Speech & Language,"['LVCSR', 'GPGPU', 'Novel hardware for ASR', 'WFST']",Harnessing graphics processors for the fast computation of acoustic likelihoods in speech recognition
635,"The task of information distillation is to extract snippets from massive multilingual audio and textual document sources that are relevant for a given templated query. We present an approach that focuses on the sentence extraction phase of the distillation process. It selects document sentences with respect to their relevance to a query via statistical classification with support vector machines. The distinguishing contribution of the approach is a novel method to generate classification features. The features are extracted from charts, compilations of elements from various annotation layers, such as word transcriptions, syntactic and semantic parses, and information extraction (IE) annotations. We describe a procedure for creating charts from documents and queries, while paying special attention to query slots (free-text descriptions of names, organizations, topic, events and so on, around which templates are centered), and suggest various types of classification features that can be extracted from these charts. While observing a 30% relative improvement due to non-lexical annotation layers, we perform a detailed analysis of the contributions of each of these layers to classification performance.","['Michael Levit', 'Dilek Hakkani-Tür', 'Gokhan Tur', 'Daniel Gillick']",October 2009,Computer Speech & Language,"['Question answering', 'Information distillation', 'Machine learning', 'Information extraction', 'Natural language processing']",IXIR: A statistical information distillation system
636,"In modern standard Arabic and in dialectal Arabic texts, short vowels and other diacritics are omitted. Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic. Scripts without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting. In this paper we present a maximum entropy approach for restoring short vowels and other diacritics in an Arabic document. The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segment-based and part-of-speech tag features. The combination of these feature types leads to a high-performance diacritic restoration model. Using a publicly available corpus (LDC’s Arabic Treebank Part 3), we achieve a diacritic error rate of 5.1%, a segment error rate 8.5%, and a word error rate of 17.3%. In case-ending-less setting, we obtain a diacritic error rate of 2.2%, a segment error rate of 4.0%, and a word error rate of 7.2%. We also show in this paper a comparison of our approach to previously published techniques and we demonstrate the effectiveness of this technique in restoring diacritics in different kind of data such as the dialectal Iraqi Arabic scripts.","['Imed Zitouni', 'Ruhi Sarikaya']",July 2009,Computer Speech & Language,"['Arabic diacritic restoration', 'Vowelization', 'Maximum entropy', 'Finite state transducer']",Arabic diacritic restoration approach based on maximum entropy models
637,"This paper presents empirical results of an analysis on the role of prosody in the recognition of dialogue acts and utterance mood in a practical dialogue corpus in Mexican Spanish. The work is configured as a series of machine-learning experimental conditions in which models are created by using intonational and other data as predictors and dialogue act tagging data as targets. We show that utterance mood can be predicted from intonational information, and that this mood information can then be used to recognize the dialogue act.","['Sergio R. Coria', 'Luis A. Pineda']",July 2009,Computer Speech & Language,"['Speech act', 'Dialogue act', 'Intonation', 'INTSINT', 'DAMSL', 'DIME-DAMSL']",An analysis of prosodic information for the recognition of dialogue acts in a multimodal corpus in Mexican Spanish
638,"The aim of this paper is twofold. On the one hand, it attempts to explore several machine learning models for pronoun resolution in Turkish, a language not sufficiently studied with respect to anaphora resolution and rarely being subjected to machine learning experiments. On the other hand, this paper offers an evaluation of the classification performances of the learning models in order to gain insight into the question of how to match a model to the task at hand. In addition to the expected observation that each model should be tuned to an optimum level of expressive power so as to avoid underfitting and overfitting, the results also suggest that non-linear models properly tuned to avoid overfitting outperform linear ones when applied to the data used in our experiments.","['Yılmaz Kılıçaslan', 'Edip Serdar Güner', 'Savaş Yıldırım']",July 2009,Computer Speech & Language,"['Machine learning', 'Pronoun resolution', 'Linear versus non-linear classifiers', 'Expressive power', 'Underfitting', 'Overfitting']",Learning-based pronoun resolution for Turkish with a comparative evaluation
639,"In this paper, we describe RavenClaw, a plan-based, task-independent dialog management framework. RavenClaw isolates the domain-specific aspects of the dialog control logic from domain-independent conversational skills, and in the process facilitates rapid development of mixed-initiative systems operating in complex, task-oriented domains. System developers can focus exclusively on describing the dialog task control logic, while a large number of domain-independent conversational skills such as error handling, timing and turn-taking are transparently supported and enforced by the RavenClaw dialog engine. To date, RavenClaw has been used to construct and deploy a large number of systems, spanning different domains and interaction styles, such as information access, guidance through procedures, command-and-control, medical diagnosis, etc. The framework has easily adapted to all of these domains, indicating a high degree of versatility and scalability.","['Dan Bohus', 'Alexander I. Rudnicky']",July 2009,Computer Speech & Language,"['Dialog management', 'Spoken dialog systems', 'Error handling', 'Focus shifting', 'Mixed-initiative']",The RavenClaw dialog management framework: Architecture and systems
640,"This paper describes in detail a novel approach to the reordering challenge in statistical machine translation (SMT). This Ngram-based reordering (NbR) approach uses the powerful techniques of SMT systems to generate a weighted reordering graph. Thus, statistical criteria reordering constraints are supplied to an SMT system, and this allows an extension to the SMT decoding search.The NbR approach is capable of generalizing reorderings that have been learned during training, through the use of word classes instead of words themselves.Improvement in translation performance is demonstrated with the EPPS task (Spanish and German to English) and the BTEC task (Arabic to English).","['Marta R. Costa-jussà', 'José A.R. Fonollosa']",July 2009,Computer Speech & Language,"['Statistical machine translation', 'Language modeling', 'Word reordering']",An Ngram-based reordering model
641,"The Gaussian mixture model – Universal background model (GMM–UBM) system is one of the predominant approaches for text-independent speaker verification, because both the target speaker model and the impostor model (UBM) have generalization ability to handle “unseen” acoustic patterns. However, since GMM–UBM uses a common anti-model, namely UBM, for all target speakers, it tends to be weak in rejecting impostors’ voices that are similar to the target speaker’s voice. To overcome this limitation, we propose a discriminative feedback adaptation (DFA) framework that reinforces the discriminability between the target speaker model and the anti-model, while preserving the generalization ability of the GMM–UBM approach. This is achieved by adapting the UBM to a target speaker dependent anti-model based on a minimum verification squared-error criterion, rather than estimating the model from scratch by applying the conventional discriminative training schemes. The results of experiments conducted on the NIST2001-SRE database show that DFA substantially improves the performance of the conventional GMM–UBM approach.","['Yi-Hsiang Chao', 'Wei-Ho Tsai', 'Hsin-Min Wang']",July 2009,Computer Speech & Language,"['Discriminative feedback adaptation', 'Log–likelihood ratio', 'Minimum verification squared-error', 'Speaker verification']",Improving GMM–UBM speaker verification using discriminative feedback adaptation
642,"In this paper, we present our recent development of a model-domain environment robust adaptation algorithm, which demonstrates high performance in the standard Aurora 2 speech recognition task. The algorithm consists of two main steps. First, the noise and channel parameters are estimated using multi-sources of information including a nonlinear environment-distortion model in the cepstral domain, the posterior probabilities of all the Gaussians in speech recognizer, and truncated vector Taylor series (VTS) approximation. Second, the estimated noise and channel parameters are used to adapt the static and dynamic portions (delta and delta–delta) of the HMM means and variances. This two-step algorithm enables joint compensation of both additive and convolutive distortions (JAC). The hallmark of our new approach is the use of a nonlinear, phase-sensitive model of acoustic distortion that captures phase asynchrony between clean speech and the mixing noise.In the experimental evaluation using the standard Aurora 2 task, the proposed Phase-JAC/VTS algorithm achieves 93.32% word accuracy using the clean-trained complex HMM backend as the baseline system for the unsupervised model adaptation. This represents high recognition performance on this task without discriminative training of the HMM system. The experimental results show that the phase term, which was missing in all previous HMM adaptation work, contributes significantly to the achieved high recognition accuracy.","['Jinyu Li', 'Li Deng', 'Dong Yu', 'Yifan Gong', 'Alex Acero']",July 2009,Computer Speech & Language,"['Phase-sensitive distortion model', 'Vector Taylor series', 'Joint compensation', 'Additive and convolutive distortions', 'Robust ASR']",A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions
643,"LVCSR systems are usually based on continuous density HMMs, which are typically implemented using Gaussian mixture distributions. Such statistical modeling systems tend to operate slower than real-time, largely because of the heavy computational overhead of the likelihood evaluation. The objective of our research is to investigate approximate methods that can substantially reduce the computational cost in likelihood evaluation without obviously degrading the recognition accuracy. In this paper, the most common techniques to speed up the likelihood computation are classified into three categories, namely machine optimization, model optimization, and algorithm optimization. Each category is surveyed and summarized by describing and analyzing the basic ideas of the corresponding techniques. The distribution of the numerical values of Gaussian mixtures within a GMM model are evaluated and analyzed to show that computations of some Gaussians are unnecessary and can thus be eliminated. Two commonly used techniques for likelihood approximation, namely VQ-based Gaussian selection and partial distance elimination, are analyzed in detail. Based on the analyses, a fast likelihood computation approach called dynamic Gaussian selection (DGS) is proposed. DGS approach is a one-pass search technique which generates a dynamic shortlist of Gaussians for each state during the procedure of likelihood computation. In principle, DGS is an extension of both techniques of partial distance elimination and best mixture prediction, and it does not require additional memory for the storage of Gaussian shortlists. DGS algorithm has been implemented by modifying the likelihood computation procedure in HTK 3.4 system. Experimental results on TIMIT and WSJ0 corpora indicate that this approach can speed up the likelihood computation significantly without introducing apparent additional recognition error.","['Jun Cai', 'Ghazi Bouselmi', 'Yves Laprie', 'Jean-Paul Haton']",April 2009,Computer Speech & Language,"['Gaussian selection', 'Fast likelihood computation', 'Hidden Markov models', 'Speech recognition']",Efficient likelihood evaluation and dynamic Gaussian selection for HMM-based speech recognition
644,"We propose an effective method for clustering unknown speech utterances based on their associated speakers. The method jointly optimizes the generated clusters and the required number of clusters by estimating and minimizing the Rand index. The metric reflects the clustering errors that arise when utterances from the same speaker are placed in different clusters; or when utterances from different speakers are placed in the same cluster. One useful characteristic of the Rand index is that its value only reaches the minimum when the number of clusters is equal to the size of the true speaker population. We approximate the Rand index by a function of the similarity measures between utterances and then use a genetic algorithm to determine the cluster in which each utterance should be located, such that the function is minimized. Our experiment results show that this novel speaker-clustering method outperforms conventional methods that use the Bayesian information criterion to determine the required number of clusters.","['Wei-Ho Tsai', 'Hsin-Min Wang']",April 2009,Computer Speech & Language,"['Genetic algorithm', 'Rand index', 'Speaker clustering']",Evolutionary minimization of the Rand index for speaker clustering
645,"We introduce a strategy for modeling speaker variability in speaker adaptation based on maximum likelihood linear regression (MLLR). The approach uses a speaker-clustering procedure that models speaker variability by partitioning a large corpus of speakers in the eigenspace of their MLLR transformations and learning cluster-specific regression class tree structures. We present experiments showing that choosing the appropriate regression class tree structure for speakers leads to a significant reduction in overall word error rates in automatic speech recognition systems. To realize these gains in unsupervised adaptation, we describe an algorithm that produces a linear combination of MLLR transformations from cluster-specific trees using weights estimated by maximizing the likelihood of a speaker’s adaptation data. This algorithm produces small improvements in overall recognition performance across a range of tasks for both English and Mandarin. More significantly, distributional analysis shows that it reduces the number of speakers with performance loss due to adaptation across a range of adaptation data sizes and word error rates.","['Arindam Mandal', 'Mari Ostendorf', 'Andreas Stolcke']",April 2009,Computer Speech & Language,"['Speech recognition', 'Speaker adaptation', 'Speaker clustering', 'Regression class trees']",Improving robustness of MLLR adaptation with speaker-clustered regression class trees
646,"Sparse coding is an efficient way of coding information. In a sparse code most of the code elements are zero; very few are active. Sparse codes are intended to correspond to the spike trains with which biological neurons communicate. In this article, we show how sparse codes can be used to do continuous speech recognition. We use the TIDIGITS dataset to illustrate the process. First a waveform is transformed into a spectrogram, and a sparse code for the spectrogram is found by means of a linear generative model. The spike train is classified by making use of a spike train model and dynamic programming. It is computationally expensive to find a sparse code. We use an iterative subset selection algorithm with quadratic programming for this process. This algorithm finds a sparse code in reasonable time if the input is limited to a fairly coarse spectral resolution. At this resolution, our system achieves a word error rate of 19%, whereas a system based on Hidden Markov Models achieves a word error rate of 15% at the same resolution.","['W.J. Smit', 'E. Barnard']",April 2009,Computer Speech & Language,"['Sparse coding', 'Spike train', 'Speech recognition', 'Linear generative model']",Continuous speech recognition with sparse coding
647,"This work deals with automatic lexical acquisition and topic discovery from a speech stream. The proposed algorithm builds a lexicon enriched with topic information in three steps: transcription of an audio stream into phone sequences with a speaker- and task-independent phone recogniser, automatic lexical acquisition based on approximate string matching, and hierarchical topic clustering of the lexical entries based on a knowledge-poor co-occurrence approach. The resulting semantic lexicon is then used to automatically cluster the incoming speech stream into topics. The main advantages of this algorithm are its very low computational requirements and its independence to pre-defined linguistic resources, which makes it easy to port to new languages and to adapt to new tasks. It is evaluated both qualitatively and quantitatively on two corpora and on two tasks related to topic clustering. The results of these evaluations are encouraging and outline future directions of research for the proposed algorithm, such as building automatic orthographic labels of the lexical items.",['Christophe Cerisara'],April 2009,Computer Speech & Language,"['Speech processing', 'Topic clustering', 'Lexical acquisition']",Automatic discovery of topics and acoustic morphemes from speech
648,"In this paper we propose models for predicting the intonation for the sequence of syllables present in the utterance. The term intonation refers to the temporal changes of the fundamental frequency (F0). Neural networks are used to capture the implicit intonation knowledge in the sequence of syllables of an utterance. We focus on the development of intonation models for predicting the sequence of fundamental frequency values for a given sequence of syllables. Labeled broadcast news data in the languages Hindi, Telugu and Tamil is used to develop neural network models in order to predict the F0 of syllables in these languages. The input to the neural network consists of a feature vector representing the positional, contextual and phonological constraints. The interaction between duration and intonation constraints can be exploited for improving the accuracy further. From the studies we find that 88% of the F0 values (pitch) of the syllables could be predicted from the models within 15% of the actual F0. The performance of the intonation models is evaluated using objective measures such as average prediction error (μ), standard deviation (σ) and correlation coefficient (γ). The prediction accuracy of the intonation models is further evaluated using listening tests. The prediction performance of the proposed intonation models using neural networks is compared with Classification and Regression Tree (CART) models.","['K. Sreenivasa Rao', 'B. Yegnanarayana']",April 2009,Computer Speech & Language,"['Intonation models', 'Feedforward neural network', 'Prediction accuracy', 'F0 of syllableand Classification and regression tree (CART) models', 'and Classification and regression tree (CART) models']",Intonation modeling for Indian languages
649,,[],January 2009,Computer Speech & Language,[],IFC
650,"Generating pronunciation variants of words is an important subject in speech research and is used extensively in automatic speech recognition and segmentation systems. Decision trees are well known tools in modeling pronunciation over words or sub-word units. In the case of word units and very large vocabulary, in order to train necessary decision trees, a huge amount of speech utterances are required. This training data must contain all of the needed words in the vocabulary with a sufficient number of repetitions for each one. Additionally, an extra corpus is needed for every word which is not included in the original training corpus and may be added to the vocabulary in the future. To overcome these drawbacks, we have designed generalized decision trees, which can be trained using a medium-size corpus over groups of similar words to share information on pronunciation, instead of training a separate tree for every single word. Generalized decision trees predict places in the word where substitution, deletion and insertion of phonemes may occur. After this step, appropriate statistical contextual rules are applied to the permitted places, in order to specifically determine word variants. The hybrids of generalized decision trees and contextual rules are designed in static and dynamic versions. The hybrid static pronunciation models take into account word phonological structures, unigram probabilities, stress and phone context information simultaneously, while the hybrid dynamic models consider an extra feature, speaking rate, to generate pronunciation variants of words. Using the word variants, generated by static and dynamic models, in the lexicon of the SHENAVA Persian continuous speech recognizer, relative word error rate reductions as high as 8.1% and 11.6% are obtained, respectively.","['Bahram Vazirnezhad', 'Farshad Almasganj', 'Seyed Mohammad Ahadi']",January 2009,Computer Speech & Language,"['Pronunciation models', 'Continuous speech recognition', 'Lexicon']",Hybrid statistical pronunciation models designed to be trained by a medium-size corpus
651,"Automatic speech recognition (ASR) has reached a very high level of performance in controlled situations. However, the performance degrades drastically when environmental noise occurs during recognition. Nowadays, the major challenge is to reach a good robustness to adverse conditions. Missing data recognition has been developed to deal with this challenge. Unlike other denoising methods, missing data recognition does not match the whole data with the acoustic models, but instead considers part of the signal as missing, i.e. corrupted by noise. The main challenge of this approach is to identify accurately missing parts (also called masks). The work reported here focuses on this issue. We start from developing Bayesian models of the masks, where every spectral feature is classified as reliable or masked, and is assumed independent of the rest of the signal. This classification strategy results in sparse and isolated masked features, like the squares of a chess-board, while oracle reliable and unreliable features tend to be clustered into consistent time–frequency blocks. We then propose to take into account frequency and temporal dependencies in order to improve the masks’ estimation accuracy. Integrating such dependencies leads to a new architecture of a missing data mask estimator. The proposed classifier has been evaluated on the noisy Aurora2 (digits recognition) and Aurora4 (continuous speech) databases. Experimental results show a significant improvement of recognition accuracy when these dependencies are considered.","['Sébastien Demange', 'Christophe Cerisara', 'Jean-Paul Haton']",January 2009,Computer Speech & Language,"['Automatic speech recognition', 'Missing data recognition', 'Missing data masks estimation', 'Frequency and temporal dependencies']",Missing data mask estimation with frequency and temporal dependencies
652,"Vocal tract length normalization (VTLN) for standard filterbank-based Mel frequency cepstral coefficient (MFCC) features is usually implemented by warping the center frequencies of the Mel filterbank, and the warping factor is estimated using the maximum likelihood score (MLS) criterion. A linear transform (LT) equivalent for frequency warping (FW) would enable more efficient MLS estimation. We recently proposed a novel LT to perform FW for VTLN and model adaptation with standard MFCC features. In this paper, we present the mathematical derivation of the LT and give a compact formula to calculate it for any FW function. We also show that our LT is closely related to different LTs previously proposed for FW with cepstral features, and these LTs for FW are all shown to be numerically almost identical for the sine-log all-pass transform (SLAPT) warping functions. Our formula for the transformation matrix is, however, computationally simpler and, unlike other previous LT approaches to VTLN with MFCC features, no modification of the standard MFCC feature extraction scheme is required. In VTLN and speaker adaptive modeling (SAM) experiments with the DARPA resource management (RM1) database, the performance of the new LT was comparable to that of regular VTLN implemented by warping the Mel filterbank, when the MLS criterion was used for FW estimation. This demonstrates that the approximations involved do not lead to any performance degradation. Performance comparable to front end VTLN was also obtained with LT adaptation of HMM means in the back end, combined with mean bias and variance adaptation according to the maximum likelihood linear regression (MLLR) framework. The FW methods performed significantly better than standard MLLR for very limited adaptation data (1 utterance), and were equally effective with unsupervised parameter estimation. We also performed speaker adaptive training (SAT) with feature space LT denoted CLTFW. Global CLTFW SAT gave results comparable to SAM and VTLN. By estimating multiple CLTFW transforms using a regression tree, and including an additive bias, we obtained significantly improved results compared to VTLN, with increasing adaptation data.","['Sankaran Panchapagesan', 'Abeer Alwan']",January 2009,Computer Speech & Language,"['Automatic speech recognition', 'Speaker normalization', 'VTLN', 'Frequency warping', 'Linear transformation', 'Speaker adaptation']",Frequency warping for VTLN and speaker adaptation by linear transformation of standard MFCC
653,"This paper describes an approach for automatic scoring of pronunciation quality for non-native speech. It is applicable regardless of the foreign language student’s mother tongue. Sentences and words are considered as scoring units. Additionally, mispronunciation and phoneme confusion statistics for the target language phoneme set are derived from human annotations and word level scoring results using a Markov chain model of mispronunciation detection. The proposed methods can be employed for building a part of the scoring module of a system for computer assisted pronunciation training (CAPT). Methods from pattern and speech recognition are applied to develop appropriate feature sets for sentence and word level scoring. Besides features well-known from and approved in previous research, e.g. phoneme accuracy, posterior score, duration score and recognition accuracy, new features such as high-level phoneme confidence measures are identified. The proposed method is evaluated with native English speech, non-native English speech from German, French, Japanese, Indonesian and Chinese adults and non-native speech from German school children. The speech data are annotated with tags for mispronounced words and sentence level ratings by native English teachers. Experimental results show, that the reliability of automatic sentence level scoring by the system is almost as high as the average human evaluator. Furthermore, a good performance for detecting mispronounced words is achieved. In a validation experiment, it could also be verified, that the system gives the highest pronunciation quality scores to 90% of native speakers’ utterances. Automatic error diagnosis based on a automatically derived phoneme mispronunciation statistic showed reasonable results for five non-native speaker groups. The statistics can be exploited in order to provide the non-native feedback on mispronounced phonemes.","['Tobias Cincarek', 'Rainer Gruhn', 'Christian Hacker', 'Elmar Nöth', 'Satoshi Nakamura']",January 2009,Computer Speech & Language,"['Non-native speech', 'Pronunciation assessment', 'Sentence scoring', 'Word scoring', 'Mispronunciation detection', 'Phoneme mispronunciation statistic']",Automatic pronunciation scoring of words and sentences independent from the non-native’s first language
654,"Reading proficiency is a fundamental component of language competency. However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. Existing measures of reading level are not well suited to this task, where students may know some difficult topic-related vocabulary items but not have the same level of sophistication in understanding complex sentence constructions. Recent work in this area has shown the benefit of using statistical language processing techniques. In this paper, we use support vector machines to combine features from n-gram language models, parses, and traditional reading level measures to produce a better method of assessing reading level. We explore the use of negative training data to handle the problem of rejecting data from classes not seen in training, and compare the use of detection vs. regression models on this task. As in many language processing problems, we find substantial variability in human annotation of reading level, and explore ways that multiple human annotations can be used in comparative assessments of system performance.","['Sarah E. Petersen', 'Mari Ostendorf']",January 2009,Computer Speech & Language,"['Reading level assessment', 'SVMs']",A machine learning approach to reading level assessment
655,"We develop a morphosyntax-based natural language watermarking scheme. In this scheme, a text is first transformed into a syntactic tree diagram where the hierarchies and the functional dependencies are made explicit. The watermarking software then operates on the sentences in syntax tree format and executes binary changes under control of Wordnet and Dictionary to avoid semantic drops. A certain level of security is provided via key-controlled randomization of morphosyntactic tools and the insertion of void watermark. The security aspects and payload aspects are evaluated statistically while the imperceptibility is measured using edit-hit counts based on human judgments. It is observed that agglutinative languages are somewhat more amenable to morphosyntax-based natural language watermarking and the free word order property of a language, like Turkish, is an extra bonus.","['Hasan Mesut Meral', 'Bülent Sankur', 'A. Sumru Özsoy', 'Tunga Güngör', 'Emre Sevinç']",January 2009,Computer Speech & Language,"['Natural language watermarking', 'Tree bank', 'Agglutinative', 'Morphosyntax', 'Text payload']",Natural language watermarking via morphosyntactic alterations
656,"This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality, sentence resemblance to the title, sentence inclusion of name entity, sentence inclusion of numerical data, sentence relative length, Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. First, we investigate the effect of each sentence feature on the summarization task. Then we use all features in combination to train genetic algorithm (GA) and mathematical regression (MR) models to obtain a suitable combination of feature weights. Moreover, we use all feature parameters to train feed forward neural network (FFNN), probabilistic neural network (PNN) and Gaussian mixture model (GMM) in order to construct a text summarizer for each model. Furthermore, we use trained models by one language to test summarization performance in the other language. The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles. The results of the proposed approach are promising, especially the GMM approach.","['Mohamed Abdel Fattah', 'Fuji Ren']",January 2009,Computer Speech & Language,"['Automatic summarization', 'Genetic algorithm', 'Mathematical regression', 'Feed forward neural network', 'Probabilistic neural network', 'Gaussian mixture model']","GA, MR, FFNN, PNN and GMM based models for automatic text summarization"
657,,[],January 2009,Computer Speech & Language,[],List of reviewers
658,,[],October 2008,Computer Speech & Language,[],IFC
659,"In this paper two types of redundancy analysis, one based on linear redundancy and the other based on nonlinear entropy-based redundancy, are applied to the OTAGO speech corpus in order to classify the phonemes according to their nonlinearity properties. The redundancy curves are compared both in qualitative as well as in quantitative manner, the latter way involving the generation of two kinds of surrogate data. The results indicate the existence of trivial and dynamic nonlinearities in certain phonemes.","['Jari Turunen', 'Tarmo Lipping']",October 2008,Computer Speech & Language,"['Phoneme analysis', 'Redundancy', 'Entropy measurement']",Phoneme analysis based on quantitative and qualitative entropy measurement
660,"Word sense disambiguation (WSD) is the problem of determining the right sense of a polysemous word in a certain context. This paper investigates the use of unlabeled data for WSD within a framework of semi-supervised learning, in which labeled data is iteratively extended from unlabeled data. Focusing on this approach, we first explicitly identify and analyze three problems inherently occurred piecemeal in the general bootstrapping algorithm; namely the imbalance of training data, the confidence of new labeled examples, and the final classifier generation; all of which will be considered integratedly within a common framework of bootstrapping. We then propose solutions for these problems with the help of classifier combination strategies. This results in several new variants of the general bootstrapping algorithm. Experiments conducted on the English lexical samples of Senseval-2 and Senseval-3 show that the proposed solutions are effective in comparison with previous studies, and significantly improve supervised WSD.","['Anh-Cuong Le', 'Akira Shimazu', 'Van-Nam Huynh', 'Le-Minh Nguyen']",October 2008,Computer Speech & Language,"['Semi-supervised learning', 'Word sense disambiguation', 'Computational linguistics']",Semi-supervised learning integrated with classifier combination for word sense disambiguation
661,"Electropalatography is a well established technique for recording information on the patterns of contact between the tongue and the hard palate during speech, leading to a stream of binary vectors representing contacts or non-contacts between the tongue and certain positions on the hard palate. A data-driven approach to mapping the speech signal onto electropalatographic information is presented. Principal component analysis is used to model the spatial structure of the electropalatographic data and support vector regression is used to map acoustic parameters onto projections of the electropalatographic data on the principal components.","['Asterios Toutios', 'Konstantinos Margaritis']",October 2008,Computer Speech & Language,"['Electropalatography', 'Speech inversion', 'Support vector regression', 'Principal component analysis']",Estimating electropalatographic patterns from the speech signal
662,"Tone-enhanced generalized character posterior probability (GCPP), a generalized form of posterior probability at subword (Chinese character) level, is proposed as a rescoring metric for improving Cantonese LVCSR performance. GCPP is computed by tone score along with the corresponding acoustic and language model scores. The tone score is output from a supra-tone model, which characterizes not only the tone contour of a single syllable but also that of adjacent ones and significantly outperforms other conventional tone models. The search network is constructed first by converting the original word graph to a restructured word graph, then a character graph and finally, a character confusion network (CCN). Based upon tone-enhanced GCPP, the character error rate (CER) is minimized or the GCPP product is maximized over a chosen graph. Experimental results show that the tone-enhanced GCPP can improve character error rate by up to 15.1%, relatively.","['Yao Qian', 'Frank K. Soong', 'Tan Lee']",October 2008,Computer Speech & Language,"['Tone modelling', 'Posterior probability', 'Decoding criterion', 'Cantonese LVCSR']",Tone-enhanced generalized character posterior probability (GCPP) for Cantonese LVCSR
663,"The Default&Refine algorithm is a new rule-based learning algorithm that was developed as an accurate and efficient pronunciation prediction mechanism for speech processing systems. The algorithm exhibits a number of attractive properties including rapid generalisation from small training sets, good asymptotic accuracy, robustness to noise in the training data, and the production of compact rule sets. We describe the Default&Refine algorithm in detail and demonstrate its performance on two benchmarked pronunciation databases (the English OALD and Flemish FONILEX pronunciation dictionaries) as well as a newly-developed Afrikaans pronunciation dictionary. We find that the algorithm learns more efficiently (achieves higher accuracy on smaller data sets) than any of the alternative pronunciation prediction algorithms considered. In addition, we demonstrate the ability of the algorithm to generate an arbitrarily small rule set in such a way that the trade-off between rule set size and accuracy is well controlled. A conceptual comparison with alternative algorithms (including Dynamically Expanding Context, Transformation-Based Learning and Pronunciation by Analogy) clarifies the competitive performance obtained with Default&Refine.","['Marelie Davel', 'Etienne Barnard']",October 2008,Computer Speech & Language,"['Pronunciation prediction', 'Grapheme-to-phoneme prediction', 'Default&Refine', 'Rule extraction']",Pronunciation prediction with Default&Refine
664,"We consider the classification of German news stories as either focusing on future-directed beliefs and intentions or lacking these. The method proposed in this article requires only a small set of labeled training data. Rather, we introduce German clues for the automatic identification of future-orientation which are used for automatic labeling of Reuters news stories. We describe the development of a high-precision procedure for automatic labeling in a bootstrapping fashion: A first version of the labeling procedure uses the absence of clues for future-directedness as indicator for non-future-directedness and is able to automatically label about one-third of the Reuters news stories with high precision. Then a perceptron is applied to the automatically labeled news stories in order to semi-automatically acquire an additional set of clues for non-future-directedness. The second version of the labeling procedure additionally uses these clues and achieves remarkably improved results in terms of recall; it can even be extended by a guessing step to perform classification with an error of 22.5%. We also investigate another way to increase the recall by using the automatically labeled news stories as training data for statistical classifiers. Three different types of statistical classifiers are applied in order to address the question, which classifier is most suited for the text classification task considered. The best statistical classifier combined with the results of improved automatic labeling is able to recognize the two classes of news stories with an error of 19%.","['Judith Eckle-Kohler', 'Michael Kohler', 'Jens Mehnert']",October 2008,Computer Speech & Language,"['Text classification', 'Automatic labeling', 'Lexical-semantic verb classes', 'Boosting', 'Neural networks']",Automatic recognition of German news focusing on future-directed beliefs and intentions
665,"Large-margin discriminative training of hidden Markov models has received significant attention recently. A natural and interesting question is whether the existing discriminative training algorithms can be extended directly to embed the concept of margin. In this paper, we give this question an affirmative answer by showing that the sigmoid bias in the conventional minimum classification error (MCE) training can be interpreted as a soft margin. We justify this claim from a theoretical classification risk minimization perspective where the loss function associated with a non-zero sigmoid bias is shown to include not only empirical error rates but also a margin-bound risk. Based on this perspective, we propose a practical optimization strategy that adjusts the margin (sigmoid bias) incrementally in the MCE training process so that a desirable balance between the empirical error rates on the training set and the margin can be achieved. We call this modified MCE training process large-margin minimum classification error (LM-MCE) training to differentiate it from the conventional MCE. Speech recognition experiments have been carried out on two tasks. First, in the TIDIGITS recognition task, LM-MCE outperforms the state-of-the-art MCE method with 17% relative digit-error reduction and 19% relative string-error reduction. Second, on the Microsoft internal large vocabulary telephony speech recognition task (with 2000 h of training data and 120 K words in the vocabulary), significant recognition accuracy improvement is achieved, demonstrating that our formulation of LM-MCE can be successfully scaled up and applied to large-scale speech recognition tasks.","['Dong Yu', 'Li Deng', 'Xiaodong He', 'Alex Acero']",October 2008,Computer Speech & Language,"['Minimum classification error training', 'Discriminative training', 'Large-margin training', 'Speech recognition', 'Large-scale speech recognition', 'Theoretical classification risk minimization']",Large-margin minimum classification error training: A theoretical risk minimization perspective☆
666,"This paper treats the problem of noise compensation in speech recognition when training and testing conditions do not match. We are interested in two types of non-stationary noise that may be present during test, namely slowly varying and abruptly varying noises. The context of our work is the Stochastic Matching framework. The Stochastic Matching compensation method transforms test data using an affine compensation function whose parameters are computed off-line. Stochastic Matching approaches are interesting since they make little assumptions about the nature and the level of the noise but they are best suited for the compensation of stationary noise.In this paper we propose an original contribution to the Stochastic Matching framework. It is based on an on-line frame-synchronous version of Stochastic Matching method to compensate for slowly varying noise. Our contribution extends this compensation algorithm in order to compensate for abruptly varying noise. The basic idea of the proposed methods is to perform the compensation and the recognition steps at the same time. The environment changes are identified using monitoring algorithms.The performance of our proposed methods is evaluated on two speech databases, one recorded in moving cars (VODIS), and another one obtained by corrupting VODIS with abruptly varying noise from NOISEX. The proposed approaches significantly outperform classical compensation methods (Off-line Stochastic Matching, Sequential Mean Cepstrum Removal, Parallel Model Combination, Spectral Subtraction). For instance, we obtain up to 32.6% word error rate reduction over S-MCR on database corrupted by a 10 dB abruptly varying white noise.","['V. Barreaud', 'I. Illina', 'D. Fohr']",July 2008,Computer Speech & Language,"['Robust speech recognition', 'Stochastic MatchingNon-stationary noise', 'Non-stationary noise']",On-line Stochastic Matching compensation for non-stationary noise
667,"We present a new methodology of user simulation applied to the evaluation and refinement of stochastic dialog systems. Common weaknesses of these systems are the scarceness of the training corpus and the cost of an evaluation made by real users. We have considered the user simulation technique as an alternative way of testing and improving our dialog system. We have developed a new dialog manager that plays the role of the user. This user dialog manager incorporates several knowledge sources, combining statistical and heuristic information in order to define its dialog strategy. Once the user simulator is integrated into the dialog system, it is possible to enhance the dialog models by an automatic strategy learning. We have performed an extensive evaluation, achieving a slight but clear improvement of the dialog system.","['Francisco Torres', 'Emilio Sanchis', 'Encarna Segarra']",July 2008,Computer Speech & Language,"['Dialog systems', 'Stochastic models', 'User simulation', 'Automatic strategy learning', 'Evaluation of dialog models']",User simulation in a stochastic dialog system
668,"In this paper, the use of discriminative linear transforms (DLT) is investigated to construct speaker adaptive speech recognition systems, where a discriminative criterion rather than ML is used for transform parameter estimation. The minimum phone error (MPE) criterion is investigated for DLT estimation, by making use of a so-called weak-sense auxiliary function to derive the estimation formulae. An implementation based on lattices is used for DLT statistics accumulation, where the use of a weakened language model allows more confusion data to be included. To improve DLT estimation for unsupervised adaptation, a method of incorporating word correctness information of the supervision into transform estimation is developed. The confidence scores calculated by confusion network decoding are used to represent the word correctness and weight the numerator statistics during DLT estimation. This makes the DLT estimation less sensitive to errors in the supervision. Experiments on transcription of read newspaper data and on conversational telephone speech transcription have shown the improvements of DLT over MLLR for both supervised and unsupervised adaptation, and the effectiveness of confidence scores for improving both normal and DLT-based MLLR adaptation.","['Lan Wang', 'Philip C. Woodland']",July 2008,Computer Speech & Language,"['Speech recognition', 'Speaker adaptation', 'Discriminative linear transforms']",MPE-based discriminative linear transforms for speaker adaptation
669,"This paper describes the design and evaluation of prosodically-sensitive concatenative units for a Korean text-to-speech (TTS) synthesis system. The diphones used are prosodically conditioned in the sense that a single conventional diphone is stored as different versions taken directly from the different prosodic domains of the prosodically labeled, read sentences. The four levels of the Korean prosodic hierarchy were observed in the diphone selection process, thereby selecting four different versions of each diphone: three edge diphones from the prosodic domains of the intonational phrase (IP), accentual phrase (AP) and prosodic word (PW), and a non-edge diphone from the domain of the prosodic word. Due to the size of the corpus that we employed, our system covers only 36.4% of the 6503 possible diphones. A listening experiment designed to evaluate the quality of the diphone database showed that listeners preferred stimuli composed of prosodically appropriate diphones. We interpret this as supporting the view that segments carry prosodic domain information.",['Kyuchul Yoon'],July 2008,Computer Speech & Language,"['Korean', 'Prosody', 'Diphone', 'Text-to-speech synthesis']",Design and evaluation of prosodically-sensitive concatenative units for a Korean TTS system☆
670,"Active learning (AL) is a framework that attempts to reduce the cost of annotating training material for statistical learning methods. While a lot of papers have been presented on applying AL to natural language processing tasks reporting impressive savings, little work has been done on defining a stopping criterion. In this work, we present a stopping criterion for active learning based on the way instances are selected during uncertainty-based sampling and verify its applicability in a variety of settings. The statistical learning models used in our study are support vector machines (SVMs), maximum entropy models and Bayesian logistic regression and the tasks performed are text classification, named entity recognition and shallow parsing. In addition, we present a method for multiclass mutually exclusive SVM active learning.",['Andreas Vlachos'],July 2008,Computer Speech & Language,"['Active learning', 'SVMs', 'Text classification', 'NER']",A stopping criterion for active learning
671,"All speech produced by humans includes information about the speaker, including conveying the emotional state of the speaker. It is thus desirable to include vocal affect in any synthetic speech where improving the naturalness of the speech produced is important. However, the speech factors which convey affect are poorly understood, and their implementation in synthetic speech systems is not yet commonplace. A prototype system for the production of emotional synthetic speech using a commercial formant synthesiser was developed based on vocal emotion descriptions given in the literature. This paper describes work to improve and augment this system, based on a detailed investigation of emotive material spoken by two actors (one amateur, one professional). The results of this analysis are summarised, and were used to enhance the existing emotion rules used in the speech synthesis system. The enhanced system was evaluated by naive listeners in a perception experiment, and the simulated emotions were found to be more realistic than in the original version of the system.","['Iain R. Murray', 'John L. Arnott']",April 2008,Computer Speech & Language,"['Speech analysis', 'Speech perception', 'Emotion', 'Affect', 'Synthesis-by rule', 'System evaluation']",Applying an analysis of acted vocal emotions to improve the simulation of synthetic speech
672,"In this work, we combine maximum mutual information parameter estimation with speaker-adapted training (SAT). As will be shown, this can be achieved by performing unsupervised estimation of speaker adaptation parameters on the test data, a distinct advantage for many recognition tasks involving conversational speech. We derive re-estimation formulae for the basic speaker-independent means and variances, the optimal regression class for each Gaussian component when multiple speaker-dependent linear transforms are used for adaptation, as well as the optimal feature-space transformation matrix for use with semi-tied covariance matrices. We also propose an approximation to the maximum likelihood and maximum mutual information SAT re-estimation formulae that greatly reduces the amount of disk space required to conduct training on corpora which contain speech from hundreds or thousands of speakers. We also present empirical evidence of the importance of combination speaker adaptation with discriminative training. In particular, on a subset of the data used for the NIST RT05 evaluation, we show that including maximum likelihood linear regression transformations in the MMI re-estimation formulae provides a WER of 35.2% compared with 39.1% obtained when speaker adaptation is ignored during discriminative training.","['John McDonough', 'Matthias Wölfel', 'Emilian Stoimenov']",April 2008,Computer Speech & Language,"['Speech recognition', 'Speaker-adapted training', 'Discriminative training']",On maximum mutual information speaker-adapted training
673,"Spoken language understanding (SLU) addresses the problem of mapping natural language speech to frame structure encoding of its meaning. The statistical sequential labeling method has been successfully applied to SLU tasks; however, most sequential labeling approaches lack long-distance dependency information handling method. In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance of the statistical SLU problem. A method we propose is to use trigger pairs automatically extracted by a feature induction algorithm. We describe a light practical version of the feature inducer for which a simple modification is efficient and successful. We evaluate our method on three SLU tasks and show an improvement of performance over the baseline local model.","['Minwoo Jeong', 'Gary Geunbae Lee']",April 2008,Computer Speech & Language,"['Spoken language understanding', 'Non-local features', 'Long-distance dependency', 'Feature induction']",Practical use of non-local features for statistical spoken language understanding☆
674,"Although speech derived from read texts, news broadcasts, and other similar prepared contexts can be recognized with high accuracy, recognition performance drastically decreases for spontaneous speech. This is due to the fact that spontaneous speech and read speech are significantly different acoustically as well as linguistically. This paper statistically and quantitatively analyzes differences in acoustic features between spontaneous and read speech using two large-scale speech corpora, “Corpus of Spontaneous Japanese (CSJ)” and “Japanese Newspaper Article Sentences (JNAS)”. Experimental results show that spontaneous speech can be characterized by reduced spectral space in comparison with that of read speech, and that the more spontaneous, the more the spectral space shrinks. This paper also clarifies that reduction in the spectral space leads to reduction in phoneme recognition accuracy. This result indicates that spectral reduction is one major reason for the decrease of recognition accuracy in spontaneous speech.","['Masanobu Nakamura', 'Koji Iwano', 'Sadaoki Furui']",April 2008,Computer Speech & Language,"['Corpus of Spontaneous Japanese', 'Spontaneous speech', 'Spectral reduction', 'Mahalanobis distance']",Differences between acoustic characteristics of spontaneous and read speech and their effects on speech recognition performance
675,"A new maximum likelihood training algorithm is proposed that compensates for weaknesses of the EM algorithm by using cross-validation likelihood in the expectation step to avoid overtraining. By using a set of sufficient statistics associated with a partitioning of the training data, as in parallel EM, the algorithm has the same order of computational requirements as the original EM algorithm. Another variation uses an approximation of bagging to reduce variance in the E-step but at a somewhat higher cost. Analyses using GMMs with artificial data show the proposed algorithms are more robust to overtraining than the conventional EM algorithm. Large vocabulary recognition experiments on Mandarin broadcast news data show that the methods make better use of more parameters and give lower recognition error rates than standard EM training.","['Takahiro Shinozaki', 'Mari Ostendorf']",April 2008,Computer Speech & Language,"['EM training', 'Overtraining', 'Cross-validation', 'Sufficient statistics', 'HMM']",Cross-validation and aggregated EM training for robust parameter estimation☆
676,"Synthesis by concatenation of natural speech improves perceptual results when phonemes and syllables are segmented at places where spectral variations are small [Klatt, D., 1987. Review of text-to-speech conversion for English. J. Acoust. Soc. Am 82 (3), 737–793]. An automatic segmentation method is explored here, using a tool based on a combination of Entropy Coding, Multiresolution Analysis, and Kohonen’s Self Organized Maps. The segmentation method considers that there are no limits imposed by any linguistic unit. Resulting waveforms represent phone chains dominated by spectral dynamic structures. Each acoustic unit obtained could be composed of a variety of phonemes or a segmented part of them at the unit boundary. The number of units and unit structure are speaker dependent, i.e. rate, segmental and suprasegmental distinctive features affect them as dynamic structure varies. Results obtained from two databases – one male, one female – of 741 sentences each show this dependence, presenting a different number of units and occurrences for each speaker. Nevertheless, both speakers show a high occurrence of three (36–24%) and four (29–27%) phoneme sequences. Vowel-consonant-vowel sequences are the most frequent type (9.7–8.3%). Consonant-vowel syllables, which are phonemically frequent in Spanish (58%), are less represented (6.6–3.2%) using this method. The relevance of half phone segmentation is verified given that 66% for the female speaker and 53% for the male speaker, of the total units start and end with a segmented phone. Perceptual experiments showed that concatenated speech, created with dynamic acoustic units, was judged more natural than with diphone units.","['H.M. Torres', 'J.A. Gurlekian']",April 2008,Computer Speech & Language,"['Text to speech', 'Unit segmentation', 'Corpus-driven', 'Polyphones']",Acoustic speech unit segmentation for concatenative synthesis
677,"Event entailment is knowledge that may prove useful for a variety of applications dealing with inferencing over events described in natural language texts. In this paper, we propose a method for automatic discovery of pairs of verbs related by entailment, such as X buy Y ⇒ X own Y and appoint X as Y ⇒ X become Y. In contrast to previous approaches that make use of lexico-syntactic patterns and distributional evidence, the underlying assumption of our method is that the implication of one event by another manifests itself in the regular co-occurrence of the two corresponding verbs within locally coherent text. Based on the analogy with the problem of learning selectional preferences Resnik’s [Resnik, P., 1993. Selection and information: a class-based approach to lexical relationships, Ph.D. Thesis, University of Pennsylvania] association strength measure is used to score the extracted verb pairs for asymmetric association in order to discover the direction of entailment in each pair. In our experimental evaluation, we examine the effect that various local discourse indicators produce on the accuracy of this model of entailment. After that we carry out a direct evaluation of the verb pairs against human subjects’ judgements and extrinsically evaluate the pairs on the task of noun phrase coreference resolution.",['Viktor Pekar'],January 2008,Computer Speech & Language,"['Lexical semantics', 'Lexical entailment', 'Local discourse', 'Coreference resolution']",Discovery of event entailment knowledge from text corpora
678,"This article describes a general and powerful approach to modelling mismatch in speaker recognition by including an explicit session term in the Gaussian mixture speaker modelling framework. Under this approach, the Gaussian mixture model (GMM) that best represents the observations of a particular recording is the combination of the true speaker model with an additional session-dependent offset constrained to lie in a low-dimensional subspace representing session variability.A novel and efficient model training procedure is proposed in this work to perform the simultaneous optimisation of the speaker model and session variables required for speaker training. Using a similar iterative approach to the Gauss–Seidel method for solving linear systems, this procedure greatly reduces the memory and computational resources required by a direct solution.Extensive experimentation demonstrates that the explicit session modelling provides up to a 68% reduction in detection cost over a standard GMM-based system and significant improvements over a system utilising feature mapping, and is shown to be effective on the corpora of recent National Institute of Standards and Technology (NIST) Speaker Recognition Evaluations, exhibiting different session mismatch conditions.","['Robbie Vogt', 'Sridha Sridharan']",January 2008,Computer Speech & Language,"['Automatic speaker verification', 'Session variability']",Explicit modelling of session variability for speaker verification
679,"This paper presents a novel prosody model in the context of computer text-to-speech synthesis applications for tone languages. We have demonstrated its applicability using the Standard Yorùbá (SY) language. Our approach is motivated by the theory that abstract and realised forms of various prosody dimensions should be modelled within a modular and unified framework [Coleman, J.S., 1994. Polysyllabic words in the YorkTalk synthesis system. In: Keating, P.A. (Ed.), Phonological Structure and Forms: Papers in Laboratory Phonology III, Cambridge University Press, Cambridge, pp. 293–324]. We have implemented this framework using the Relational Tree (R-Tree) technique. R-Tree is a sophisticated data structure for representing a multi-dimensional waveform in the form of a tree.The underlying assumption of this research is that it is possible to develop a practical prosody model by using appropriate computational tools and techniques which combine acoustic data with an encoding of the phonological and phonetic knowledge provided by experts. To implement the intonation dimension, fuzzy logic based rules were developed using speech data from native speakers of Yorùbá. The Fuzzy Decision Tree (FDT) and the Classification and Regression Tree (CART) techniques were tested in modelling the duration dimension. For practical reasons, we have selected the FDT for implementing the duration dimension of our prosody model.To establish the effectiveness of our prosody model, we have also developed a Stem-ML prosody model for SY. We have performed both quantitative and qualitative evaluations on our implemented prosody models. The results suggest that, although the R-Tree model does not predict the numerical speech prosody data as accurately as the Stem-ML model, it produces synthetic speech prosody with better intelligibility and naturalness. The R-Tree model is particularly suitable for speech prosody modelling for languages with limited language resources and expertise, e.g. African languages. Furthermore, the R-Tree model is easy to implement, interpret and analyse.","['Ọdẹ´túnjí A. Ọdẹ´jọbí', 'Shun Ha Sylvia Wong', 'Anthony J. Beaumont']",January 2008,Computer Speech & Language,"['Speech synthesis', 'Prosody modelling', 'Standard Yorùbá', 'Tone languages', 'Modular holistic model', 'Relational trees']",A modular holistic approach to prosody modelling for Standard Yorùbá speech synthesis
680,"This paper presents a speech enhancement method based on the tracking and denoising of the formants of a linear prediction (LP) model of the spectral envelope of speech and the parameters of a harmonic noise model (HNM) of its excitation. The main advantages of tracking and denoising the prominent energy contours of speech are the efficient use of the spectral and temporal structures of successive speech frames and a mitigation of processing artefact known as the ‘musical noise’ or ‘musical tones’.The formant-tracking linear prediction (FTLP) model estimation consists of three stages: (a) speech pre-cleaning based on a spectral amplitude estimation, (b) formant-tracking across successive speech frames using the Viterbi method, and (c) Kalman filtering of the formant trajectories across successive speech frames.The HNM parameters for the excitation signal comprise; voiced/unvoiced decision, the fundamental frequency, the harmonics’ amplitudes and the variance of the noise component of excitation. A frequency-domain pitch extraction method is proposed that searches for the peak signal to noise ratios (SNRs) at the harmonics. For each speech frame several pitch candidates are calculated. An estimate of the pitch trajectory across successive frames is obtained using a Viterbi decoder. The trajectories of the noisy excitation harmonics across successive speech frames are modeled and denoised using Kalman filters.The proposed method is used to deconstruct noisy speech, de-noise its model parameters and then reconstitute speech from its cleaned parts. Experimental evaluations show the performance gains of the formant tracking, pitch extraction and noise reduction stages.","['Qin Yan', 'Saeed Vaseghi', 'Esfandiar Zavarehei', 'Ben Milner', 'Jonathan Darch', 'Paul White', 'Ioannis Andrianakis']",January 2008,Computer Speech & Language,"['HNM', 'Kalman', 'Formant']",Kalman tracking of linear predictor and harmonic noise models for noisy speech enhancement
681,"Most attempts to provide text-to-speech for modern standard Arabic (MSA) have concentrated on solving the problem of diacritic assignment (i.e. of recovering phonetically relevant information, such as choice of short vowels, which is not explicitly provided in the surface form of MSA). This is clearly a crucial issue: you can hardly produce intelligible spoken output if you do not know what the vowels are.We describe an approach to the task of generating speech from MSA text which not only solves this initial problem, but also provides the information required for imposing an appropriate intonation contour.","['Allan Ramsay', 'Hanady Mansour']",January 2008,Computer Speech & Language,"['Text to speech', 'Modern standard Arabic', 'Prosody']",Towards including prosody in a text-to-speech system for modern standard Arabic
682,,[],January 2008,Computer Speech & Language,[],List of reviewers
683,,[],October 2007,Computer Speech & Language,[],IFC
684,,[],October 2007,Computer Speech & Language,[],Publisher’s note
685,"This paper presents work on developing speech corpora and recognition tools for Turkish by porting SONIC, a speech recognition tool developed initially for English at the Center for Spoken Language Research of the University of Colorado at Boulder. The work presented in this paper had two objectives: The first one is to collect a standard phonetically-balanced Turkish microphone speech corpus for general research use. A 193-speaker triphone-balanced audio corpus and a pronunciation lexicon for Turkish have been developed. The corpus has been accepted for distribution by the Linguistic Data Consortium (LDC) of the University of Pennsylvania in October 2005, and it will serve as a standard corpus for Turkish speech researchers. The second objective was to develop speech recognition tools (a phonetic aligner and a phone recognizer) for Turkish, which provided a starting point for obtaining a multilingual speech recognizer by porting SONIC to Turkish. This part of the work was the first port of this particular recognizer to a language other than English; subsequently, SONIC has been ported to over 15 languages. Using the phonetic aligner developed, the audio corpus has been provided with word, phone and HMM-state level alignments. For the phonetic aligner, it is shown that 92.6% of the automatically labeled phone boundaries are placed within 20 ms of manually labeled locations for the Turkish audio corpus. Finally, a phone recognition error rate of 29.2% is demonstrated for the phone recognizer.","['Özgül Salor', 'Bryan L. Pellom', 'Tolga Ciloglu', 'Mübeccel Demirekler']",October 2007,Computer Speech & Language,"['Phonetic aligner', 'Phone recognizer', 'Language porting', 'Speech corpora', 'Speech recognition']",Turkish speech corpora and recognition tools developed by porting SONIC: Towards multilingual speech recognition
686,"Parallel corpora have become an essential resource for work in multilingual natural language processing. However, sentence aligned parallel corpora are more efficient than non-aligned parallel corpora for cross-language information retrieval and machine translation applications. In this paper, we present two new approaches to align English–Arabic sentences in bilingual parallel corpora based on probabilistic neural network (P-NNT) and Gaussian mixture model (GMM) classifiers. A feature vector is extracted from the text pair under consideration. This vector contains text features such as length, punctuation score, and cognate score values. A set of manually prepared training data was assigned to train the probabilistic neural network and Gaussian mixture model. Another set of data was used for testing. Using the probabilistic neural network and Gaussian mixture model approaches, we could achieve error reduction of 27% and 50%, respectively, over the length based approach when applied on a set of parallel English–Arabic documents. In addition, the results of (P-NNT) and (GMM) outperform the results of the combined model which exploits length, punctuation and cognates in a dynamic framework. The GMM approach outperforms Melamed and Moore’s approaches too. Moreover these new approaches are valid for any languages pair and are quite flexible since the feature vector may contain more, less or different features, such as a lexical matching feature and Hanzi characters in Japanese–Chinese texts, than the ones used in the current research.","['Mohamed Abdel Fattah', 'David B. Bracewell', 'Fuji Ren', 'Shingo Kuroiwa']",October 2007,Computer Speech & Language,"['Sentence alignment', 'English/Arabic parallel corpus', 'Parallel corpora', 'Probabilistic neural network', 'Gaussian mixture model']",Sentence alignment using P-NNT and GMM
687,"Previous partially supervised classification methods can partition unlabeled data into positive examples and negative examples for a given class by learning from positive labeled examples and unlabeled examples, but they cannot further group the negative examples into meaningful clusters even if there are many different classes in the negative examples. Here we proposed an automatic method to obtain a natural partitioning of mixed data (labeled data + unlabeled data) by maximizing a stability criterion defined on classification results from an extended label propagation algorithm over all the possible values of model order (or the number of classes) in mixed data. Our experimental results on benchmark corpora for word sense disambiguation task indicate that this model order identification algorithm with the extended label propagation algorithm as the base classifier outperforms SVM, a one-class partially supervised classification algorithm, and the model order identification algorithm with semi-supervised k-means clustering as the base classifier when labeled data is incomplete.","['Zheng-Yu Niu', 'Dong-Hong Ji', 'Chew Lim Tan']",October 2007,Computer Speech & Language,"['Word sense disambiguation', 'Partially supervised classification', 'Semi-supervised clustering']","Learning model order from labeled and unlabeled data for partially supervised classification, with application to word sense disambiguation"
688,We describe a dynamic Bayesian network for articulatory feature recognition. The model is intended to be a component of a speech recognizer that avoids the problems of conventional “beads-on-a-string” phoneme-based models. We demonstrate that the model gives superior recognition of articulatory features from the speech signal compared with a state-of-the-art neural network system. We also introduce a training algorithm that offers two major advances: it does not require time-aligned feature labels and it allows the model to learn a set of asynchronous feature changes in a data-driven manner.,"['Joe Frankel', 'Mirjam Wester', 'Simon King']",October 2007,Computer Speech & Language,"['Articulatory feature recognition', 'Dynamic Bayesian network', 'Neural network', 'Automatic speech recognition', 'Virtual evidence']",Articulatory feature recognition using dynamic Bayesian networks
689,"This experiment assessed the effect of variation in speech rate on comprehension and persuasiveness of a message presented in text-to-speech (TTS) synthesis to native and non-native listeners. Eighty non-native speakers of English and 80 native speakers of Australian English were randomly assigned to listen to eight banking product descriptions under one of four conditions: normal rate (155 words per minute) with no background noise, normal rate with multi-talker background noise (+6 dB SNR), fast-normal (178 words per minute) with no background noise, and fast-normal with multi-talker background noise. Participants completed comprehension tests and rated each product’s usefulness. A faster rate lowers comprehension for both native and non-native listeners but does not influence the persuasiveness of the message. The findings have implications for the selection of speech rates for persuasive messages delivered to native and non-native listeners using TTS.","['Caroline Jones', 'Lynn Berry', 'Catherine Stevens']",October 2007,Computer Speech & Language,"['Synthetic speech', 'Comprehension', 'Second language', 'Intelligibility', 'Persuasion']",Synthesized speech intelligibility and persuasion: Speech rate and non-native listeners
690,"Most large speech corpora are delivered with a lexicon that contains a canonical transcription of every word in the orthographic transcription. Such a lexicon can be used for generating a hypothetical ‘canonical’ phonetic transcription from the orthography. In addition, time and money permitting, some speech corpora are provided with a manually verified broad phonetic transcription of at least part of the material. Since the manual verification of phonetic transcriptions is time-consuming and expensive, we investigated whether existing automatic transcription procedures and combinations of such procedures can offer a quick and cheap alternative for the generation of phonetic transcriptions like the manually verified transcriptions delivered with large speech corpora. In our study, we used 10 automatic transcription procedures to generate a broad phonetic transcription of well-prepared speech (read-aloud texts) and spontaneous speech (telephone dialogues) from the Spoken Dutch Corpus. The performance was assessed in terms of the number and the nature of the discrepancies between the emerging phonetic transcriptions and the corresponding manually verified phonetic transcriptions delivered with the Spoken Dutch Corpus. Some of the resulting automatic transcriptions appeared to be comparable to the manually verified transcriptions.","['Christophe Van Bael', 'Lou Boves', 'Henk van den Heuvel', 'Helmer Strik']",October 2007,Computer Speech & Language,"['Large speech corpora', 'Automatic phonetic transcription', 'Transcription evaluation']",Automatic phonetic transcription of large speech corpora
691,"Hidden Markov models (HMMs) are the most commonly used acoustic model for speech recognition. In HMMs, the probability of successive observations is assumed independent given the state sequence. This is known as the conditional independence assumption. Consequently, the temporal (inter-frame) correlations are poorly modelled. This limitation may be reduced by incorporating some form of trajectory modelling. In this paper, a general perspective on trajectory modelling is provided, where time-varying model parameters are used for the Gaussian components. A discriminative semi-parametric trajectory model is then described where the Gaussian mean vector and covariance matrix parameters vary with time. The time variation is modelled as a semi-parametric function of the observation sequence via a set of centroids in the acoustic space. The model parameters are estimated discriminatively using the minimum phone error (MPE) criterion. The performance of these models is investigated and benchmarked against a state-of-the-art CUHTK Mandarin evaluation systems.","['K.C. Sim', 'M.J.F. Gales']",October 2007,Computer Speech & Language,"['Speech recognition', 'Trajectory model', 'Discriminative training', 'Minimum phone error']",Discriminative semi-parametric trajectory model for speech recognition
692,,[],October 2007,Computer Speech & Language,[],Subject Index to Volume 21
693,We describe the use of support vector machines (SVMs) for continuous speech recognition by incorporating them in segmental minimum Bayes risk decoding. Lattice cutting is used to convert the Automatic Speech Recognition search space into sequences of smaller recognition problems. SVMs are then trained as discriminative models over each of these problems and used in a rescoring framework. We pose the estimation of a posterior distribution over hypotheses in these regions of acoustic confusion as a logistic regression problem. We also show that GiniSVMs can be used as an approximation technique to estimate the parameters of the logistic regression problem. On a small vocabulary recognition task we show that the use of GiniSVMs can improve the performance of a well trained hidden Markov model system trained under the Maximum Mutual Information criterion. We also find that it is possible to derive reliable confidence scores over the GiniSVM hypotheses and that these can be used to good effect in hypothesis combination. We discuss the problems that we expect to encounter in extending this approach to large vocabulary continuous speech recognition and describe initial investigation of constrained estimation techniques to derive feature spaces for SVMs.,"['Veera Venkataramani', 'Shantanu Chakrabartty', 'William Byrne']",July 2007,Computer Speech & Language,[],Ginisupport vector machines for segmental minimum Bayes risk decoding of continuous speech☆
694,"Automatic speech recognition (ASR) has reached very high levels of performance in controlled situations. However, the performance degrades significantly when environmental noise occurs during the recognition process. Nowadays, the major challenge is to reach a good robustness to adverse conditions, so that automatic speech recognizers can be used in real situations. Missing data theory is a very attractive and promising approach. Unlike other denoising methods, missing data recognition does not match the whole data with the acoustic models, but instead considers part of the signal as missing, i.e. corrupted by noise. While speech recognition with missing data can be handled efficiently by methods such as data imputation or marginalization, accurately identifying missing parts (also called masks) remains a very challenging task. This paper reviews the main approaches that have been proposed to address this problem. The objective of this study is to identify the mask estimation methods that have been proposed so far, and to open this domain up to other related research, which could be adapted to overcome this difficult challenge. In order to restrict the range of methods, only the techniques using a single microphone are considered.","['Christophe Cerisara', 'Sébastien Demange', 'Jean-Paul Haton']",July 2007,Computer Speech & Language,[],On noise masking for automatic missing data speech recognition: A survey and discussion
695,"The paper presents the Position Specific Posterior Lattice (PSPL), a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents. This technique explicitly takes into consideration the content uncertainty by means of using soft-hits. Indexing position information allows one to approximate N-gram expected counts and at the same time use more general proximity features in the relevance score calculation. In fact, one can easily port any state-of-the-art text-retrieval algorithm to the scenario of indexing ASR lattices for spoken documents, rather than using the 1-best recognition result.Experiments performed on a collection of lecture recordings—MIT iCampus database—show that the spoken document ranking performance was improved by 17–26% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer (ASR).The paper also addresses the problem of integrating speech and text content sources for the document search problem, as well as its usefulness from an ad hoc retrieval—keyword search—point of view. In this context, the PSPL formulation is naturally extended to deal with both speech and text content for a given document, where a new relevance ranking framework is proposed for integrating the different sources of information available. Experimental results on the MIT iCampus corpus show a relative improvement of 302% in Mean Average Precision (MAP) when using speech content and text-only metadata as opposed to just text-only metadata (which constitutes about 1% of the amount of data in the transcription of the speech content, measured in number of words). Further experiments show that even in scenarios for which the metadata size is artificially augmented such that it contains more than 10% of the spoken document transcription, the speech content still provides significant performance gains in MAP with respect to only using the text-metadata for relevance ranking.","['Ciprian Chelba', 'Jorge Silva', 'Alex Acero']",July 2007,Computer Speech & Language,[],Soft indexing of speech content for search in spoken documents
696,"In this article, we investigate the performance of a hybrid prediction system with a phrase prediction utility in English word prediction from two viewpoints. From the application user’s point of view, measures of effort savings are important in word prediction. Global performance measures such as the average percentage of keystroke or character savings, however, hide rather than display the details of the functioning of the prediction system as a whole. In the present study, we analysed in detail the performance of a prediction system with a phrase prediction utility along with single word prediction. Our preliminary results with a corpus of 383 lexical bundles show that, from a technological viewpoint, the following three parameters affect the practical utility of the phrase prediction method in a hybrid prediction system: (1) cost of selecting an appropriate prediction mode for single word prediction and phrase prediction; (2) token frequency of phrases in the text predicted, and (3) coverage of the phrasal lexicon. We found that all three affect the phrase prediction performance in different proportions. When the percent of ambiguous search keys finding both phrases and single words is 20%, phrase frequency 35%, and coverage of the phrasal lexicon 98%, the character savings percentage for the whole text will be improved by 6% points under optimal conditions. The system is practically useful as long as an appropriate prediction mode can be selected automatically or the cost of disambiguation of a prediction mode is not too high.","['Pertti Alvar Väyrynen', 'Kai Noponen', 'Tapio Seppänen']",July 2007,Computer Speech & Language,[],Analysing performance in a word prediction system with multiple prediction methods☆
697,"This paper describes the use of a neural network language model for large vocabulary continuous speech recognition. The underlying idea of this approach is to attack the data sparseness problem by performing the language model probability estimation in a continuous space. Highly efficient learning algorithms are described that enable the use of training corpora of several hundred million words. It is also shown that this approach can be incorporated into a large vocabulary continuous speech recognizer using a lattice rescoring framework at a very low additional processing time. The neural network language model was thoroughly evaluated in a state-of-the-art large vocabulary continuous speech recognizer for several international benchmark tasks, in particular the Nist evaluations on broadcast news and conversational speech recognition. The new approach is compared to four-gram back-off language models trained with modified Kneser–Ney smoothing which has often been reported to be the best known smoothing method. Usually the neural network language model is interpolated with the back-off language model. In that way, consistent word error rate reductions for all considered tasks and languages were achieved, ranging from 0.4% to almost 1% absolute.",['Holger Schwenk'],July 2007,Computer Speech & Language,[],Continuous space language models☆
698,"Determining the position of breaks in a sentence is a key task for a text-to-speech system. A synthesized sentence containing incorrect breaks at best requires increased listening effort, and at worst, may have lower intelligibility and different semantics from a correctly phrased sentence. In addition, the position of breaks must be known before other components of the sentence’s prosodic structure can be determined. We consider here some methods for phrase break prediction in which the whole sentence is analysed, in contrast to most previous work which has focused on analysing an area around an individual juncture. One of the main features we use is part-of-speech tags. First, we report an algorithm that reduces the number of tags in the tagset whilst improving break prediction accuracy. We then describe three approaches to break prediction: by analogy, in which we find the best-matching sentence in our training data to the unseen sentence; by phrase modelling, in which we build stochastic models of phrases and use these, together with a “phrase grammar”, to segment the unseen sentence; and finally, using features derived from a syntactic parse of the sentence. All techniques achieve well above our baseline performance, which used punctuation symbols to determine break positions, and performance increased with each successive technique. Our best result, obtained on the MARSEC corpus and using a combination of parse tree derived features and a local feature, gave an F score of 81.6%, which we believe to be the highest published on this dataset.","['Ian Read', 'Stephen Cox']",July 2007,Computer Speech & Language,"['Text-to-speech', 'Prosody', 'Phrase breaks']",Stochastic and syntactic techniques for predicting phrase breaks
699,"This paper presents a formant tracking linear prediction (LP) model for speech processing in noise. The main focus of this work is on the utilization of the correlation of the energy contours of speech, along the formant tracks, for improved formant and LP model estimation in noise. The approach proposed in this paper provides a systematic framework for modelling and utilization of the inter-frame correlation of speech parameters across successive speech frames; the within frame correlations are modelled by the LP parameters. The formant tracking LP model estimation is composed of three stages: (1) a pre-cleaning spectral amplitude estimation stage where an initial estimate of the LP model of speech for each frame is obtained, (2) a formant classification and estimation stage using probability models of formants and Viterbi-decoders and (3) an inter-frame formant de-noising and smoothing stage where Kalman filters are used to model the formant trajectories and reduce the effect of residue noise on formants. The adverse effects of car and train noise on estimates of formant tracks and LP models are investigated. The evaluation results for the estimation of the formant tracking LP model demonstrate that the proposed combination of the initial noise reduction stage with formant tracking and Kalman smoothing stages, results in a significant reduction in errors and distortions.","['Qin Yan', 'Saeed Vaseghi', 'Esfandiar Zavarehei', 'Ben Milner', 'Jonathan Darch', 'Paul White', 'Ioannis Andrianakis']",July 2007,Computer Speech & Language,[],Formant tracking linear prediction model using HMMs and Kalman filters for noisy speech processing
700,"Here we seek to understand the similarities and differences between two speech recognition approaches, namely the HMM/ANN hybrid and the posterior-based segmental model. Both these techniques create local posterior probability estimates and combine these estimates into global posteriors – but they are built on somewhat different concepts and mathematical derivations. The HMM/ANN hybrid combines the local estimates via a formulation that is inherited from the generative HMM concept, while the components of the segment-based model correspond quite directly to the two subtasks of phonetic decoding: segmentation and classification. In this paper we attempt to identify the corresponding components of the segmental model within the hybrid model, with the intent of gaining an insight from this unusual point of view. As regards one of these components, the segment-based phone posteriors, we show that the independence-based product rule combination applied in the hybrid produces strongly biased estimates. As for the other component, the segmentation probability factor, we argue that it is present in the hybrid thanks to the bias of the product rule – that is, the product rule goes wrong in such a special way that it helps the model find the best segmentation of the input. To prove this assertion, we combine this bias with the posterior estimates obtained by averaging, and find that the resulting ‘averaging hybrid’ slightly outperforms the standard one on a phone recognition task and a word recognition task as well. Overall we conclude that the contribution of the product rule to the decoding process is just as important for the segmentation subtask as it is for the segment classification subtask.","['László Tóth', 'András Kocsor']",July 2007,Computer Speech & Language,[],A segment-based interpretation of HMM/ANN hybrids
701,,[],July 2007,Computer Speech & Language,[],Call for papers
702,"In speaker verification over public telephone networks, utterances can be obtained from different types of handsets. Different handsets may introduce different degrees of distortion to the speech signals. This paper attempts to combine a handset selector with (1) handset-specific transformations, (2) reinforced learning, and (3) stochastic feature transformation to reduce the effect caused by the acoustic distortion. Specifically, during training, the clean speaker models and background models are firstly transformed by MLLR-based handset-specific transformations using a small amount of distorted speech data. Then reinforced learning is applied to adapt the transformed models to handset-dependent speaker models and handset-dependent background models using stochastically transformed speaker patterns. During a verification session, a GMM-based handset classifier is used to identify the most likely handset used by the claimant; then the corresponding handset-dependent speaker and background model pairs are used for verification. Experimental results based on 150 speakers of the HTIMIT corpus show that environment adaptation based on the combination of MLLR, reinforced learning and feature transformation outperforms CMS, Hnorm, Tnorm, and speaker model synthesis.","['K.K. Yiu', 'M.W. Mak', 'S.Y. Kung']",April 2007,Computer Speech & Language,[],Environment adaptation for robust speaker verification by cascading maximum likelihood linear regression and reinforced learning☆
703,"In this paper, an improved method of model complexity selection for nonnative speech recognition is proposed by using maximum a posteriori (MAP) estimation of bias distributions. An algorithm is described for estimating hyper-parameters of the priors of the bias distributions, and an automatic accent classification algorithm is also proposed for integration with dynamic model selection and adaptation. Experiments were performed on the WSJ1 task with American English speech, British accented speech, and mandarin Chinese accented speech. Results show that the use of prior knowledge of accents enabled more reliable estimation of bias distributions with very small amounts of adaptation speech, or without adaptation speech. Recognition results show that the new approach is superior to the previous maximum expected likelihood (MEL) method, especially when adaptation data are very limited.","['Xiaodong He', 'Yunxin Zhao']",April 2007,Computer Speech & Language,"['Nonnative speech recognition', 'Model selection', 'Maximum expected likelihood', 'Accent classification', 'Maximum a posteriori estimation']",Prior knowledge guided maximum expected likelihood based model selection and adaptation for nonnative speech recognition
704,"This paper presents an application of the common vector approach (CVA), an approach mainly used for speech recognition problems when the number of data items exceeds the dimension of the feature vectors. The calculation of a unique common vector for each class involves the use of principal component analysis. CVA and other subspace methods are compared both theoretically and experimentally. TI-digit database is used in the experimental study to show the practical use of CVA for the isolated word recognition problems. It can be concluded that CVA results are higher in terms of recognition rates when compared with those of other subspace methods in training and test sets. It is also seen that the consideration of only within-class scatter in CVA gives better performance than considering both within- and between-class scatters in Fisher’s linear discriminant analysis. The recognition rates obtained for CVA are also better than those obtained with the HMM method.","['M. Bilginer Gülmezoğlu', 'Vakıf Dzhafarov', 'Rifat Edizkan', 'Atalay Barkana']",April 2007,Computer Speech & Language,[],The common vector approach and its comparison with other subspace methods in case of sufficient data
705,"In this paper, we propose a neural network model for predicting the durations of syllables. A four layer feedforward neural network trained with backpropagation algorithm is used for modeling the duration knowledge of syllables. Broadcast news data in three Indian languages Hindi, Telugu and Tamil is used for this study. The input to the neural network consists of a set of features extracted from the text. These features correspond to phonological, positional and contextual information. The relative importance of the positional and contextual features is examined separately. For improving the accuracy of prediction, further processing is done on the predicted values of the durations. We also propose a two-stage duration model for improving the accuracy of prediction. From the studies we find that 85% of the syllable durations could be predicted from the models within 25% of the actual duration. The performance of the duration models is evaluated using objective measures such as average prediction error (μ), standard deviation (σ) and correlation coefficient (γ).","['K. Sreenivasa Rao', 'B. Yegnanarayana']",April 2007,Computer Speech & Language,[],Modeling durations of syllables using neural networks
706,"When users access information from text, they engage in strategic fixation, visually scanning the text to focus on regions of interest. However, because speech is both serial and ephemeral, it does not readily support strategic fixation. This paper describes two design principles, indexing and transcript-centric access that address the problem of speech access by supporting strategic fixation. Indexing involves users constructing external visual indices into speech. Users visually scan these indices to find information-rich regions of speech for more detailed processing and playback. Transcription involves transcribing speech using automatic speech recognition (ASR) and enriching that transcription with visual cues. The resulting enriched transcript is time-aligned to the original speech, allowing users to scan the transcript as a whole or the additional visual cues present in the transcript, to fixate and play regions of interest.We tested the effectiveness of these two approaches on a set of reference tasks derived from observations of current voicemail practice. A field trial evaluation of JotMail, an indexed-based interface similar to commercial unified messaging clients, showed that our approaches were effective in supporting speech scanning, information extraction and status tracking, but not archive management. However, users found it onerous to take manual notes with JotMail to provide effective retrieval indices. We therefore built SCANMail, a transcript-based interface that constructs indices automatically, using ASR to generate a transcript of the speech data. SCANMail also uses information extraction techniques to identify regions of potential interest, e.g. telephone numbers, within the transcript. Laboratory and field trials showed that SCANMail overcame most of the problems users reported with JotMail, supporting scanning, information extraction and archiving. Importantly, our evaluations showed that, despite errors, ASR transcripts provide a highly effective tool for browsing. Users exploited the enriched transcript to determine the gist of the underlying speech, and as a guide to identifying areas of speech that it was critical for them to play. Long-term field trials also showed the utility of transcripts to support notification and mobile access.","['Steve Whittaker', 'Julia Hirschberg']",April 2007,Computer Speech & Language,[],Accessing speech data using strategic fixation
707,"In this paper, we present syllable-based duration modelling in the context of a prosody model for Standard Yorùbá (SY) text-to-speech (TTS) synthesis applications. Our prosody model is conceptualised around a modular holistic framework. This framework is implemented using the Relational Tree (R-Tree) techniques. An important feature of our R-Tree framework is its flexibility in that it facilitates the independent implementation of the different dimensions of prosody, i.e. duration, intonation, and intensity, using different techniques and their subsequent integration. We applied the Fuzzy Decision Tree (FDT) technique to model the duration dimension. In order to evaluate the effectiveness of FDT in duration modelling, we have also developed a Classification And Regression Tree (CART) based duration model using the same speech data. Each of these models was integrated into our R-Tree based prosody model.We performed both quantitative (i.e. Root Mean Square Error (RMSE) and Correlation (Corr)) and qualitative (i.e. intelligibility and naturalness) evaluations on the two duration models. The results show that CART models the training data more accurately than FDT. The FDT model, however, shows a better ability to extrapolate from the training data since it achieved a better accuracy for the test data set. Our qualitative evaluation results show that our FDT model produces synthesised speech that is perceived to be more natural than our CART model. In addition, we also observed that the expressiveness of FDT is much better than that of CART. That is because the representation in FDT is not restricted to a set of piece-wise or discrete constant approximation. We, therefore, conclude that the FDT approach is a practical approach for duration modelling in SY TTS applications.","['Ọdẹ´túnjí A. Ọdẹ´jọbí', 'Shun Ha Sylvia Wong', 'Anthony J. Beaumont']",April 2007,Computer Speech & Language,[],A fuzzy decision tree-based duration model for Standard Yorùbá text-to-speech synthesis
708,"We describe methods for improving the performance of statistical machine translation (SMT) between four linguistically different languages, i.e., Chinese, English, Japanese, and Korean by using morphosyntactic knowledge. For the purpose of reducing the translation ambiguities and generating grammatically correct and fluent translation output, we address the use of shallow linguistic knowledge, that is: (1) enriching a word with its morphosyntactic features, (2) obtaining shallow linguistically-motivated phrase pairs, (3) iteratively refining word alignment using filtered phrase pairs, and (4) building a language model from morphosyntactically enriched words. Previous studies reported that the introduction of syntactic features into SMT models resulted in only a slight improvement in performance in spite of the heavy computational expense, however, this study demonstrates the effectiveness of morphosyntactic features, when reliable, discriminative features are used. Our experimental results show that word representations that incorporate morphosyntactic features significantly improve the performance of the translation model and language model. Moreover, we show that refining the word alignment using fine-grained phrase pairs is effective in improving system performance.","['Young-Sook Hwang', 'Andrew Finch', 'Yutaka Sasaki']",April 2007,Computer Speech & Language,[],Improving statistical machine translation using shallow linguistic knowledge
709,"This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on maximizing the regularized conditional log-likelihood. The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data. We describe a method based on regularized likelihood that makes use of the feature set given by the perceptron algorithm, and initialization with the perceptron’s weights; this method gives an additional 0.5% reduction in word error rate (WER) over training with the perceptron alone. The final system achieves a 1.8% absolute reduction in WER for a baseline first-pass recognition system (from 39.2% to 37.4%), and a 0.9% absolute reduction in WER for a multi-pass recognition system (from 28.9% to 28.0%).","['Brian Roark', 'Murat Saraclar', 'Michael Collins']",April 2007,Computer Speech & Language,[],Discriminative n-gram language modeling☆
710,"In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.","['Jason D. Williams', 'Steve Young']",April 2007,Computer Speech & Language,"['Spoken dialog system', 'Dialog management', 'Planning under uncertainty', 'User modelling', 'Markov decision processes', 'Decision theory']",Partially observable Markov decision processes for spoken dialog systems
711,"Recently, minimum perfect hashing (MPH)-based language model (LM) lookup methods have been proposed for fast access of N-gram LM scores in lexical-tree based LVCSR (large vocabulary continuous speech recognition) decoding. Methods of node-based LM cache and LM context pre-computing (LMCP) have also been proposed to combine with MPH for further reduction of LM lookup time. Although these methods are effective, LM lookup still takes a large share of overall decoding time when trigram LM lookahead (LMLA) is used for lower word error rate than unigram or bigram LMLAs. Besides computation time, memory cost is also an important performance aspect of decoding systems. Most speedup methods for LM lookup obtain higher speed at the cost of increased memory demand, which makes system performance unpredictable when running on computers with smaller memory capacities. In this paper, an order-preserving LM context pre-computing (OPCP) method is proposed to achieve both fast speed and small memory cost in LM lookup. By reducing hashing operations through order-preserving access of LM scores, OPCP cuts down LM lookup time effectively. In the meantime, OPCP significantly reduces memory cost because of reduced size of hashing keys and the need for only last word index of each N-gram in LM storage. Experimental results are reported on two LVCSR tasks (Wall Street Journal 20K and Switchboard 33K) with three sizes of trigram LMs (small, medium, large). In comparison with above-mentioned existing methods, OPCP reduced LM lookup time from about 30–80% of total decoding time to about 8–14%, without any increase of word error rate. Except for the small LM, the total memory cost of OPCP for LM lookup and storage was about the same or less than the original N-gram LM storage, much less than the compared methods. The time and memory savings in LM lookup by using OPCP became more pronounced with the increase of LM size.","['Xiaolong Li', 'Yunxin Zhao']",January 2007,Computer Speech & Language,[],A fast and memory-efficient N-gram language model lookup method for large vocabulary continuous speech recognition
712,"In the present paper, we investigate the validity and reliability of de-facto evaluation standards, defined for measuring or predicting the quality of the interaction with spoken dialogue systems. Two experiments have been carried out with a dialogue system for controlling domestic devices. During these experiments, subjective judgments of quality have been collected by two questionnaire methods (ITU-T Rec. P.851 and SASSI), and parameters describing the interaction have been logged and annotated. Both metrics served the derivation of prediction models according to the PARADISE approach. Although the limited database allows only tentative conclusions to be drawn, the results suggest that both questionnaire methods provide valid measurements of a large number of different quality aspects; most of the perceptive dimensions underlying the subjective judgments can also be measured with a high reliability. The extracted parameters mainly describe quality aspects which are directly linked to the system, environmental and task characteristics. Used as an input to prediction models, the parameters provide helpful information for system design and optimization, but not general predictions of system usability and acceptability.","['Sebastian Möller', 'Paula Smeele', 'Heleen Boland', 'Jan Krebber']",January 2007,Computer Speech & Language,[],Evaluating spoken dialogue systems according to de-facto standards: A case study
713,"Humans are able to recognise a word before its acoustic realisation is complete. This in contrast to conventional automatic speech recognition (ASR) systems, which compute the likelihood of a number of hypothesised word sequences, and identify the words that were recognised on the basis of a trace back of the hypothesis with the highest eventual score, in order to maximise efficiency and performance. In the present paper, we present an ASR system, SpeM, based on principles known from the field of human word recognition that is able to model the human capability of ‘early recognition’ by computing word activation scores (based on negative log likelihood scores) during the speech recognition process.Experiments on 1463 polysyllabic words in 885 utterances showed that 64.0% (936) of these polysyllabic words were recognised correctly at the end of the utterance. For 81.1% of the 936 correctly recognised polysyllabic words the local word activation allowed us to identify the word before its last phone was available, and 64.1% of those words were already identified one phone after their lexical uniqueness point.We investigated two types of predictors for deciding whether a word is considered as recognised before the end of its acoustic realisation. The first type is related to the absolute and relative values of the word activation, which trade false acceptances for false rejections. The second type of predictor is related to the number of phones of the word that have already been processed and the number of phones that remain until the end of the word. The results showed that SpeM’s performance increases if the amount of acoustic evidence in support of a word increases and the risk of future mismatches decreases.","['Odette Scharenborg', 'Louis ten Bosch', 'Lou Boves']",January 2007,Computer Speech & Language,[],‘Early recognition’ of polysyllabic words in continuous speech
714,"A novel speaker-adaptive learning algorithm is developed and evaluated for a hidden trajectory model of speech coarticulation and reduction. Central to this model is the process of bi-directional (forward and backward) filtering of the vocal tract resonance (VTR) target sequence. The VTR targets are key parameters of the model that control the hidden VTR’s dynamic behavior and the subsequent acoustic properties (those of the cepstral vector sequence). We describe two techniques for training these target parameters: (1) speaker-independent training that averages out the target variability over all speakers in the training set; and (2) speaker-adaptive training that takes into account the variability in the target values among individual speakers. The adaptive learning is applied also to adjust each unknown test speaker’s target values towards their true values. All the learning algorithms make use of the results of accurate VTR tracking as developed in our earlier work. In this paper, we present details of the learning algorithms and the analysis results comparing speaker-independent and speaker-adaptive learning. We also describe TIMIT phone recognition experiments and results, demonstrating consistent superiority of speaker adaptive learning over speaker-independent one measured by the phonetic recognition performance.","['Dong Yu', 'Li Deng', 'Alex Acero']",January 2007,Computer Speech & Language,[],Speaker-adaptive learning of resonance targets in a hidden trajectory model of speech coarticulation
715,"In this paper, we introduce the backoff hierarchical class n-gram language models to better estimate the likelihood of unseen n-gram events. This multi-level class hierarchy language modeling approach generalizes the well-known backoff n-gram language modeling technique. It uses a class hierarchy to define word contexts. Each node in the hierarchy is a class that contains all the words of its descendant nodes. The closer a node to the root, the more general the class (and context) is. We investigate the effectiveness of the approach to model unseen events in speech recognition. Our results illustrate that the proposed technique outperforms backoff n-gram language models. We also study the effect of the vocabulary size and the depth of the class hierarchy on the performance of the approach. Results are presented on Wall Street Journal (WSJ) corpus using two vocabulary set: 5000 words and 20,000 words. Experiments with 5000 word vocabulary, which contain a small numbers of unseen events in the test set, show up to 10% improvement of the unseen event perplexity when using the hierarchical class n-gram language models. With a vocabulary of 20,000 words, characterized by a larger number of unseen events, the perplexity of unseen events decreases by 26%, while the word error rate (WER) decreases by 12% when using the hierarchical approach. Our results suggest that the largest gains in performance are obtained when the test set contains a large number of unseen events.",['Imed Zitouni'],January 2007,Computer Speech & Language,[],Backoff hierarchical class n-gram language models: effectiveness to model unseen events in speech recognition
716,"Language modeling is the problem of predicting words based on histories containing words already hypothesized. Two key aspects of language modeling are effective history equivalence classification and robust probability estimation. The solution of these aspects is hindered by the data sparseness problem.Application of random forests (RFs) to language modeling deals with the two aspects simultaneously. We develop a new smoothing technique based on randomly grown decision trees (DTs) and apply the resulting RF language models to automatic speech recognition. This new method is complementary to many existing ones dealing with the data sparseness problem. We study our RF approach in the context of n-gram type language modeling in which n − 1 words are present in a history. Unlike regular n-gram language models, RF language models have the potential to generalize well to unseen data, even when histories are longer than four words. We show that our RF language models are superior to the best known smoothing technique, the interpolated Kneser–Ney smoothing, in reducing both the perplexity (PPL) and word error rate (WER) in large vocabulary state-of-the-art speech recognition systems. In particular, we will show statistically significant improvements in a contemporary conversational telephony speech recognition system by applying the RF approach only to one of its many language models.","['Peng Xu', 'Frederick Jelinek']",January 2007,Computer Speech & Language,[],Random forests and the data sparseness problem in language modeling
717,"In the present paper, a trajectory model, derived from a hidden Markov model (HMM) by imposing explicit relationships between static and dynamic feature vector sequences, is developed and evaluated. The derived model, named a trajectory HMM, can alleviate two limitations of the standard HMM, which are (i) piece-wise constant statistics within a state and (ii) conditional independence assumption of state output probabilities, without increasing the number of model parameters. In the present paper, a Viterbi-type training algorithm based on the maximum likelihood criterion is also derived. The performance of the trajectory HMM was evaluated both in speech recognition and synthesis. In a speaker-dependent continuous speech recognition experiment, the trajectory HMM achieved an error reduction over the corresponding standard HMM. Subjective listening test results showed that the introduction of the trajectory HMM improved the naturalness of synthetic speech.","['Heiga Zen', 'Keiichi Tokuda', 'Tadashi Kitamura']",January 2007,Computer Speech & Language,[],Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic feature vector sequences
718,"In this paper, we consider the topic of iterative, one dimensional, signal reconstruction (specifically speech signals) from the magnitude spectrum and the phase spectrum. While this topic has been extensively researched and documented, we wish to recast some well-established results for the benefit of new researchers and those who desire a short, yet comprehensive, review of the subject. The three main points of the review are: (i) a signal can be reconstructed to within a scale factor from its phase spectrum, (ii) a signal cannot be reconstructed to within a scale factor from its magnitude spectrum, and (iii) a signal can be reconstructed to within a scale factor from its magnitude spectrum when the phase-sign (i.e., one bit of phase spectrum information) is known. Through a number of illustrative examples, we first demonstrate how the algorithms work when the spectral information is determined over the entire duration of the signal. We then demonstrate that the algorithms are equally valid for reconstruction of a signal from the spectra obtained from short-time segments. In addition, we present the results of some further experimentation in which we have attempted to reconstruct a speech signal from only partial phase spectrum information (in the absence of all magnitude spectrum information). We make the following observations: (i) intelligible signal reconstruction (albeit noisy) is possible from knowledge of only the phase spectrum sign information, (ii) an intelligible signal cannot be reconstructed from knowledge of only the phase spectrum frequency-derivative or only the phase spectrum time-derivative, and (iii) an intelligible signal can be reconstructed from the combined knowledge of both the phase spectrum frequency-derivative and time-derivative.","['Leigh D. Alsteris', 'Kuldip K. Paliwal']",January 2007,Computer Speech & Language,[],Iterative reconstruction of speech from short-time Fourier transform phase and magnitude spectra
719,"In this paper, a set of features derived by filtering and spectral peak extraction in autocorrelation domain are proposed. We focus on the effect of the additive noise on speech recognition. Assuming that the channel characteristics and additive noises are stationary, these new features improve the robustness of speech recognition in noisy conditions. In this approach, initially, the autocorrelation sequence of a speech signal frame is computed. Filtering of the autocorrelation of speech signal is carried out in the second step, and then, the short-time power spectrum of speech is obtained from the speech signal through the fast Fourier transform. The power spectrum peaks are then calculated by differentiating the power spectrum with respect to frequency. The magnitudes of these peaks are then projected onto the mel-scale and pass the filter bank. Finally, a set of cepstral coefficients are derived from the outputs of the filter bank. The effectiveness of the new features for speech recognition in noisy conditions will be shown in this paper through a number of speech recognition experiments.A task of multi-speaker isolated-word recognition and another one of multi-speaker continuous speech recognition with various artificially added noises such as factory, babble, car and F16 were used in these experiments. Also, a set of experiments were carried out on Aurora 2 task. Experimental results show significant improvements under noisy conditions in comparison to the results obtained using traditional feature extraction methods. We have also reported the results obtained by applying cepstral mean normalization on the methods to get robust features against both additive noise and channel distortion.","['G. Farahani', 'S.M. Ahadi', 'M.M. Homayounpour']",January 2007,Computer Speech & Language,[],Features based on filtering and spectral peaks in autocorrelation domain for robust speech recognition
720,"This paper addresses the problem of recognizing a vocabulary of over 50,000 city names in a telephone access spoken dialogue system. We adopt a two-stage framework in which only major cities are represented in the first stage lexicon. We rely on an unknown word model encoded as a phone loop to detect OOV city names (referred to as ‘rare city’ names). We use SpeM, a tool that can extract words and word-initial cohorts from phone graphs from a large fallback lexicon, to provide an N-best list of promising city name hypotheses on the basis of the phone graph corresponding to the OOV. This N-best list is then inserted into the second stage lexicon for a subsequent recognition pass.Experiments were conducted on a set of spontaneous telephone-quality utterances; each containing one rare city name. It appeared that SpeM was able to include nearly 75% of the correct city names in an N-best hypothesis list of 3000 city names. With the names found by SpeM to extend the lexicon of the second stage recognizer, a word accuracy of 77.3% could be obtained. The best one-stage system yielded a word accuracy of 72.6%. The absolute number of correctly recognized rare city names almost doubled, from 62 for the best one-stage system to 102 for the best two-stage system. However, even the best two-stage system recognized only about one-third of the rare city names retrieved by SpeM. The paper discusses ways for improving the overall performance in the context of an application.","['Odette Scharenborg', 'Stephanie Seneff', 'Lou Boves']",January 2007,Computer Speech & Language,[],A two-pass approach for handling out-of-vocabulary words in a large vocabulary recognition task
721,"In this study we present various techniques to evaluate the pronunciation of students of a foreign language without any knowledge of the uttered text. Previous attempts have shown that it is feasible to evaluate the pronunciation of a non-native speaker by having implicit or explicit knowledge of the uttered text, provided that enough utterances are available. Our approach is to use characteristics of the mother tongue (SOURCE language) of the speaker in the evaluation of his/her pronunciation. We recorded 20 Greek students speaking English (TARGET language) and evaluated their pronunciation using algorithms that include characteristics of the SOURCE language (Greek). We show that the pronunciation scores that are based on both TARGET- and SOURCE-language characteristics have better correlation with the human scores than those based only on characteristics of the TARGET language. As in previous studies, we found that the best-performing algorithms for automatic evaluation of pronunciation are based on speech recognition technology.","['N. Moustroufas', 'V. Digalakis']",January 2007,Computer Speech & Language,[],Automatic pronunciation evaluation of foreign speakers using unknown text
722,"This paper describes a hand-written rule-based grapheme-to-phoneme (GTP) conversion system for Korean built within the Festival text-to-speech (TTS) synthesis framework. The core of the GTP conversion system is a simple implementation of nine linguistically motivated morphophonological rules. These rules, which are well known to students of Korean linguistics, were implemented in Festival rewrite formalism, and were applied to 1.3 million distinct orthographic words (space-delimited eojeols) from the Korean Newswire corpus. The outputs were evaluated against a representative subset of eojeols. The subset was examined by three native speakers of Korean, who judged 91.17% of the word types in a stratified sample of Korean eojeols to be acceptable pronunciations, which means that our system converted 99.63% of the grapheme tokens correctly. This performance is comparable to that obtained from earlier studies such as Kim et al. [Morpheme-based grapheme to phoneme conversion using phonetic patterns and morphophonemic connectivity information. ACM Transactions on Asian Language Information Processing 1 (1) (2002) 65–82] which, contrary to our system, used an elaborate morphological analysis module. This is evidence of the potential benefit of well-abstracted linguistic knowledge. In addition, because our approach is based on well-known linguistic principles, error analysis is fairly straightforward. Straightforward error analysis is an essential step in knowing what features are likely to be informative in training a hybrid system where exceptions to rules are handled by a machine-learning component.","['Kyuchul Yoon', 'Chris Brew']",October 2006,Computer Speech & Language,[],A linguistically motivated approach to grapheme-to-phoneme conversion for Korean
723,"A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.The technique is applied to the problem of automatically capitalizing uniformly cased text. Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking. Capitalization can be also used as a preprocessing step in named entity extraction or machine translation.A “background” capitalizer trained on 20 M words of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996.The “in-domain” performance of the WSJ capitalizer is 45% better relative to the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60% relative; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25–70k words – is about 20–25% relative. Overall, automatic capitalization error rate of 1.4% is achieved on BN data.The performance gain obtained by employing our adaptation technique using a tiny amount of out-of-domain training data on top of the background data is striking: as little as 0.14 M words of in-domain data brings more improvement than using 10 times more background training data (from 2 M words to 20 M words).","['Ciprian Chelba', 'Alex Acero']",October 2006,Computer Speech & Language,[],Adaptation of maximum entropy capitalizer: Little data can help a lot☆
724,"Automatic recognition of children’s speech using acoustic models trained by adults results in poor performance due to differences in speech acoustics. These acoustical differences are a consequence of children having shorter vocal tracts and smaller vocal cords than adults. Hence, speaker adaptation needs to be performed. However, in real-world applications, the amount of adaptation data available may be less than what is needed by common speaker adaptation techniques to yield reasonable performance. In this paper, we first study, in the discrete frequency domain, the relationship between frequency warping in the front-end and corresponding transformations in the back-end. Three common feature extraction schemes are investigated and their transformation linearity in the back-end are discussed. In particular, we show that under certain approximations, frequency warping of MFCC features with Mel-warped triangular filter banks equals a linear transformation in the cepstral space. Based on that linear transformation, a formant-like peak alignment algorithm is proposed to adapt adult acoustic models to children’s speech. The peaks are estimated by Gaussian mixtures using the Expectation-Maximization (EM) algorithm [Zolfaghari, P., Robinson, T., 1996. Formant analysis using mixtures of Gaussians, Proceedings of International Conference on Spoken Language Processing, 1229–1232]. For limited adaptation data, the algorithm outperforms traditional vocal tract length normalization (VTLN) and maximum likelihood linear regression (MLLR) techniques.","['Xiaodong Cui', 'Abeer Alwan']",October 2006,Computer Speech & Language,[],Adaptation of children’s speech with limited data based on formant-like peak alignment
725,"This paper presents a new technique to enhance the performance of the input interface of spoken dialogue systems based on a procedure that combines during speech recognition the advantages of using prompt-dependent language models with those of using a language model independent of the prompts generated by the dialogue system. The technique proposes to create a new speech recognizer, termed contextual speech recognizer, that uses a prompt-independent language model to allow recognizing any kind of sentence permitted in the application domain, and at the same time, uses contextual information (in the form of prompt-dependent language models) to take into account that some sentences are more likely to be uttered than others at a particular moment of the dialogue. The experiments show the technique allows enhancing clearly the performance of the input interface of a previously developed dialogue system based exclusively on prompt-dependent language models. But most important, in comparison with a standard speech recognizer that uses just one prompt-independent language model without contextual information, the proposed recognizer allows increasing the word accuracy and sentence understanding rates by 4.09% and 4.19% absolute, respectively. These scores are slightly better than those obtained using linear interpolation of the prompt-independent and prompt-dependent language models used in the experiments.","['R. López-Cózar', 'Z. Callejas']",October 2006,Computer Speech & Language,[],Combining language models in the input interface of a spoken dialogue system
726,"Differences in speaker characteristics, recording conditions, and signal processing algorithms affect output quality in voice conversion systems. This study focuses on formulating robust techniques for a codebook mapping based voice conversion algorithm. Three different methods are used to improve voice conversion performance: confidence measures, pre-emphasis, and spectral equalization. Analysis is performed for each method and the implementation details are discussed. The first method employs confidence measures in the training stage to eliminate problematic pairs of source and target speech units that might result from possible misalignments, speaking style differences or pronunciation variations. Four confidence measures are developed based on the spectral distance, fundamental frequency (f0) distance, energy distance, and duration distance between the source and target speech units. The second method focuses on the importance of pre-emphasis in line-spectral frequency (LSF) based vocal tract modeling and transformation. The last method, spectral equalization, is aimed at reducing the differences in the source and target long-term spectra when the source and target recording conditions are significantly different. The voice conversion algorithm that employs the proposed techniques is compared with the baseline voice conversion algorithm with objective tests as well as three subjective listening tests. First, similarity to the target voice is evaluated in a subjective listening test and it is shown that the proposed algorithm improves similarity to the target voice by 23.0%. An ABX test is performed and the proposed algorithm is preferred over the baseline algorithm by 76.4%. In the third test, the two algorithms are compared in terms of the subjective quality of the voice conversion output. The proposed algorithm improves the subjective output quality by 46.8% in terms of mean opinion score (MOS).","['Oytun Turk', 'Levent M. Arslan']",October 2006,Computer Speech & Language,[],Robust processing techniques for voice conversion☆
727,"Enriching speech recognition output with sentence boundaries improves its human readability and enables further processing by downstream language processing modules. We have constructed a hidden Markov model (HMM) system to detect sentence boundaries that uses both prosodic and textual information. Since there are more nonsentence boundaries than sentence boundaries in the data, the prosody model, which is implemented as a decision tree classifier, must be constructed to effectively learn from the imbalanced data distribution. To address this problem, we investigate a variety of sampling approaches and a bagging scheme. A pilot study was carried out to select methods to apply to the full NIST sentence boundary evaluation task across two corpora (conversational telephone speech and broadcast news speech), using both human transcriptions and recognition output. In the pilot study, when classification error rate is the performance measure, using the original training set achieves the best performance among the sampling methods, and an ensemble of multiple classifiers from different downsampled training sets achieves slightly poorer performance, but has the potential to reduce computational effort. However, when performance is measured using receiver operating characteristics (ROC) or area under the curve (AUC), then the sampling approaches outperform the original training set. This observation is important if the sentence boundary detection output is used by downstream language processing modules. Bagging was found to significantly improve system performance for each of the sampling methods. The gain from these methods may be diminished when the prosody model is combined with the language model, which is a strong knowledge source for the sentence detection task. The patterns found in the pilot study were replicated in the full NIST evaluation task. The conclusions may be dependent on the task, the classifiers, and the knowledge combination approach.","['Yang Liu', 'Nitesh V. Chawla', 'Mary P. Harper', 'Elizabeth Shriberg', 'Andreas Stolcke']",October 2006,Computer Speech & Language,[],A study in machine learning from imbalanced data for sentence boundary detection in speech
728,"We are interested in the problem of robust understanding from noisy spontaneous speech input. With the advances in automated speech recognition (ASR), there has been increasing interest in spoken language understanding (SLU). A challenge in large vocabulary spoken language understanding is robustness to ASR errors. State of the art spoken language understanding relies on the best ASR hypotheses (ASR 1-best). In this paper, we propose methods for a tighter integration of ASR and SLU using word confusion networks (WCNs). WCNs obtained from ASR word graphs (lattices) provide a compact representation of multiple aligned ASR hypotheses along with word confidence scores, without compromising recognition accuracy. We present our work on exploiting WCNs instead of simply using ASR one-best hypotheses. In this work, we focus on the tasks of named entity detection and extraction and call classification in a spoken dialog system, although the idea is more general and applicable to other spoken language processing tasks. For named entity detection, we have improved the F-measure by using both word lattices and WCNs, 6–10% absolute. The processing of WCNs was 25 times faster than lattices, which is very important for real-life applications. For call classification, we have shown between 5% and 10% relative reduction in error rate using WCNs compared to ASR 1-best output.","['Dilek Hakkani-Tür', 'Frédéric Béchet', 'Giuseppe Riccardi', 'Gokhan Tur']",October 2006,Computer Speech & Language,[],Beyond ASR 1-best: Using word confusion networks in spoken language understanding
729,"In the speech recognition of highly inflecting or compounding languages, the traditional word-based language modeling is problematic. As the number of distinct word forms can grow very large, it becomes difficult to train language models that are both effective and cover the words of the language well. In the literature, several methods have been proposed for basing the language modeling on sub-word units instead of whole words. However, to our knowledge, considerable improvements in speech recognition performance have not been reported.In this article, we present a language-independent algorithm for discovering word fragments in an unsupervised manner from text. The algorithm uses the Minimum Description Length principle to find an inventory of word fragments that is compact but models the training text effectively. Language modeling and speech recognition experiments show that n-gram models built over these fragments perform better than n-gram models based on words. In two Finnish recognition tasks, relative error rate reductions between 12% and 31% are obtained. In addition, our experiments suggest that word fragments obtained using grammatical rules do not outperform the fragments discovered from text. We also present our recognition system and discuss how utilizing fragments instead of words affects the decoding process.","['Teemu Hirsimäki', 'Mathias Creutz', 'Vesa Siivola', 'Mikko Kurimo', 'Sami Virpioja', 'Janne Pylkkönen']",October 2006,Computer Speech & Language,[],Unlimited vocabulary speech recognition with morph language models applied to Finnish
730,"Confidence measures are computed to estimate the certainty that target acoustic units are spoken in specific speech segments. They are applied in tasks such as keyword verification or utterance verification. Because many of the confidence measures use the same set of models and features as in recognition, the resulting scores may not provide an independent measure of reliability. In this paper, we propose two articulatory feature (AF) based phoneme confidence measures that estimate the acoustic reliability based on the match in AF properties. While acoustic-based features, such as Mel-frequency cepstral coefficients (MFCC), are widely used in speech processing, some recent works have focus on linguistically based features, such as the articulatory features that relate directly to the human articulatory process which may better capture speech characteristics. The articulatory features can either replace or complement the acoustic-based features in speech processing. The proposed AF-based measures in this paper were evaluated, in comparison and in combination, with the HMM-based scores on phoneme and keyword verification tasks using children’s speech collected for a computer-based English pronunciation learning project. To fully evaluate their usefulness, the proposed measures and combinations were evaluated on both native and non-native data; and under field test conditions that mis-matches with the training condition. The experimental results show that under the different environments, combinations of the AF scores with the HMM-based scores outperforms HMM-based scores alone on phoneme and keyword verification.","['Ka-Yee Leung', 'Manhung Siu']",October 2006,Computer Speech & Language,[],Articulatory-feature-based confidence measures
731,"This paper presents a novel intonation modelling approach and demonstrates its applicability using the Standard Yorùbá language. Our approach is motivated by the theory that abstract and realised forms of intonation and other dimensions of prosody should be modelled within a modular and unified framework. In our model, this framework is implemented using the Relational Tree (R-Tree) technique. The R-Tree is a sophisticated data structure for representing a multi-dimensional waveform in the form of a tree.Our R-Tree for an utterance is generated in two steps. First, the abstract structure of the waveform, called the Skeletal Tree (S-Tree), is generated using tone phonological rules for the target language. Second, the numerical values of the perceptually significant peaks and valleys on the S-Tree are computed using a fuzzy logic based model. The resulting points are then joined by applying interpolation techniques. The actual intonation contour is synthesised by Pitch Synchronous Overlap Technique (PSOLA) using the Praat software.We performed both quantitative and qualitative evaluations of our model. The preliminary results suggest that, although the model does not predict the numerical speech data as accurately as contemporary data-driven approaches, it produces synthetic speech with comparable intelligibility and naturalness. Furthermore, our model is easy to implement, interpret and adapt to other tone languages.","['Ọdẹ́túnjí A. Ọdẹ́jọbí', 'Anthony J. Beaumont', 'Shun Ha Sylvia Wong']",October 2006,Computer Speech & Language,[],Intonation contour realisation for Standard Yorùbá text-to-speech synthesis: A fuzzy computational approach
732,"Language modeling for large-vocabulary conversational Arabic speech recognition is faced with the problem of the complex morphology of Arabic, which increases the perplexity and out-of-vocabulary rate. This problem is compounded by the enormous dialectal variability and differences between spoken and written language. In this paper, we investigate improvements in Arabic language modeling by developing various morphology-based language models. We present four different approaches to morphology-based language modeling, including a novel technique called factored language models. Experimental results are presented for both rescoring and first-pass recognition experiments.","['Katrin Kirchhoff', 'Dimitra Vergyri', 'Jeff Bilmes', 'Kevin Duh', 'Andreas Stolcke']",October 2006,Computer Speech & Language,[],Morphology-based language modeling for conversational Arabic speech recognition
733,"Lexico-semantic collocations (LSCs) are a prominent type of multiword expressions. Over the last decade, the automatic compilation of LSCs from text corpora has been addressed in a significant number of works. However, very often, the output of an LSC-extraction program is a plain list of LSCs. Being useful as raw material for dictionary construction, plain lists of LSCs are of a rather limited use in NLP-applications. For NLP, LSCs must be assigned syntactic and, especially, semantic information. Our goal is to develop an “off-the-shelf” LSC-acquisition program that annotates each LSC identified in the corpus with its syntax and semantics. In this article, we address the annotation task as a classification task,viewing it as a machine learning problem. The LSC-typology we use are the lexical functions from the Explanatory Combinatorial Lexicology; as lexico-semantic resource, EuroWordnet has been used. The applied machine learning technique is a variant of the nearest neighbor-family, which is defined over lexico-semantic features of the elements of LSCs. The technique has been tested on Spanish verb–noun bigrams.","['Leo Wanner', 'Bernd Bohnet', 'Mark Giereth']",October 2006,Computer Speech & Language,[],Making sense of collocations☆
734,"There is fast growing research on designing energy-efficient computational devices and applications running on them. As one of the most compelling applications for mobile devices, automatic speech recognition (ASR) requires new methods to allow it to use fewer computational and memory resources while still achieving a high level of accuracy. One way to achieve this is through parameter quantization. In this work, we compare a variety of novel sub-vector clustering procedures for ASR system parameter quantization. Specifically, we look at systematic data-driven sub-vector selection techniques, most of which are based on entropy minimization, and others on recognition accuracy maximization on a development set. We compare performance on two speech databases, phonebook, an isolated word speech recognition task, and timit, a phonetically diverse connected-word speech corpus. While the optimal entropy-minimizing or accuracy-driven quantization methods are intractable, several simple schemes including scalar quantization with separate codebooks per parameter and joint scalar quantization with normalization perform well in their attempt to approximate the optimal clustering.","['Karim Filali', 'Xiao Li', 'Jeff Bilmes']",October 2006,Computer Speech & Language,[],Algorithms for data-driven ASR parameter quantization
735,"Grammar induction, also known as grammar inference, is one of the most important research areas in the domain of natural language processing. Availability of large corpora has encouraged many researchers to use statistical methods for grammar induction. This problem can be divided into three different categories of supervised, semi-supervised, and unsupervised, based on type of the required data set for the training phase. Most current inductive methods are supervised, which need a bracketed data set for their training phase; but the lack of this kind of data set in many languages, encouraged us to focus on unsupervised approaches. Here, we introduce a novel approach, which we call history-based inside-outside (HIO), for unsupervised grammar inference, by using part-of-speech tag sequences as the only source of lexical information. HIO is an extension of the inside-outside algorithm enriched by using some notions of history based approaches. Our experiments on English and Persian languages show that by adding some conditions to the rule assumptions of the induced grammar, one can achieve acceptable improvement in the quality of the output grammar.","['Heshaam Feili', 'Gholamreza Ghassem-Sani']",October 2006,Computer Speech & Language,[],Unsupervised grammar induction using history based approach☆
736,,[],October 2006,Computer Speech & Language,[],Contents to Volume 20 - autogenerate
737,,[],October 2006,Computer Speech & Language,[],Subject Index to Volume 20 - autogenerate
738,,"['Joseph P. Campbell', 'John Mason', 'Javier Ortega-Garcia']",April–July 2006,Computer Speech & Language,[],Editorial
739,"In the past years, several text-independent speaker recognition evaluation campaigns have taken place. This paper reports on results of the NIST evaluation of 2004 and the NFI-TNO forensic speaker recognition evaluation held in 2003, and reflects on the history of the evaluation campaigns. The effects of speech duration, training handsets, transmission type, and gender mix show expected behaviour on the DET curves. New results on the influence of language show an interesting dependence of the DET curves on the accent of speakers. We also report on a number of statistical analysis techniques that have recently been introduced in the speaker recognition community, as well as a new application of the analysis of deviance analysis. These techniques are used to determine that the two evaluations held in 2003, by NIST and NFI-TNO, are of statistically different difficulty to the speaker recognition systems.","['David A. van Leeuwen', 'Alvin F. Martin', 'Mark A. Przybocki', 'Jos S. Bouten']",April–July 2006,Computer Speech & Language,[],NIST and NFI-TNO evaluations of automatic speaker recognition
740,"Important aspects of Technical Forensic Speaker Recognition, particularly those associated with evidence, are exemplified and critically discussed, and comparisons drawn with generic Speaker Recognition. The centrality of the Likelihood Ratio of Bayes’ theorem in correctly evaluating strength of forensic speech evidence is emphasised, as well as the many problems involved in its accurate estimation. It is pointed out that many different types of evidence are of use, both experimentally and forensically, in discriminating same-speaker from different-speaker speech samples, and some examples are given from real forensic case-work to illustrate the Likelihood Ratio-based approach. The extent to which Technical Forensic Speaker Recognition meets the Daubert requirement of testability is also discussed.",['Phil Rose'],April–July 2006,Computer Speech & Language,[],"Technical forensic speaker recognition: Evaluation, types and testing of evidence"
741,"The use of quality information for multilevel speaker recognition systems is addressed in this contribution. From a definition of what constitutes a quality measure, two applications are proposed at different phases of the recognition process: scoring and multilevel fusion stages. The traditional likelihood scoring stage is further developed providing guidelines for the practical application of the proposed ideas. Conventional user-independent multilevel support vector machine (SVM) score fusion is also adapted for the inclusion of quality information in the fusion process. In particular, quality measures meeting three different goodness criteria: SNR, F0 deviations and the ITU P.563 objective speech quality assessment are used in the speaker recognition process. Experiments carried out in the Switchboard-I database assess the benefits of the proposed quality-guided recognition approach for both the score computation and score fusion stages.","['Daniel Garcia-Romero', 'Julian Fierrez-Aguilar', 'Joaquin Gonzalez-Rodriguez', 'Javier Ortega-Garcia']",April–July 2006,Computer Speech & Language,[],Using quality measures for multilevel speaker recognition
742,"Support vector machines (SVMs) have proven to be a powerful technique for pattern classification. SVMs map inputs into a high-dimensional space and then separate classes with a hyperplane. A critical aspect of using SVMs successfully is the design of the inner product, the kernel, induced by the high dimensional mapping. We consider the application of SVMs to speaker and language recognition. A key part of our approach is the use of a kernel that compares sequences of feature vectors and produces a measure of similarity. Our sequence kernel is based upon generalized linear discriminants. We show that this strategy has several important properties. First, the kernel uses an explicit expansion into SVM feature space—this property makes it possible to collapse all support vectors into a single model vector and have low computational complexity. Second, the SVM builds upon a simpler mean-squared error classifier to produce a more accurate system. Finally, the system is competitive and complimentary to other approaches, such as Gaussian mixture models (GMMs). We give results for the 2003 NIST speaker and language evaluations of the system and also show fusion with the traditional GMM approach.","['W.M. Campbell', 'J.P. Campbell', 'D.A. Reynolds', 'E. Singer', 'P.A. Torres-Carrasquillo']",April–July 2006,Computer Speech & Language,[],Support vector machines for speaker and language recognition
743,"We propose and motivate an alternative to the traditional error-based or cost-based evaluation metrics for the goodness of speaker detection performance. The metric that we propose is an information-theoretic one, which measures the effective amount of information that the speaker detector delivers to the user. We show that this metric is appropriate for the evaluation of what we call application-independent detectors, which output soft decisions in the form of log-likelihood-ratios, rather than hard decisions. The proposed metric is constructed via analysis and generalization of cost-based evaluation metrics. This construction forms an interpretation of this metric as an expected cost, or as a total error-rate, over a range of different application-types. We further show how the metric can be decomposed into a discrimination and a calibration component. We conclude with an experimental demonstration of the proposed technique to evaluate three speaker detection systems submitted to the NIST 2004 Speaker Recognition Evaluation.","['Niko Brümmer', 'Johan du Preez']",April–July 2006,Computer Speech & Language,[],Application-independent evaluation of speaker detection
744,"Whilst several examples of segment based approaches to language identification (LID) have been published, they have been typically conducted using only a small number of languages, or varying feature sets, thus making it difficult to determine how the segment length influences the accuracy of LID systems. In this study, phone-triplets are used as crude approximates for a syllable-length sub-word segmental unit. The proposed pseudo-syllabic length framework is subsequently used for both qualitative and quantitative examination of the contributions made by acoustic, phonotactic and prosodic information sources, and trialled in accordance with the NIST 1996 LID protocol. Firstly, a series of experimental comparisons are conducted which examine the utility of using segmental units for modelling short term acoustic features. These include comparisons between language specific Gaussian mixture models (GMMs), language specific GMMs for each segmental unit, and finally language specific hidden Markov models (HMM) for each segment, undertaken in an attempt to better model the temporal evolution of acoustic features. In a second tier of experiments, the contribution of both broad and fine class phonotactic information, when considered over an extended time frame, is contrasted with an implementation of the currently popular parallel phone recognition language modelling (PPRLM) technique. Results indicate that this information can be used to complement existing PPRLM systems to obtain improved performance. The pseudo-syllabic framework is also used to model prosodic dynamics and compared to an implemented version of a recently published system, achieving comparable levels of performance.","['Terrence Martin', 'Brendan Baker', 'Eddie Wong', 'Sridha Sridharan']",April–July 2006,Computer Speech & Language,[],A syllable-scale framework for language identification
745,"This paper summarizes the collaboration of the LIA and CLIPS laboratories on speaker diarization of broadcast news during the spring NIST Rich Transcription 2003 evaluation campaign (NIST-RT’03S). The speaker diarization task consists of segmenting a conversation into homogeneous segments which are then grouped into speaker classes.Two approaches are described and compared for speaker diarization. The first one relies on a classical two-step speaker diarization strategy based on a detection of speaker turns followed by a clustering process, while the second one uses an integrated strategy where both segment boundaries and speaker tying of the segments are extracted simultaneously and challenged during the whole process. These two methods are used to investigate various strategies for the fusion of diarization results.Furthermore, segmentation into acoustic macro-classes is proposed and evaluated as a priori step to speaker diarization. The objective is to take advantage of the a priori acoustic information in the diarization process. Along with enriching the resulting segmentation with information about speaker gender, channel quality or background sound, this approach brings gains in speaker diarization performance thanks to the diversity of acoustic conditions found in broadcast news.The last part of this paper describes some ongoing works carried out by the CLIPS and LIA laboratories and presents some results obtained since 2002 on speaker diarization for various corpora.","['Sylvain Meignier', 'Daniel Moraru', 'Corinne Fredouille', 'Jean-François Bonastre', 'Laurent Besacier']",April–July 2006,Computer Speech & Language,"['Speaker indexing', 'Speaker segmentation and clustering', 'Speaker diarization', 'E-HMM', 'Integrated approach', 'Step-by-step approach']",Step-by-step and integrated approaches in broadcast news speaker diarization
746,"In this contribution, the Bayesian framework for interpretation of evidence when applied to forensic speaker recognition is introduced. Different aspects of the use of voice as evidence in the court are addressed, as well as the use by the forensic expert of the likelihood ratio as the right way to express the strength of the evidence. Details on computation procedures of likelihood ratios (LR) are given, along with the assessment tools and methods to validate the performance of these Bayesian forensic systems. However, due to the practical scarcity of suspect data and the mismatched conditions between traces and reference populations common in daily casework, significant errors appear in LR estimation if specific robust techniques are not applied. Original contributions for the robust estimation of likelihood ratios are fully described, including TDLRA (target dependent likelihood ratio alignment), oriented to guarantee the presumption of innocence of suspected but non-perpetrators speakers. These algorithms are assessed with extensive Switchboard experiments but moreover through blind LR-based submissions to both NFI-TNO 2003 Forensic SRE and NIST 2004 SRE, where the strength of the evidence was successfully provided for every questioned speech-suspect recording pair in the respective evaluations.","['Joaquin Gonzalez-Rodriguez', 'Andrzej Drygajlo', 'Daniel Ramos-Castro', 'Marta Garcia-Gomar', 'Javier Ortega-Garcia']",April–July 2006,Computer Speech & Language,[],"Robust estimation, interpretation and assessment of likelihood ratios in forensic speaker recognition"
747,,['A. Stolcke'],January 2006,Computer Speech & Language,[],EditorialEditorial for computer speech and language
748,"This paper investigates the potential of exploiting the redundancy implicit in multiple resolution analysis for automatic speech recognition systems. The analysis is performed by a binary tree of elements, each one of which is made by a half-band filter followed by a down sampler which discards odd samples. Filter design and feature computation from samples are discussed and recognition performance with different choices is presented.A paradigm consisting in redundant feature extraction, followed by feature normalization, followed by dimensionality reduction is proposed. Feature normalization is performed by denoising algorithms. Two of them are considered and evaluated, namely, signal-to-noise ratio-dependent spectral subtraction and soft thresholding. Dimensionality reduction is performed with principal component analysis.Experiments using telephone corpora and the Aurora3 corpus are reported. They indicate that the proposed paradigm leads to a recognition performance with clean speech, measured in word error rate, marginally superior to the one obtained with perceptual linear prediction coefficients. Nevertheless, performance of the proposed analysis paradigm is significantly superior when used with noisy data and the same denoising algorithm is applied to all the analysis methods, which are compared.","['Roberto Gemello', 'Franco Mana', 'Dario Albesano', 'Renato De Mori']",January 2006,Computer Speech & Language,[],Multiple resolution analysis for robust automatic speech recognition
749,"Recently, there has been interest in the use of classifiers based on the product of experts (PoE) framework. PoEs offer an alternative to the standard mixture of experts (MoE) framework. It may be viewed as examining the intersection of a series of experts, rather than the union as in the MoE framework. This paper presents a particular implementation of PoEs, the normalised product of Gaussians (PoG). Here, each expert is a Gaussian mixture model. In this work, the PoG model is presented within a hidden Markov model framework. This allows the classification of variable length data, such as speech data. Training and initialisation procedures are described for this PoG system. The relationship of the PoG system with other schemes, including covariance modeling schemes, is also discussed. In addition the scheme is shown to be related to a standard speech recognition approach, multiple stream systems. The PoG system performance is examined on an automatic speech recognition task, Switchboard. The performance is compared to standard Gaussian mixture systems and multiple stream systems.","['M.J.F. Gales', 'S.S. Airey']",January 2006,Computer Speech & Language,[],Product of Gaussians for speech recognition
750,"This paper investigates supervised and unsupervised adaptation of stochastic grammars, including n-gram language models and probabilistic context-free grammars (PCFGs), to a new domain. It is shown that the commonly used approaches of count merging and model interpolation are special cases of a more general maximum a posteriori (MAP) framework, which additionally allows for alternate adaptation approaches. This paper investigates the effectiveness of different adaptation strategies, and, in particular, focuses on the need for supervision in the adaptation process. We show that n-gram models as well as PCFGs benefit from either supervised or unsupervised MAP adaptation in various tasks. For n-gram models, we compare the benefit from supervised adaptation with that of unsupervised adaptation on a speech recognition task with an adaptation sample of limited size (about 17 h), and show that unsupervised adaptation can obtain 51% of the 7.7% adaptation gain obtained by supervised adaptation. We also investigate the benefit of using multiple word hypotheses (in the form of a word lattice) for unsupervised adaptation on a speech recognition task for which there was a much larger adaptation sample available. The use of word lattices for adaptation required the derivation of a generalization of the well-known Good-Turing estimate. Using this generalization, we derive a method that uses Monte Carlo sampling for building Katz backoff models. The adaptation results show that, for adaptation samples of limited size (several tens of hours), unsupervised adaptation on lattices gives a performance gain over using transcripts. The experimental results also show that with a very large adaptation sample (1050 h), the benefit from transcript-based adaptation matches that of lattice-based adaptation. Finally, we show that PCFG domain adaptation using the MAP framework provides similar gains in F-measure accuracy on a parsing task as was seen in ASR accuracy improvements with n-gram adaptation. Experimental results show that unsupervised adaptation provides 37% of the 10.35% gain obtained by supervised adaptation.","['Michiel Bacchiani', 'Michael Riley', 'Brian Roark', 'Richard Sproat']",January 2006,Computer Speech & Language,[],MAP adaptation of stochastic grammars☆
751,"This paper presents a prosodic phrasing model for Korean to be used in a text-to-speech synthesis (TTS) system. Read text corpora were morpho-syntactically parsed and prosodically labeled following the Penn Korean Treebank (Han, Chunghye, Ko, Eon-Suk, Yi, Heejong, Palmer, M., 2002. Penn Korean Treebank: development and evaluation. In: Proceedings of the 16th Pacific Asian Conference on Language and Computation. Korean Society for Language and Information.) and K-ToBI prosodic labeling conventions (Sun-Ah, J., 2000. K-ToBI (Korean ToBI) labelling conventions. Version 3.1. Available from: URL <http://www.linguistics.ucla.edu/people/jun/ktobi/K-tobi.html>.), respectively. Decision trees were trained with morpho-syntactic and textual distance features to predict locations of accentual and intonational phrase breaks. Our phrasing model cross-validated on a 300-sentence corpus (6936 words or 21,436 syllables, with an average of 72 syllables or 23 words per sentence) predicted non-breaks with F = 92.4% and breaks with F = 88.0% (F = 72.8% for accentual phrase breaks and F = 71.3% for intonational phrase breaks).",['Kyuchul Yoon'],January 2006,Computer Speech & Language,[],A prosodic phrasing model for a Korean text-to-speech synthesis system
752,"This paper describes the architecture and the implementation of a full-scale pronunciation lexicon for Turkish using finite state technology. The system produces at its output, a parallel representation of the pronunciation and the morphological analysis of the word form so that further disambiguation processes can be used to disambiguate pronunciation. The pronunciation representation is based on the SAMPA standard and also encodes the position of the primary stress. The computation of the position of the primary stress depends on an interplay of any exceptional stress in root words and stress properties of certain morphemes, and requires that a full morphological analysis be done. The system has been implemented using XRCE Finite State Toolkit.","['Kemal Oflazer', 'Sharon Inkelas']",January 2006,Computer Speech & Language,[],The architecture and the implementation of a finite state pronunciation lexicon for Turkish☆
753,"In this paper, speaker adaptive acoustic modeling is investigated by using a novel method for speaker normalization and a well known vocal tract length normalization method. With the novel normalization method, acoustic observations of training and testing speakers are mapped into a normalized acoustic space through speaker-specific transformations with the aim of reducing inter-speaker acoustic variability. For each speaker, an affine transformation is estimated with the goal of reducing the mismatch between the acoustic data of the speaker and a set of target hidden Markov models. This transformation is estimated through constrained maximum likelihood linear regression and then applied to map the acoustic observations of the speaker into the normalized acoustic space.Recognition experiments made use of two corpora, the first one consisting of adults’ speech, the second one consisting of children’s speech. Performing training and recognition with normalized data resulted in a consistent reduction of the word error rate with respect to the baseline systems trained on unnormalized data. In addition, the novel method always performed better than the reference vocal tract length normalization method adopted in this work.When unsupervised static speaker adaptation was applied in combination with each of the two speaker normalization methods, a different behavior was observed on the two corpora: in one case performance became very similar while in the other case the difference remained significant.","['Diego Giuliani', 'Matteo Gerosa', 'Fabio Brugnara']",January 2006,Computer Speech & Language,[],Improved automatic speech recognition through speaker normalization
754,,"['Aline Villavicencio', 'Francis Bond', 'Anna Korhonen', 'Diana McCarthy']",October 2005,Computer Speech & Language,[],EditorialIntroduction to the special issue on multiword expressions: Having a crack at a hard nut
755,"Automatic extraction of multiword expressions (MWEs) presents a tough challenge for the NLP community and corpus linguistics. Indeed, although numerous knowledge-based symbolic approaches and statistically driven algorithms have been proposed, efficient MWE extraction still remains an unsolved issue. In this paper, we evaluate the Lancaster UCREL Semantic Analysis System (henceforth USAS (Rayson, P., Archer, D., Piao, S., McEnery, T., 2004. The UCREL semantic analysis system. In: Proceedings of the LREC-04 Workshop, Beyond Named Entity Recognition Semantic labelling for NLP tasks, Lisbon, Portugal. pp. 7–12)) for MWE extraction, and explore the possibility of improving USAS by incorporating a statistical algorithm. Developed at Lancaster University, the USAS system automatically annotates English corpora with semantic category information. Employing a large-scale semantically classified multi-word expression template database, the system is also capable of detecting many multiword expressions, as well as assigning semantic field information to the MWEs extracted. Whilst USAS therefore offers a unique tool for MWE extraction, allowing us to both extract and semantically classify MWEs, it can sometimes suffer from low recall. Consequently, we have been comparing USAS, which employs a symbolic approach, to a statistical tool, which is based on collocational information, in order to determine the pros and cons of these different tools, and more importantly, to examine the possibility of improving MWE extraction by combining them. As we report in this paper, we have found a highly complementary relation between the different tools: USAS missed many domain-specific MWEs (law/court terms in this case), and the statistical tool missed many commonly used MWEs that occur in low frequencies (lower than three in this case). Due to their complementary relation, we are proposing that MWE coverage can be significantly increased by combining a lexicon-based symbolic approach and a collocation-based statistical approach.","['Scott Songlin Piao', 'Paul Rayson', 'Dawn Archer', 'Tony McEnery']",October 2005,Computer Speech & Language,[],Comparing and combining a semantic tagger and a statistical tool for MWE extraction
756,"This paper proposes a range of techniques for extracting English verb-particle constructions from raw text corpora, complete with valence information. We propose four basic methods, based on the output of a POS tagger, chunker, chunk grammar and dependency parser, respectively. We then present a combined classifier which we show to consolidate the strengths of the component methods.",['Timothy Baldwin'],October 2005,Computer Speech & Language,[],Deep lexical acquisition of verb–particle constructions
757,"In this paper, we investigate the phenomenon of verb–particle constructions, discussing their characteristics and availability in some lexical resources. Given the limited coverage provided by these resources and the constantly growing number of verb–particle combinations, possible ways of extending their coverage are investigated, taking into account regular patterns found in some productive combinations of verbs and particles. We propose, in particular, the use of a semantic classification of verbs (such as that defined by [English verb classes and alternations – a preliminary investigation, The University of Chicago Press]) as a means to obtain productive verb–particle constructions and the use of the World Wide Web to validate them, and discuss the issues involved in adopting such an approach.",['Aline Villavicencio'],October 2005,Computer Speech & Language,[],The availability of verb–particle constructions in lexical resources: How much is enough?
758,"The study presented in this paper was aimed at exploring the possibilities of modelling specific pronunciation characteristics of multiword expressions (MWEs) for both automatic speech recognition (ASR) and automatic phonetic transcription (APT). For this purpose, we first drew up an inventory of frequently found N-grams extracted from orthographic transcriptions of spontaneous speech contained in a large corpus of spoken Dutch. These N-grams were filtered and subsequently assigned to linguistic categories. For a small selection of these N-grams we examined the phonetic transcriptions contained in the corpus. We found that the pronunciation of these N-grams differed to a large extent from the canonical form. In order to determine whether this is a general characteristic of spontaneous speech or rather the effect of the specific status of these N-grams, we analysed the pronunciations of the individual words composing the N-grams in two context conditions: (1) in the N-gram context and (2) in any other context. We found that words in N-grams do indeed have peculiar pronunciation patterns. This seems to suggest that the N-grams investigated may be considered as MWEs that should be treated as lexical entries in the pronunciation lexicons used in ASR and APT, with their own specific pronunciation variants.","['Diana Binnenpoorte', 'Catia Cucchiarini', 'Lou Boves', 'Helmer Strik']",October 2005,Computer Speech & Language,[],Multiword expressions in spoken language: An exploratory study on pronunciation variation
759,"In this paper, we describe the empirical evaluation of statistical association measures for the extraction of lexical collocations from text corpora. We argue that the results of an evaluation experiment cannot easily be generalized to a different setting. Consequently, such experiments have to be carried out under conditions that are as similar as possible to the intended use of the measures. Finally, we show how an evaluation strategy based on random samples can reduce the amount of manual annotation work significantly, making it possible to perform many more evaluation experiments under specific conditions.","['Stefan Evert', 'Brigitte Krenn']",October 2005,Computer Speech & Language,[],Using small random samples for the manual evaluation of statistical association measures
760,"This paper describes a distributional approach to the semantics of verb particle constructions (e.g., put up, make off). In common with many other varieties of multiword expression, verb–particles vary in the extent to which the component words of the phrase contribute independent meanings. A technique for automatically detecting when and how a component word is making such a contribution would be very useful in the construction of lexicons. Using verb particles as a test case we suggest that a comparison of the lexical contexts in which the phrase and the components occur can provide us with vital information in this regard. Our hypothesis is that the lexical contexts in which a given verb–particle construction occurs across a corpus will be more similar to those of a given component word if that component word is contributing an independent meaning to the phrase. We demonstrate a convincing correlation between contextual similarity and the compositionality judgements of expert and non-expert annotators.",['Colin Bannard'],October 2005,Computer Speech & Language,[],Learning about the meaning of verb–particle constructions from corpora
761,"This paper provides new insights on the semantic characteristics of two and three noun compounds. An analysis is performed using two sets of semantic classification categories: a list of 8 prepositional paraphrases previously proposed by Lauer [Designing statistical language learners: experiments on noun compounds, Ph.D. Thesis, Macquarie University, Australia] and a new set of 35 semantic relations introduced by us. We show the distribution of these semantic categories on a corpus of noun compounds and present several models for the bracketing and the semantic classification of noun compounds. The results are compared against state-of-the-art models reported in the literature.","['Roxana Girju', 'Dan Moldovan', 'Marta Tatu', 'Daniel Antohe']",October 2005,Computer Speech & Language,[],On the semantics of noun compounds
762,"The purpose of this study is to disambiguate Japanese compound verbs (JCVs) using two methods: (1) a statistical sense discrimination method based on verb-combinatoric information, which feeds into a first-sense statistical sense disambiguation method and (2) a manual rule-based sense disambiguation method which draws on argument structure and verb semantics. In evaluation, we found that the rule-based method outperformed the statistical method at 94.6% token-level accuracy, suggesting that fine-grained semantic analysis is an important component of JCV disambiguation. At the same time, the performance of the fully automated statistical method was found to be surprisingly good at 82.6%, without making use of syntactic or lexical semantic knowledge.","['Kiyoko Uchiyama', 'Timothy Baldwin', 'Shun Ishizaki']",October 2005,Computer Speech & Language,[],Disambiguating Japanese compound verbs
763,"Compound terms play a surprisingly key role in the organization of lexical ontologies. However, their inclusion forces one to address the issues of completeness and consistency that naturally arise from this organizational role. In this paper, we show how creative exploration in the space of literal compounds can reveal not only additional compound terms to systematically balance an ontology, but can also discover new and potentially innovative concepts in their own right.","['Jer Hayes', 'Nuno Seco', 'Tony Veale']",October 2005,Computer Speech & Language,[],Creative discovery in the lexical “validation gap”
764,"This paper presents a three-level structuring of multiword terms basing on lexical inclusion, WordNet similarity and a clustering approach. Term clustering by automatic data analysis methods offers an interesting way of organizing a domain’s knowledge structure, useful for several information-oriented tasks like science and technology watch, textmining, computer-assisted ontology population, Question Answering (Q–A). This paper explores how this three-level term structuring brings to light the knowledge structures from a corpus of genomics and compares the mapping of the domain topics against a hand-built ontology (the GENIA ontology). Ways of integrating the results into a Q–A system are discussed.","['Eric SanJuan', 'James Dowdall', 'Fidelia Ibekwe-SanJuan', 'Fabio Rinaldi']",October 2005,Computer Speech & Language,[],A symbolic approach to automatic multiword term structuring
765,,[],October 2005,Computer Speech & Language,[],Contents to Volume 18 - autogenerate
766,,[],October 2005,Computer Speech & Language,[],Subject Index to Volume 18 - autogenerate
767,"This paper is devoted to the estimation of stochastic context-free grammars (SCFGs) and their use as language models. Classical estimation algorithms, together with new ones that consider a certain subset of derivations in the estimation process, are presented in a unified framework. This set of derivations is chosen according to both structural and statistical criteria. The estimated SCFGs have been used in a new hybrid language model to combine both a word-based n-gram, which is used to capture the local relations between words, and a category-based SCFG together with a word distribution into categories, which is defined to represent the long-term relations between these categories. We describe methods for learning these stochastic models for complex tasks, and we present an algorithm for computing the word transition probability using this hybrid language model. Finally, experiments on the UPenn Treebank corpus show significant improvements in the test set perplexity with regard to the classical word trigram models.","['J.M. Benedí', 'J.A. Sánchez']",July 2005,Computer Speech & Language,[],Estimation of stochastic context-free grammars and their use as language models
768,"This paper introduces a finite-state computational approach to four main types of orthographic variations in Arabic which are: the variations in the writing of verb-initial glottal stop, verb-medial glottal stop, verb-final glottal stop, and verb-final weak letters. This approach is based on a linguistically motivated account for 42 types of variations. This account captures generalizations governing such variations, and it is based on syllabification.",['Salah Alnajem'],July 2005,Computer Speech & Language,[],A computational approach to the variations in Arabic verbal orthography
769,"The extended union model (EUM) was recently proposed and shown to be effective in handling short time temporal corruption. Because of the computational complexity, the EUM probability can only be computed over groups of consecutive observations (called segments) and recognition can only be performed under N-best re-scoring paradigm. In this paper, we introduce a hidden variable called “pattern of corruption” and re-formulate the extended union model as marginalizing over possible patterns of corruption with likelihood computed via the missing feature theory. We then introduce a recursive relationship between the EUM probabilities of two successive observation sequences that can greatly simplify the EUM probability computation. This makes it possible to compute the EUM probability over a long sequence. Using this recursive relationship, the EUM probability over frames, called the “frame-based EUM” can easily be computed. To simplify the EUM-based recognition, we propose an approximated, dynamic programming-based EUM recognition algorithm, called the Frame-based EUM Viterbi algorithm (FEVA), that performs recognition directly instead of via N-best re-scoring. Experimental results on digit recognition under added impulsive noises show that both the frame-base EUM and the FEVA outperform the segment-based EUM.","['Arthur Chan', 'Manhung Siu']",July 2005,Computer Speech & Language,[],Efficient computation of the frame-based extended union model and its application in speech recognition against partial temporal corruptions
770,"Although syntactic structure has been used in recent work in language modeling, there has not been much effort in using semantic analysis for language models. In this study, we propose three new language modeling techniques that use semantic analysis for spoken dialog systems. We call these methods concept sequence modeling, two-level semantic-lexical modeling, and joint semantic-lexical modeling. These models combine lexical information with varying amounts of semantic information, using annotation supplied by either a shallow semantic parser or full hierarchical parser. These models also differ in how the lexical and semantic information is combined, ranging from simple interpolation to tight integration using maximum entropy modeling. We obtain improvements in recognition accuracy over word and class N-gram language models in three different task domains. Interpolation of the proposed models with class N-gram language models provides additional improvement in the air travel reservation domain. We show that as we increase the semantic information utilized and as we increase the tightness of integration between lexical and semantic items, we obtain improved performance when interpolating with class language models, indicating that the two types of models become more complementary in nature.","['Hakan Erdogan', 'Ruhi Sarikaya', 'Stanley F. Chen', 'Yuqing Gao', 'Michael Picheny']",July 2005,Computer Speech & Language,[],Using semantic analysis to improve speech recognition performance
771,"As core speech recognition technology improves, opening up a wider range of applications, genericity and portability are becoming important issues. Most of todays recognition systems are still tuned to a particular task and porting the system to a new task (or language) requires a substantial investment of time and money, as well as human expertise.This paper addresses issues in speech recognizer portability and in the development of generic core speech recognition technology. First, the genericity of wide domain models is assessed by evaluating their performance on several tasks of varied complexity. Then, techniques aimed at enhancing the genericity of these wide domain models are investigated. Multi-source acoustic training is shown to reduce the performance gap between task-independent and task-dependent acoustic models, and for some tasks to out-perform task-dependent acoustic models.Transparent methods for porting generic models to a specific task are also explored. Transparent unsupervised acoustic model adaptation is contrasted with supervised adaptation, and incremental unsupervised adaptation of both the acoustic and linguistic models is investigated. Experimental results on a dialog task show that with the proposed scheme, a transparently adapted generic system can perform nearly as well (about a 1% absolute gap in word error rate) as a task-specific system trained on several tens of hours of manually transcribed data.","['Fabrice Lefevre', 'Jean-Luc Gauvain', 'Lori Lamel']",July 2005,Computer Speech & Language,[],Genericity and portability for task-independent speech recognition
772,"Three experiments are reported that use new experimental methods for the evaluation of text-to-speech (TTS) synthesis from the user's perspective. Experiment 1, using sentence stimuli, and Experiment 2, using discrete “call centre” word stimuli, investigated the effect of voice gender and signal quality on the intelligibility of three concatenative TTS synthesis systems. Accuracy and search time were recorded as on-line, implicit indices of intelligibility during phoneme detection tasks. It was found that both voice gender and noise affect intelligibility. Results also indicate interactions of voice gender, signal quality, and TTS synthesis system on accuracy and search time. In Experiment 3 the method of paired comparisons was used to yield ranks of naturalness and preference. As hypothesized, preference and naturalness ranks were influenced by TTS system, signal quality and voice, in isolation and in combination. The pattern of results across the four dependent variables – accuracy, search time, naturalness, preference – was consistent. Natural speech surpassed synthetic speech, and TTS system C elicited relatively high scores across all measures. Intelligibility, judged naturalness and preference are modulated by several factors and there is a need to tailor systems to particular commercial applications and environmental conditions.","['Catherine Stevens', 'Nicole Lees', 'Julie Vonwiller', 'Denis Burnham']",April 2005,Computer Speech & Language,[],"On-line experimental methods to evaluate text-to-speech (TTS) synthesis: effects of voice gender and signal quality on intelligibility, naturalness and preference"
773,"The Bayesian Information Criterion (BIC) is a widely adopted method for audio segmentation, and has inspired a number of dominant algorithms for this application. At present, however, literature lacks in analytical and experimental studies on these algorithms. This paper tries to partially cover this gap.Typically, BIC is applied within a sliding variable-size analysis window where single changes in the nature of the audio are locally searched. Three different implementations of the algorithm are described and compared: (i) the first keeps updated a pair of sums, that of input vectors and that of square input vectors, in order to save computations in estimating covariance matrices on partially shared data; (ii) the second implementation, recently proposed in literature, is based on the encoding of the input signal with cumulative statistics for an efficient estimation of covariance matrices; (iii) the third implementation consists of a novel approach, and is characterized by the encoding of the input stream with the cumulative pair of sums of the first approach.Furthermore, a dynamic programming algorithm is presented that, within the BIC model, finds a globally optimal segmentation of the input audio stream.All algorithms are analyzed in detail from the viewpoint of the computational cost, experimentally evaluated on proper tasks, and compared.","['Mauro Cettolo', 'Michele Vescovi', 'Romeo Rizzi']",April 2005,Computer Speech & Language,[],Evaluation of BIC-based algorithms for audio segmentation
774,"We present a novel language model, suitable for large-vocabulary continuous speech recognition, based on parsing with a probabilistic left corner grammar (PLCG). The PLCG probabilities are conditioned on local and non-local features of the partial parse tree, and some of these features are lexical. They are not derived from another stochastic grammar, but directly induced from a treebank, a corpus of text sentences, annotated with parse trees. A context-enriched constituent represents all partial parse trees that are equivalent with respect to the probability of the next parse move. For computational efficiency the parsing problem is represented as a traversal through a compact stochastic network of constituents connected by PLCG moves. The efficiency of the algorithm is due to the fact that the network consists of recursively nested, shared subnetworks. The PLCG-based language model results from accumulating the probabilities of all (partial) paths through this network. Next word probabilities can be computed synchronously with the probabilistic left corner parsing algorithm in one single pass from left to right. They are guaranteed to be normalized, even when pruning less likely paths. Finally, it is shown experimentally that the PLCG-based language model is a competitive alternative to other syntax-based language models, both in efficiency and accuracy.","['Dong Hoon Van Uytsel', 'Dirk Van Compernolle']",April 2005,Computer Speech & Language,[],Language modeling with probabilistic left corner parsing☆
775,"A novel multi-level segmented HMM (MSHMM) is presented in which the relationship between symbolic (phonetic) and surface (acoustic) representations of speech is regulated by an intermediate ‘articulatory’ representation. Speech dynamics are characterised as linear trajectories in the articulatory space, which are transformed into the acoustic space using an articulatory-to-acoustic mapping. Recognition is then performed. The results of phonetic classification experiments are presented for monophone and triphone MSHMMs using three formant-based ‘articulatory’ parameterisations and sets of between 1 and 49 linear articulatory-to-acoustic mappings. The NIST Matched Pair Sentence Segment (Word Error) test shows that, for a sufficiently rich combination of articulatory parameterisation and mappings, differences between these results and those obtained with an optimal classifier are not statistically significant. It is also shown that, compared with a conventional HMM, superior performance can be achieved using a MSHMM with 25% fewer parameters.","['Martin J. Russell', 'Philip J.B. Jackson']",April 2005,Computer Speech & Language,[],A multiple-level linear/linear segmental HMM with a formant-based intermediate layer☆
776,"Fuse is a situated spoken language understanding system that uses visual context to steer the interpretation of speech. Given a visual scene and a spoken description, the system finds the object in the scene that best fits the meaning of the description. To solve this task, Fuse performs speech recognition and visually-grounded language understanding. Rather than treat these two problems separately, knowledge of the visual semantics of language and the specific contents of the visual scene are fused during speech processing. As a result, the system anticipates various ways a person might describe any object in the scene, and uses these predictions to bias the speech recognizer towards likely sequences of words. A dynamic visual attention mechanism is used to focus processing on likely objects within the scene as spoken utterances are processed. Visual attention and language prediction reinforce one another and converge on interpretations of incoming speech signals which are most consistent with visual context. In evaluations, the introduction of visual context into the speech recognition process results in significantly improved speech recognition and understanding accuracy. The underlying principles of this model may be applied to a wide range of speech understanding problems including mobile and assistive technologies in which contextual information can be sensed and semantically interpreted to bias processing.","['Deb Roy', 'Niloy Mukherjee']",April 2005,Computer Speech & Language,[],Towards situated speech understanding: visual context priming of language models
777,,[],January 2005,Computer Speech & Language,[],2004 computer speech and language paper award
778,"In this article we review several successful extensions to the standard hidden-Markov-model/artificial neural network (HMM/ANN) hybrid, which have recently made important contributions to the field of noise robust automatic speech recognition. The first extension to the standard hybrid was the “multi-band hybrid”, in which a separate ANN is trained on each frequency sub-band, followed by some form of weighted combination of ANN state posterior probability outputs prior to decoding. However, due to the inaccurate assumption of sub-band independence, this system usually gives degraded performance, except in the case of narrow-band noise. All of the systems which we review overcome this independence assumption and give improved performance in noise, while also improving or not significantly degrading performance with clean speech. The “all-combinations multi-band” hybrid trains a separate ANN for each sub-band combination. This, however, typically requires a large number of ANNs. The “all-combinations multi-stream” hybrid trains an ANN expert for every combination of just a small number of complementary data streams. Multiple ANN posteriors combination using maximum a-posteriori (MAP) weighting gives rise to the further successful strategy of hypothesis level combination by MAP selection. An alternative strategy for exploiting the classification capacity of ANNs is the “tandem hybrid” approach in which one or more ANN classifiers are trained with multi-condition data to generate discriminative and noise robust features for input to a standard ASR system. The “multi-stream tandem hybrid” trains an ANN for a number of complementary feature streams, permitting multi-stream data fusion. The “narrow-band tandem hybrid” trains an ANN for a number of particularly narrow frequency sub-bands. This gives improved robustness to noises not seen during training. Of the systems presented, all of the multi-stream systems provide generic models for multi-modal data fusion. Test results for each system are presented and discussed.","['Astrid Hagen', 'Andrew Morris']",January 2005,Computer Speech & Language,"['Hidden Markov model', 'Artificial neural network', 'Gaussian mixture model', 'Multi-band', 'Multi-stream', 'Expert combination', 'Robust speech recognition']",Recent advances in the multi-stream HMM/ANN hybrid approach to noise robust ASR
779,"The aim of this investigation is to determine to what extent automatic speech recognition may be enhanced if, in addition to the linear compensation accomplished by mean and variance normalisation, a non-linear mismatch reduction technique is applied to the cepstral and energy features, respectively. An additional goal is to determine whether the degree of mismatch between the feature distributions of the training and test data that is associated with acoustic mismatch, differs for the cepstral and energy features. Towards these aims, two non-linear mismatch reduction techniques – time domain noise reduction and histogram normalisation – were evaluated on the Aurora2 digit recognition task as well as on a continuous speech recognition task with noisy test conditions similar to those in the Aurora2 experiments. The experimental results show that recognition performance is enhanced by the application of both non-linear mismatch reduction techniques. The best results are obtained when the two techniques are applied simultaneously. The results also reveal that the mismatch in the energy features is quantitatively and qualitatively much larger than the corresponding mismatch associated with the cepstral coefficients. The most substantial gains in average recognition rate are therefore accomplished by reducing training-test mismatch for the energy features.","['Febe de Wet', 'Johan de Veth', 'Loe Boves', 'Bert Cranen']",January 2005,Computer Speech & Language,[],Additive background noise as a source of non-linear mismatch in the cepstral and log-energy domain
780,"The quality of text-to-speech systems can be effectively assessed only on the basis of reliable and valid listening tests to assess overall system performance. A mean opinion scale (MOS) has been the recommended measure of synthesized speech quality [ITU-T Recommendation P.85, 1994. Telephone transmission quality subjective opinion tests. A method for subjective performance assessment of the quality of speech voice output devices]. We assessed this MOS scale and developed and tested a modified measure of speech quality. This modified measure has new items specific to text-to-speech systems. Our research was motivated by the lack of clear evidence of the conceptual content of as well as the psychometric properties of the MOS scale. We present conceptual arguments and empirical evidence for the reliability and validity of a modified scale. Moreover, we employ state of the art psychometric techniques such as confirmatory factor analysis to provide strong tests of psychometric properties. This modified scale is better suited to appraise synthesis systems since it includes items that are specific to the artifacts found in synthesized speech. We believe that the speech synthesis research communities will find this modified scale a better fit for listening tests to assess synthesized speech.","['Mahesh Viswanathan', 'Madhubalan Viswanathan']",January 2005,Computer Speech & Language,[],Measuring speech quality for text-to-speech systems: development and assessment of a modified mean opinion score (MOS) scale
781,"This paper discusses semantic processing using the Hidden Vector State (HVS) model. The HVS model extends the basic discrete Markov model by encoding context in each state as a vector. State transitions are then factored into a stack shift operation similar to those of a push-down automaton followed by a push of a new preterminal semantic category label. The key feature of the model is that it can capture hierarchical structure without the use of treebank data for training.Experiments have been conducted in the travel domain using the relatively simple ATIS corpus and the more complex DARPA Communicator Task. The results show that the HVS model can be robustly trained from only minimally annotated corpus data. Furthermore, when measured by its ability to extract attribute-value pairs from natural language queries in the travel domain, the HVS model outperforms a conventional finite-state semantic tagger by 4.1% in F-measure for ATIS and by 6.6% in F-measure for Communicator, suggesting that the benefit of the HVS model's ability to encode context increases as the task becomes more complex.","['Yulan He', 'Steve Young']",January 2005,Computer Speech & Language,[],Semantic processing using the Hidden Vector State model
782,"This paper presents algorithms for generating targeted name lists for candidate out-of-vocabulary (OOV) words for applications in language processing, particularly speech recognition. Focusing on names, which are shown to be the dominant class of OOVs in news broadcasts, the approach involves offline generation of a large name list and online pruning based on a phonetic distance. The resulting list can be used in a rescoring pass in automatic speech recognition. We also show that a simple variation of the approach can be used to generate alternate name spellings, which may be useful for query expansion in information retrieval. By using a wide variety of sources, including automatic name phrase tagging of temporally relevant news text, OOV coverage can be improved by nearly a factor of two with only a 10% increase in the word list size. For one source, coverage increased from 13% to 94%. Phonetic pruning can be used to reduce the list size by an order of magnitude with only a small loss in coverage.","['David D. Palmer', 'Mari Ostendorf']",January 2005,Computer Speech & Language,[],Improving out-of-vocabulary name resolution
783,"One approach to the transcription of written text into sounds (phonetization) is to use a set of well-defined language-dependent rules, which are in most situations augmented by a dictionary of exceptional words that constitute their on rules. The process of transcribing into sounds starts by pre-processing the text into lexical items to which the rules are applicable. The rules can be segregated into phonemic and phonetic rules. Phonemic rules operate on the graphemes to convert them into phonemes. Phonetic rules operate onto the phonemes and convert them into phones or actual sounds. Converting from written text into actual sounds and developing a comprehensive set of rules for any language is marked by several problems that have their origins in the relative lack of correspondence between the spelling of the lexical items and their sound contents. For standard Arabic (SA) these problems are not as severe as they are for English or French but they do exist. This paper presents a detailed investigation into all aspects of the phonetization of SA for the purpose of developing a comprehensive system for letter-to-sound conversion for the standard Arabic language and assessing the quality of the letter-to-sound transcription system. In particular the paper deals with the following issues: (1) investigation of the spelling and other problems of SA writing system and their impact on converting graphemes into phonemes. (2) The development of a comprehensive set of rules to be used in the transcription of graphemes into phonemes and (3) investigations of the important contextual phonetic variations of SA phonemes so as to determine viable variants (phones) of the phonemes. (4) The development of a set of rules to be used in the transcription of phonemes into phones. (5) The formulation of the rules for grapheme to phoneme and the phoneme to phone transcriptions into algorithms that lend themselves to computer-based processing. (6) An objective evaluation of the performance of the process of converting SA text into actual sounds.Phonetization of text is an important component in any natural language processing (NLP) domain that envisages text-to-speech (TTS) conversion and has applications beyond speech synthesis such as acoustic modeling for speech recognition and other natural language processing applications.",['Yousif A. El-Imam'],October 2004,Computer Speech & Language,[],Phonetization of Arabic: rules and algorithms
784,"Pronunciations in spontaneous speech differ significantly from citation form and pronunciation modeling for automatic speech recognition has received considerable attention in the last few years. Most methods describe alternate pronunciations of a word using multiple entries in a dictionary or using a network of phones, assuming implicitly that a deviation from the canonical pronunciation results in a “complete” change as described by the alternate pronunciation. We investigate this implicit assumption about pronunciation change in conversational speech and demonstrate here that in most cases, the change is only partial; a phone is not completely deleted or substituted by another phone but is modified only partially. Evidence supporting this conclusion comes from the three-way analysis of features extracted from the acoustic signal for use in a speech recognition system, canonical pronunciations from a dictionary, and careful phonetic transcriptions produced by human labelers. Most often, when a deviation from the canonical pronunciation is marked, neither the canonical nor the manually labeled phones represent the actual acoustics adequately. Further analysis of the manual phonetic transcription reveals a significant number (>20%) of instances where even human labelers disagree on the identity of the surface-form. In light of this evidence, two methods are suggested for accommodating such partial pronunciation change in the automatic recognition of spontaneous speech and experimental results are presented for each method.","['Murat Saraçlar', 'Sanjeev Khudanpur']",October 2004,Computer Speech & Language,[],Pronunciation change in conversational speech and its implications for automatic speech recognition☆
785,"Recent approaches to large vocabulary decoding with weighted finite-state transducers have focused on the use of determinization and minimization algorithms to produce compact decoding graphs. This paper addresses the problem of compiling decoding graphs with long span cross-word context dependency between acoustic models. To this end, we extend the finite-state approach by developing complementary arc factorization techniques that operate on non-deterministic graphs. The use of these techniques allows us to statically compile decoding graphs in which the acoustic models utilize a full word of cross-word context. This is in significant contrast to typical systems which use only a single phone. We show that the particular arc-minimization problem that arises is in fact an NP-complete combinatorial optimization problem. Heuristics for this problem are then presented, and are used in experiments on a Switchboard task, illustrating the moderate sizes and runtimes of the graphs we build.","['François Yvon', 'Geoffrey Zweig', 'George Saon']",October 2004,Computer Speech & Language,[],Arc minimization in finite-state decoding graphs with cross-word acoustic context☆
786,"This paper investigates the problem of updating over time the statistical language model (LM) of an Italian broadcast news transcription system. Statistical adaptation methods are proposed which try to cope with the complex dynamics of news by exploiting newswire texts daily available on the Internet. In particular, contemporary news reports are used to extend the lexicon of the LM, to minimize the out-of-vocabulary (OOV) word rate, and to adapt the n-gram probabilities. Experiments performed on 19 news shows, spanning a period of one month, showed relative reductions of 58% in OOV word rate, 16% in perplexity, and 4% in word error rate (WER).","['Marcello Federico', 'Nicola Bertoldi']",October 2004,Computer Speech & Language,[],Broadcast news LM adaptation over time☆
787,,[],October 2004,Computer Speech & Language,[],Contents to volume 18
788,,[],October 2004,Computer Speech & Language,[],Subject index to volume 18
789,,"['Judita Preiss', 'Mark Stevenson']",July 2004,Computer Speech & Language,[],EditorialIntroduction to the special issue on word sense disambiguation
790,"This paper explores factors correlating with lack of inter-annotator agreement on a word sense disambiguation (WSD) task taken from SENSEVAL-2. Twenty-seven subjects were given a series of tasks requiring word sense judgments. Subjects were asked to judge the applicability of word senses to polysemous words used in context. Metrics of lexical ability were evaluated as predictors of agreement between judges. A strong interaction effect was found for lexical ability, in which differences between levels of lexical knowledge predict disagreement. Individual levels of lexical knowledge, however, were not independently predictive of disagreement. The finding runs counter to previous assumptions regarding expert agreement on WSD annotation tasks, which in turn impacts notions of a meaningful “gold standard” for systems evaluation.","['G.Craig Murray', 'Rebecca Green']",July 2004,Computer Speech & Language,[],Lexical knowledge and human disagreement on a WSD task
791,"This article describes an algorithm called HyperLex that is capable of automatically determining word uses in a textbase without recourse to a dictionary. The algorithm makes use of the specific properties of word cooccurrence graphs, which are shown as having “small world” properties. Unlike earlier dictionary-free methods based on word vectors, it can isolate highly infrequent uses (as rare as 1% of all occurrences) by detecting “hubs” and high-density components in the cooccurrence graphs. The algorithm is applied here to information retrieval on the Web, using a set of highly ambiguous test words. An evaluation of the algorithm showed that it only omitted a very small number of relevant uses. In addition, HyperLex offers automatic tagging of word uses in context with excellent precision (97%, compared to 73% for baseline tagging, with an 82% recall rate). Remarkably good precision (96%) was also achieved on a selection of the 25 most relevant pages for each use (including highly infrequent ones). Finally, HyperLex is combined with a graphic display technique that allows the user to navigate visually through the lexicon and explore the various domains detected for each word use.",['Jean Véronis'],July 2004,Computer Speech & Language,[],HyperLex: lexical cartography for information retrieval☆
792,"This paper describes a sense disambiguation method for a polysemous target noun using the context words surrounding the target noun and its WordNet relatives, such as synonyms, hypernyms and hyponyms. The result of sense disambiguation is a relative that can substitute for that target noun in a context. The selection is made based on co-occurrence frequency between candidate relatives and each word in the context. Since the co-occurrence frequency is obtainable from a raw corpus, the method is considered to be an unsupervised learning algorithm and therefore does not require a sense-tagged corpus. In a series of experiments using SemCor and the corpus of SENSEVAL-2 lexical sample task, all in English, and using some Korean data, the proposed method was shown to be very promising. In particular, its performance was superior to that of the other approaches evaluated on the same test corpora.","['Hee-Cheol Seo', 'Hoojung Chung', 'Hae-Chang Rim', 'Sung Hyon Myaeng', 'Soo-Hong Kim']",July 2004,Computer Speech & Language,[],Unsupervised word sense disambiguation using WordNet relatives
793,"Domains are common areas of human discussion, such as economics, politics, law, science, etc., which are at the basis of lexical coherence. This paper explores the dual role of domains in word sense disambiguation (WSD). On one hand, domain information provides generalized features at the paradigmatic level that are useful to discriminate among word senses. On the other hand, domain distinctions constitute a useful level of coarse grained sense distinctions, which lends itself to more accurate disambiguation with lower amounts of knowledge.In this paper we extend and ground the modeling of domains and the exploitation of WordNet Domains, an extension of WordNet in which each synset is labeled with domain information. We propose a novel unsupervised probabilistic method for the critical step of estimating domain relevance for contexts, and suggest utilizing it within unsupervised domain driven disambiguation for word senses, as well as within a traditional supervised approach.The paper presents empirical assessments of the potential utilization of domains in WSD at a wide range of comparative settings, supervised and unsupervised. Following the dual role of domains we report experiments that evaluate both the extent to which domain information provides effective features for WSD, as well as the accuracy obtained by WSD at domain-level sense granularity. Furthermore, we demonstrate the potential for either avoiding or minimizing manual annotation thanks to the generalized level of information provided by domains.","['Alfio Gliozzo', 'Carlo Strapparava', 'Ido Dagan']",July 2004,Computer Speech & Language,[],Unsupervised and supervised exploitation of semantic domains in lexical disambiguation☆
794,"This paper presents a suite of methods and results for the semantic disambiguation of WordNet glosses. WordNet is a resource widely used in natural language processing and artificial intelligence. Intended and designed as a lexical database, WordNet exhibits some deficiencies when used as a knowledge base. By semantically disambiguating the words in the glosses, we add pointers from each word to its concept or synset, and this increases the connectivity between the WordNet concepts by approximately an order of magnitude. We show how lexical chains and other applications can be built on this richly connected WordNet. The semantic disambiguation of the WordNet glosses is performed using automatic methods based on a set of heuristics. The precision of the semantic annotation is improved by using voting between the disambiguation system described here and another WSD system. The entire WordNet 2.0 has been disambiguated with an overall precision of 86% and is available at http://xwn.hlt.utdallas.edu.","['Dan Moldovan', 'Adrian Novischi']",July 2004,Computer Speech & Language,[],Word sense disambiguation of WordNet glosses
795,We present a theoretically motivated method for creating probabilistic word sense disambiguation (WSD) systems. The method works by composing multiple probabilistic components: such modularity is made possible by an application of Bayesian statistics and Lidstone's smoothing method. We show that a probabilistic WSD system created along these lines is a strong competitor to state-of-the-art WSD systems.,['Judita Preiss'],July 2004,Computer Speech & Language,[],Probabilistic word sense disambiguation
796,"The minimum classification error (MCE) framework is an approach to discriminative training for pattern recognition that explicitly incorporates a smoothed version of classification performance into the recognizer design criterion. Many studies have confirmed the effectiveness of MCE for speech recognition. In this article, we present a theoretical analysis of the smoothness of the MCE loss function. Specifically, we show that the MCE criterion function is equivalent to a Parzen window-based estimate of the theoretical classification risk. In this analysis, each training token is mapped to the center of a Parzen kernel in the domain of a suitably defined random variable. The kernels are summed to produce a density estimate; this estimate in turn can easily be integrated over the domain of incorrect classifications, yielding the risk estimate. The expression of risk for each kernel corresponds directly to the usual MCE loss function. The specific form of the Parzen window corresponds to the specific form of the MCE loss function. The derivation presented here shows that the smooth MCE loss function, far from being an ad-hoc approximation of the true error, can be seen as the direct consequence of using a well-understood type of smoothing, Parzen estimation, to estimate the theoretical risk from a finite training set. This analysis provides a novel link between the MCE empirical cost measured on a finite training set and the theoretical classification risk.","['Erik McDermott', 'Shigeru Katagiri']",April 2004,Computer Speech & Language,[],A derivation of minimum classification error from the theoretical classification risk using Parzen estimation
797,"The first goal of this study was to investigate the effect of changing several properties of a continuous speech recognizer (CSR) on the automatic phonetic transcriptions generated by the same CSR. Our results show that the quality of the automatic transcriptions can be improved by using ‘short’ hidden Markov models (HMMs) and by reducing the amount of contamination in the HMMs. The amount of contamination can be reduced by training the HMMs on the basis of a transcription that better matches the actual pronunciation, e.g., by modeling pronunciation variation or by training HMMs on read speech. Furthermore, we found that context-dependent HMMs should preferably not be trained on baseline transcriptions if there is a mismatch between these baseline transcriptions of the speech material and the realized pronunciation. Finally, we found that by combining the changes in the properties of the CSR, the quality of automatic transcription can be further improved.The second goal of this study was to find out whether a relationship exists between word error rate (WER) and transcription quality. As no clear relationship was found, we conclude that taking the CSR with the lowest WER does not necessarily provide the optimal solution for obtaining optimal automatic transcriptions.","['Judith M. Kessens', 'Helmer Strik']",April 2004,Computer Speech & Language,[],On automatic phonetic transcription quality: lower word error rates do not guarantee better transcriptions
798,"We propose new methods to exploit contemporaneous text, such as on-line news articles, to improve language models for automatic speech recognition and other natural language processing applications. In particular, we investigate the use of text from a resource-rich language to sharpen language models for processing a news story or article in a language with scarce linguistic resources. We demonstrate that even with fairly crude cross-language information retrieval and simple machine translation, one can construct story-specific Chinese language models which exploit cues from a side-corpus of English newswire to significantly improve the performance of language models estimated from a static Chinese corpus. Our investigations cover cases when the amount of available Chinese text is small, and a case when a large Chinese text corpus is available. We examine the effectiveness of our techniques both when the side-corpus contains English documents that are near-translations of the Chinese documents being processed, and when the English side-corpus is merely from contemporaneous and independent news sources. We present experimental results for automatic transcription of speech from the Mandarin Broadcast News corpus.","['Sanjeev Khudanpur', 'Woosung Kim']",April 2004,Computer Speech & Language,"['Multi-lingual processing', 'Statistical language modeling', 'Automatic speech recognition', 'Resource-deficient languages', 'Lexical triggers', 'Maximum entropy']",Contemporaneous text as side-information in statistical language modeling
799,"This paper describes the Mandarin–English Information (MEI) project, where we investigated the problem of cross-language spoken document retrieval (CL-SDR), and developed one of the first English–Chinese CL-SDR systems. Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection. Hence, this is a cross-language and cross-media retrieval task. We applied a multi-scale approach to our problem, which unifies the use of phrases, words and subwords in retrieval. The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation. Untranslatable named entities are transliterated by a novel subword translation technique. The multi-scale approach can be divided into three subtasks – multi-scale query formulation, multi-scale audio indexing (by speech recognition) and multi-scale retrieval. Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.","['Helen M. Meng', 'Berlin Chen', 'Sanjeev Khudanpur', 'Gina-Anne Levow', 'Wai-Kit Lo', 'Douglas Oard', 'Patrick Schone', 'Karen Tang', 'Hsin-min Wang', 'Jianqiang Wang']",April 2004,Computer Speech & Language,"['Multi-scale spoken document retrieval', 'English–Chinese cross-language spoken document retrieval']",Mandarin–English Information (MEI): investigating translingual speech retrieval
800,"Recently various techniques to improve the correlation model of feature vector elements in speech recognition systems have been proposed. Such techniques include semi-tied covariance hidden Markov models (HMMs) and systems based on factor analysis. All these schemes have been shown to improve the speech recognition performance without dramatically increasing the number of model parameters compared to standard diagonal covariance Gaussian mixture HMMs. This paper introduces a general form of acoustic model, the factor analysed HMM. A variety of configurations of this model and parameter sharing schemes, some of which correspond to standard systems, were examined. An EM algorithm for the parameter optimisation is presented along with a number of methods to increase the efficiency of training. The performance of FAHMMs on medium to large vocabulary continuous speech recognition tasks was investigated. The experiments show that without elaborate complexity control an equivalent or better performance compared to a standard diagonal covariance Gaussian mixture HMM system can be achieved with considerably fewer parameters.","['A-V.I. Rosti', 'M.J.F. Gales']",April 2004,Computer Speech & Language,[],Factor analysed hidden Markov models for speech recognition
801,"In this paper, we present an inductive approach to the automatic extraction of knowledge about inter-regional pronunciation variation. We compare two different rule-induction techniques, both popular in language engineering applications, viz. the rule sequence learner Transformation-Based Error-Driven Learning (TBEDL) Brill (1995) and the decision tree learner C5.0 Quinlan (1993). We investigate whether both techniques detect the same regularities and evaluate the extracted rules in terms of accuracy and in terms of linguistic relevance. As a case study, we apply the approach to Dutch and Flemish (the variety of Dutch spoken in Flanders, a part of Belgium), based on Celex and Fonilex, pronunciation lexica for Dutch and Flemish, respectively. Our main goal is to show that this approach allows the automatic acquisition of compact, interpretable translation rules between pronunciation varieties, on the basis of phonemic representations of words in both varieties (as output of phoneme recognition or, as in our case, on the basis of existing lexica). We also show that the observed differences coincide with the tendencies studied and described in linguistic comparative research of inter-regional pronunciation variation in standard Dutch.","['Véronique Hoste', 'Walter Daelemans', 'Steven Gillis']",January 2004,Computer Speech & Language,[],Using rule-induction techniques to model pronunciation variation in Dutch
802,"Speech-input translation can be properly approached as a pattern recognition problem by means of statistical alignment models and stochastic finite-state transducers. Under this general framework, some specific models are presented. One of the features of such models is their capability of automatically learning from training examples. Moreover, the stochastic finite-state transducers permit an integrated architecture similar to one used in speech recognition. In this case, the acoustic models (hidden Markov models) are embedded into the finite-state transducers, and the translation of a source utterance is the result of a (Viterbi) search on the integrated network. These approaches have been followed in the framework of the European project EuTrans. Translation experiments have been performed from Spanish to English and from Italian to English in an application involving the interaction of a customer with a receptionist at the frontdesk of a hotel.","['F. Casacuberta', 'H. Ney', 'F.J. Och', 'E. Vidal', 'J.M. Vilar', 'S. Barrachina', 'I. Garcı́a-Varea', 'D. Llorens', 'C. Martı́nez', 'S. Molau', 'F. Nevado', 'M. Pastor', 'D. Picó', 'A. Sanchis', 'C. Tillmann']",January 2004,Computer Speech & Language,"['Speech-input translation', 'Statistical alignment models', 'Stochastic finite-state transducers']",Some approaches to statistical and finite-state speech-to-speech translation
803,"A two-level mixture linear dynamic system model, with frame-level switching parameters in the observation equation and with segment-level switching parameters in the target-directed state equation, is developed and evaluated. The main contributions of this work are: (1) the new framework for dealing with mixed-level switching in the dynamic system and (2) the novel use of piecewise linear functions, enabled by the introduction of frame-level switching, to approximate the nonlinear function between the hidden vocal-tract-resonance space and the observable acoustic space. The approximation is accomplished by the frame-dependent switching parameters in the observation equation. In this paper, in a self-contained manner, we highlight the key algorithm differences from the earlier model having only single segment-level switching that is synchronous between the state and observation equations. A series of speech recognition experiments are carried out to evaluate this new model using a subset of Switchboard conversational speech data. The experimental results show that the approximation accuracy is improved with an increased number of switching-parameter values. The speech recognizer built from the new mixed-level switching dynamic system model using an N-best re-scoring evaluation paradigm show moderate word error rate reduction compared with using either single-level switching or no switching parameters.","['Jeff Ma', 'Li Deng']",January 2004,Computer Speech & Language,[],A mixed-level switching dynamic system for continuous speech recognition☆
804,"Two different systems are proposed for the task of capitalisation generation. The first system is a slightly modified speech recogniser. In this system, every word in the vocabulary is duplicated: once in a de-capitalised form and again in capitalised forms. In addition, the language model is re-trained on mixed case texts. The other system is based on Named Entity (NE) recognition and punctuation generation, since most capitalised words are the first words in sentences or NE words. Both systems are compared when every procedure is fully automated. The system based on NE recognition and punctuation generation shows better results by word error rate, by F-measure and by slot error rate than the system modified from the speech recogniser. This is because the latter system has a distorted language model and a sparser language model. The detailed performance of the system based on NE recognition and punctuation generation is investigated by including one or more of the following: the reference word sequences, the reference NE classes and the reference punctuation marks. The results show that this system is robust to NE recognition errors. Although most punctuation generation errors cause errors in this capitalisation generation system, the number of errors caused in capitalisation generation does not exceed the number of errors from punctuation generation. In addition, the results demonstrate that the effect of NE recognition errors is independent of the effect of punctuation generation errors for capitalisation generation.","['Ji-Hwan Kim', 'Philip C Woodland']",January 2004,Computer Speech & Language,[],Automatic capitalisation generation for speech input
805,"To parse real texts to derive precise semantics, grammar-based parsers must deal with conjuncts and fragments not traditionally considered constituents. Currently available large-scale grammar-based parsers do not systematically handle such phenomena. This paper reports two parser implementations based on Combinatory Categorial Grammar, which can recognize constituents more flexibly but accurately, and shows their practicality after overcoming a potential efficiency problem associated with this flexibility arising from so-called ‘spurious ambiguity’.",['Nobo Komagata'],January 2004,Computer Speech & Language,[],A solution to the spurious ambiguity problem for practical Combinatory Categorial Grammar parsers
806,,[],January 2004,Computer Speech & Language,[],Call for papers: multiword expressions
807,,[],October 2003,Computer Speech & Language,[],Change of editor
808,,[],October 2003,Computer Speech & Language,[],2003 computer speech and language paper award
809,"Current speech recognition systems perform poorly on conversational speech as compared to read speech, arguably due to the large acoustic variability inherent in conversational speech. Our hypothesis is that there are systematic effects in local context, associated with syllabic structure, that are not being captured in the current acoustic models. Such variation may be modeled using a broader definition of context than in traditional systems which restrict context to be the neighboring phonemes. In this paper, we study the use of word- and syllable-level context conditioning in recognizing conversational speech. We describe a method to extend standard tree-based clustering to incorporate a large number of features, and we report results on the Switchboard task which indicate that syllable structure outperforms pentaphones and incurs less computational cost. It has been hypothesized that previous work in using syllable models for recognition of English was limited because of ignoring the phenomenon of resyllabification (change of syllable structure at word boundaries), but our analysis shows that accounting for resyllabification does not impact recognition performance.","['Izhak Shafran', 'Mari Ostendorf']",October 2003,Computer Speech & Language,[],Acoustic model clustering based on syllable structure
810,"Large vocabulary continuous speech recognition can benefit from an efficient data structure for representing a large number of acoustic hypotheses compactly. Word graphs or lattices have been chosen as such an efficient interface between acoustic recognition engines and subsequent language processing modules. This paper first investigates the effect of pruning during acoustic decoding on the quality of word lattices and shows that by combining different pruning options (at the model level and word level), we can obtain word lattices with comparable accuracy to the original lattices and a manageable size. In order to use the word lattices as the input for a post-processing language module, they should preserve the target hypotheses and their scores while being as small as possible. In this paper, we introduce a word graph compression algorithm that significantly reduces the number of words in the graphical representation without eliminating utterance hypotheses or distorting their acoustic scores. We compare this word graph compression algorithm with several other lattice size-reducing approaches and demonstrate the relative strength of the new word graph compression algorithm for decreasing the number of words in the representation. Experiments are conducted across corpora and vocabulary sizes to determine the consistency of the pruning and compression results.","['Yang Liu', 'Mary P Harper', 'Michael T Johnson', 'Leah H Jamieson']",October 2003,Computer Speech & Language,[],The effect of pruning and compression on graphical representations of the output of a speech recognizer
811,"The high error rate in spontaneous speech recognition is due in part to the poor modeling of pronunciation variations. An analysis of acoustic data reveals that pronunciation variations include both complete changes and partial changes. Complete changes are the replacement of a canonical phoneme by another alternative phone, such as ‘b’ being pronounced as ‘p’. Partial changes are the variations within the phoneme, such as nasalization, centralization, voiceless, voiced, etc. Most current work in pronunciation modeling attempts to represent pronunciation variations either by alternative phonetic representations or by the concatenation of subphone units at the hidden Markov state level. In this paper, we show that partial changes are a lot less clear-cut than previously assumed and cannot be modeled by mere representation by alternate phones or a concatenation of phone units. We propose modeling partial changes through acoustic model reconstruction. We first propose a partial change phone model (PCPM) to differentiate pronunciation variations. In order to improve the model resolution without increasing the parameter size too much, PCPM is used as a hidden model and merged into the pre-trained acoustic model through model reconstruction. To avoid model confusion, auxiliary decision trees are established for PCPM triphones, and one auxiliary decision tree can only be used by one standard decision tree. The acoustic model reconstruction on triphones is equivalent to decision tree merging. The effectiveness of this approach is evaluated on the 1997 Hub4NE Mandarin Broadcast News corpus (1997 MBN) with different styles of speech. It gives a significant 2.39% syllable error rate absolute reduction in spontaneous speech.","['Yi Liu', 'Pascale Fung']",October 2003,Computer Speech & Language,[],Modeling partial pronunciation variations for spontaneous Mandarin speech recognition
812,"An analysis-based non-linear feature extraction approach is proposed, inspired by a model of how speech amplitude spectra are affected by additive noise. Acoustic features are extracted based on the noise-robust parts of speech spectra without losing discriminative information. Two non-linear processing methods, harmonic demodulation and spectral peak-to-valley ratio locking, are designed to minimize mismatch between clean and noisy speech features. A previously studied method, peak isolation [IEEE Transactions on Speech and Audio Processing 5 (1997) 451], is also discussed with this model. These methods do not require noise estimation and are effective in dealing with both stationary and non-stationary noise. In the presence of additive noise, ASR experiments show that using these techniques in the computation of MFCCs improves recognition performance greatly. For the TI46 isolated digits database, the average recognition rate across several SNRs is improved from 60% (using unmodified MFCCs) to 95% (using the proposed techniques) with additive speech-shaped noise. For the Aurora 2 connected digit-string database, the average recognition rate across different noise types, including non-stationary noise background, and SNRs improves from 58% to 80%.","['Qifeng Zhu', 'Abeer Alwan']",October 2003,Computer Speech & Language,[],Non-linear feature extraction for robust speech recognition in stationary and non-stationary noise☆
813,"The characterization of a speech signal using non-linear dynamical features has been the focus of intense research lately. In this work, the results obtained with time-dependent largest Lyapunov exponents (TDLEs) in a text-dependent speaker verification task are reported. The baseline system used Gaussian mixture models (GMMs), obtained from the adaptation of a universal background model (UBM), for the speaker voice models. Sixteen cepstral and 16 delta cepstral features were used in the experiments, and it is shown how the addition of TDLEs can improve the system’s accuracy. Cepstral mean subtraction was applied to all features in the tests for channel equalization, and silence frames were discarded. The corpus used, obtained from a subset of the Center for Spoken Language Understanding (CSLU) Speaker Recognition corpus, consisted of telephone speech from 91 different speakers.","['Adriano Petry', 'Dante Augusto Couto Barone']",October 2003,Computer Speech & Language,[],Preliminary experiments in speaker verification using time-dependent largest Lyapunov exponents
814,,"['E.W.D. Whittaker', 'P.C. Woodland']",October 2003,Computer Speech & Language,[],ErratumLanguage modelling for Russian and English using words and classes [Computer Speech and Language 17 (2003) 87–104]
815,"In recent years there has been an upsurge in interest in approaches to speech pattern processing which go beyond the conventional hidden Markov model (HMM) framework. Current HMM-based models are fragile in noise, limited in their ability to handle pronunciation variation, and costly for large vocabulary spontaneous speech transcription. Their ability to represent dynamic behaviour is limited, and they are incompatible with modern, non-linear theories of phonology. This special issue of Computer Speech and Language on new computational paradigms for acoustic modeling in speech recognition brings together nine papers which are representative of current research in acoustic modeling which seeks to overcome these limitations.","['Martin J. Russell', 'Jeff A. Bilmes']",April–July 2003,Computer Speech & Language,[],EditorialIntroduction to the special issue on new computational paradigms for acoustic modeling in speech recognition
816,"During the last decade, the most significant advances in the field of continuous speech recognition (CSR) have arisen from the use of hidden Markov models (HMM) for acoustic modeling. These models address one of the major issues for CSR: simultaneous modeling of temporal and frequency distortions in the speech signal. In the HMM, the temporal dimension is managed through an oriented states graph, each state accounting for the local frequency distortions through a probability density function. In this study, improvement of the HMM performance is expected from the introduction of a very effective non-parametric probability density function estimate: the k-nearest neighbors (k-nn) estimate.First, experiments on a short-term speech spectrum identification task are performed to compare the k-nn estimate and the widespread estimate based on mixtures of Gaussian functions. Then adaptations implied by the integration of the k-nn estimate in an HMM-based recognition system are developed. An optimal training protocol is obtained based on the introduction of the membership coefficients in the HMM parameters. The membership coefficients measure the degree of association between a reference acoustic vector and a HMM state. The training procedure uses the expectation-maximization (EM) algorithm applied to the membership coefficient estimation. Its convergence is shown according to the maximum likelihood criterion. This study leads to the development of a baseline k-nn/HMM recognition system which is evaluated on the TIMIT speech database. Further improvements of the k-nn/HMM system are finally sought through the introduction of a temporal information into the representation space (delta coefficients) and the adaptation of the references (mainly, gender modeling and contextual modeling).",['Fabrice Lefèvre'],April–July 2003,Computer Speech & Language,[],Non-parametric probability estimation for HMM-based automatic speech recognition
817,"Most current speech recognizers use an observation space based on a temporal sequence of measurements extracted from fixed-length “frames” (e.g., Mel-cepstra). Given a hypothetical word or sub-word sequence, the acoustic likelihood computation always involves all observation frames, though the mapping between individual frames and internal recognizer states will depend on the hypothesized segmentation. There is another type of recognizer whose observation space is better represented as a network, or graph, where each arc in the graph corresponds to a hypothesized variable-length segment that is represented by a fixed-dimensional “feature”. In such feature-based recognizers, each hypothesized segmentation will correspond to a segment sequence, or path, through the overall segment-graph that is associated with a subset of all possible feature vectors in the total observation space. In this work we examine a maximum a posteriori decoding strategy for feature-based recognizers and develop a normalization criterion useful for a segment-based Viterbi or A* search. Experiments are reported for both phonetic and word recognition tasks.",['James R Glass'],April–July 2003,Computer Speech & Language,[],A probabilistic framework for segment-based speech recognition
818,"This paper presents an experimental comparison of the performance of the multilayer perceptron (MLP) with that of the mixture density network (MDN) for an acoustic-to-articulatory mapping task. A corpus of acoustic-articulatory data recorded by electromagnetic articulography (EMA) for a single speaker was used as training and test data for this purpose. In theory, the MDN is able to provide a richer, more flexible description of the target variables in response to a given input vector than the least-squares trained MLP. Our results show that the mean likelihoods of the target articulatory parameters for an unseen test set were indeed consistently higher with the MDN than with the MLP. The increase ranged from approximately 3% to 22%, depending on the articulatory channel in question. On the basis of these results, we argue that using a more flexible description of the target domain, such as that offered by the MDN, can prove beneficial when modelling the acoustic-to-articulatory mapping.","['Korin Richmond', 'Simon King', 'Paul Taylor']",April–July 2003,Computer Speech & Language,[],Modelling the uncertainty in recovering articulation from acoustics
819,"This paper describes the theory and implementation of Bayesian networks in the context of automatic speech recognition. Bayesian networks provide a succinct and expressive graphical language for factoring joint probability distributions, and we begin by presenting the structures that are appropriate for doing speech recognition training and decoding. This approach is notable because it expresses all the details of a speech recognition system in a uniform way using only the concepts of random variables and conditional probabilities. A powerful set of computational routines complements the representational utility of Bayesian networks, and the second part of this paper describes these algorithms in detail. We present a novel view of inference in general networks – where inference is done via a change-of-variables that renders the network tree-structured and amenable to a very simple form of inference. We present the technique in terms of straightforward dynamic programming recursions analogous to HMM α–β computation, and then extend it to handle deterministic constraints amongst variables in an extremely efficient manner. The paper concludes with a sequence of experimental results that show the range of effects that can be modeled, and that significant reductions in error-rate can be expected from intelligently factored state representations.",['Geoffrey Zweig'],April–July 2003,Computer Speech & Language,[],Bayesian network structures and inference techniques for automatic speech recognition
820,"This paper presents the theoretical basis and preliminary experimental results of a new HMM model, referred to as HMM2, which can be considered as a mixture of HMMs. In this new model, the emission probabilities of the temporal (primary) HMM are estimated through secondary, state specific, HMMs working in the acoustic feature space. Thus, while the primary HMM is performing the usual time warping and integration, the secondary HMMs are responsible for extracting/modeling the possible feature dependencies, while performing frequency warping and integration. Such a model has several potential advantages, such as a more flexible modeling of the time/frequency structure of the speech signal. When working with spectral features, such a system can also perform nonlinear spectral warping, effectively implementing a form of nonlinear vocal tract normalization. Furthermore, it will be shown that HMM2 can be used to extract noise robust features, supposed to be related to formant regions, which can be used as extra features for traditional HMM recognizers to improve their performance. These issues are evaluated in the present paper, and different experimental results are reported on the Numbers95 database.","['Katrin Weber', 'Shajith Ikbal', 'Samy Bengio', 'Hervé Bourlard']",April–July 2003,Computer Speech & Language,[],Robust speech recognition and feature extraction using HMM2
821,"In this work, buried Markov models (BMM) are introduced. In a BMM, a Markov chain state at time t determines the conditional independence patterns that exist between random variables lying within a local time window surrounding t. This model is motivated by and can be fully described by “graphical models”, a general technique to describe families of probability distributions. In the paper, it is shown how information-theoretic criterion functions can be used to induce sparse, discriminative, and class-conditional network structures that yield an optimal approximation to the class posterior probability, and therefore are useful for classification tasks such as speech recognition. Using a new structure learning heuristic, the resulting structurally discriminative models are tested on a medium-vocabulary isolated-word speech recognition task. It is demonstrated that discriminatively structured BMMs, when trained in a maximum likelihood setting using EM, can outperform both hidden Markov models (HMMs) and other dynamic Bayesian networks with a similar number of parameters.",['Jeff A. Bilmes'],April–July 2003,Computer Speech & Language,[],Buried Markov models: a graphical-modeling approach to automatic speech recognition
822,"While Hidden Markov Models (HMMs) have been successful in many speech recognition tasks, performance on conversational speech is somewhat less successful, arguably due in part to the greater variation in timing of articulatory events. Loosely Coupled or Factorial HMMs (FHMMs) represent a family of models that have more flexibility for modeling such variation in speech, but there are tradeoffs to be studied in terms of computation and potential added confusability. This paper investigates two specific instances – Mixed-Memory and Parameter-Tied FHMMs – that can both be thought of as loosely coupled HMMs for modelling multiple time series. The Parameter-Tied FHMM, introduced here, has a potential advantage for speech modelling since it allows a left-to-right topology across the product state space. Experimental results on the ISOLET task show both models are feasible for speech recognition; TI-DIGITS recognition results show the Parameter-Tied FHMM is competitive with Multiband Models. State occupancy and pruning analyses show trends related to asynchrony that hold across the different models.","['H.J. Nock', 'M. Ostendorf']",April–July 2003,Computer Speech & Language,[],Parameter reduction schemes for loosely coupled HMMs
823,"This paper presents a new approach to multi-band automatic speech recognition which has the advantage to overcome many limitations of classical muti-band systems. The principle of this new approach is to build a speech model in the time–frequency domain using the formalism of dynamic Bayesian networks. In contrast to classical multi-band modeling, this formalism leads to a probabilistic speech model which allows communications between the different sub-bands and, consequently, no recombination step is required in recognition. We develop efficient learning and decoding algorithms both for isolated and continuous speech recognition. We present illustrative experiments on isolated and connected digit recognition tasks. These experiments show that the this new approach is very promising in the field of noisy speech recognition.","['Khalid Daoudi', 'Dominique Fohr', 'Christophe Antoine']",April–July 2003,Computer Speech & Language,[],Dynamic Bayesian networks for multi-band automatic speech recognition
824,"This paper provides a summary of our studies on robust speech recognition based on a new statistical approach – the probabilistic union model. We consider speech recognition given that part of the acoustic features may be corrupted by noise. The union model is a method for basing the recognition on the clean part of the features, thereby reducing the effect of the noise on recognition. To this end, the union model is similar to the missing feature method. However, the two methods achieve this end through different routes. The missing feature method usually requires the identity of the noisy data for noise removal, while the union model combines the local features based on the union of random events, to reduce the dependence of the model on information about the noise. We previously investigated the applications of the union model to speech recognition involving unknown partial corruption in frequency band, in time duration, and in feature streams. Additionally, a combination of the union model with conventional noise-reduction techniques was studied, as a means of dealing with a mixture of known or trainable noise and unknown unexpected noise. In this paper, a unified review, in the context of dealing with unknown partial feature corruption, is provided into each of these applications, giving the appropriate theory and implementation algorithms, along with an experimental evaluation.","['Ji Ming', 'F. Jack Smith']",April–July 2003,Computer Speech & Language,[],Speech recognition with unknown partial feature corruption – a review of the union model
825,,[],April–July 2003,Computer Speech & Language,[],Call for PapersSpecial Issue on Word Sense Disambiguation
826,,[],January 2003,Computer Speech & Language,[],Publisher
827,,[],January 2003,Computer Speech & Language,[],2002 Computer Speech and Language Paper Award
828,"In this paper we present an algorithm for segmenting or locating the endpoints of speech in a continuous signal stream. The proposed algorithm is based on non-linear likelihood-based projections derived from a Bayesian classifier. It utilizes class distributions in a speech/non-speech classifier to project the signal into a 2-dimensional space where, in the ideal case, optimal classification can be performed with a simple linear discriminant. The projection results in the transformation of diffuse, nebulous classes in high-dimensional space into compact clusters in the low-dimensional space that can be easily separated by simple clustering mechanisms. In this space, decision boundaries for optimal classification can be more easily identified using simple clustering criteria. The segmentation algorithm proposed utilizes this property to determine and update optimal classification thresholds continuously for the signal being segmented. The performance of the proposed algorithm has been evaluated on data recorded under extremely diverse environmental noise conditions. The experiments show that the algorithm performs comparably to manual segmentations even under these diverse conditions.","['Bhiksha Raj', 'Rita Singh']",January 2003,Computer Speech & Language,[],Classifier-based non-linear projection for adaptive endpointing of continuous speech
829,"In natural language and especially in spontaneous speech, people often group words in order to constitute phrases which become usual expressions. This is due to phonological (to make the pronunciation easier), or to semantic reasons (to remember more easily a phrase by assigning a meaning to a block of words). Classical language models do not adequately take into account such phrases. A better approach consists in modeling some word sequences as if they were individual dictionary elements. Sequences are considered as additional entries of the vocabulary, on which language models are computed.In this paper, we present a method for automatically retrieving the most relevant phrases from a corpus of written sentences. The originality of our approach resides in the fact that the extracted phrases are obtained from a linguistically tagged corpus. Therefore, the obtained phrases are linguistically viable. To measure the contribution of classes in retrieving phrases, we have implemented the same algorithm without using classes. The class-based method outperformed by 11% the other method.Our approach uses information theoretic criteria which insure a high statistical consistency and make the decision of selecting a potential sequence optimal in accordance with the language perplexity. We propose several variants of language model with and without word sequences. Among them, we present a model in which the trigger pairs are linguistically more significant. We show that the use of sequences decrease the word error rate and improve the normalized perplexity. For instance, the best sequence model improves the perplexity by 16%, and the the accuracy of our dictation system (MAUD) by approximately 14%. Experiments, in terms of perplexity and recognition rate, have been carried out on a vocabulary of 20,000 words extracted from a corpus of 43 million words made up of two years of the French newspaper Le Monde. The acoustic model (HMM) is trained with the Bref80 corpus.","['Imed Zitouni', 'Kamel Smaı̈li', 'Jean-Paul Haton']",January 2003,Computer Speech & Language,[],Statistical language modeling based on variable-length sequences
830,"Most modern speech synthesis systems using context dependent decision trees in their acoustic synthesis modules are unit selection style concatenative speech synthesis systems using the trees essentially as a form of pruning during their segment search. The IBM Trainable Speech Synthesis System is one such system. This paper begins by discussing the advantages and disadvantages of the decision tree and non-decision tree approaches to unit selection synthesis. It goes on to present the results of formal listening tests conducted on the IBM system to investigate a number of different topics pertinent to decision tree based systems. These include the use of extended context features during clustering, the effect of using trees with different numbers of leaves and different numbers of segments per leaf, and the performance of several different off-line segment preselection algorithms.",['R.E Donovan'],January 2003,Computer Speech & Language,[],Topics in decision tree based speech synthesis
831,"This paper focuses on modeling pronunciation variation in two different ways: data-derived and knowledge-based. The knowledge-based approach consists of using phonological rules to generate variants. The data-derived approach consists of performing phone recognition, followed by smoothing using decision trees (D-trees) to alleviate some of the errors in the phone recognition. Using phonological rules led to a small improvement in WER; a data-derived approach in which the phone recognition was smoothed using D-trees prior to lexicon generation led to larger improvements compared to the baseline. The lexicon was employed in two different recognition systems: a hybrid HMM/ANN system and a HMM-based system, to ascertain whether pronunciation variation was truly being modeled. This proved to be the case as no significant differences were found between the results obtained with the two systems. A comparison between the knowledge-based and data-derived methods showed that 17% of variants generated by the phonological rules were also found using phone recognition, and this increases to 46% when the phone recognition output is smoothed by using D-trees.",['Mirjam Wester'],January 2003,Computer Speech & Language,[],Pronunciation modeling for ASR – knowledge-based and data-derived methods
832,"This paper examines statistical language modelling of Russian and English in the context of automatic speech recognition. The characteristics of both a Russian and an English text corpus of similar composition are discussed with reference to the properties of both languages. In particular, it is shown that to achieve the same vocabulary coverage as a 65,000 word vocabulary for English, a 430,000 word vocabulary is required for Russian. The implications of this observation motivate the remainder of the paper. Perplexity experiments are reported for word-based N-gram modelling of the two languages and the differences are examined. It is found that, in contrast to English, there is little gain in using 4-grams over trigrams for modelling Russian. Class-based N-gram modelling is then considered and perplexity experiments are reported for two different types of class models, a two-sided model and a novel, one-sided model for which classes are generated automatically. Word and class model combinations show the two-sided model results in lower perplexities than combinations with the one-sided model. However, the very large Russian vocabulary favours the use of the one-sided model since the clustering algorithm, used to obtain word classes automatically, is significantly faster. Lattice rescoring experiments are then reported on an English-language broadcast news task which show that both combinations of the word model with either type of class model produce identical reductions in word error rate.","['E.W.D Whittaker', 'P.C Woodland']",January 2003,Computer Speech & Language,[],Language modelling for Russian and English using words and classes
833,,[],January 2003,Computer Speech & Language,[],Obituary: W A (Bill) Ainsworth
834,,"['Marilyn A Walker', 'Owen C Rambow']",July–October 2002,Computer Speech & Language,[],Spoken language generation
835,"This paper describes the response planning and generation components of the mercury flight reservation system, a mixed-initiative spoken dialogue system that supports both voice-only interaction and multi-modal interaction augmenting spoken inputs with typing or clicking at a displayed Web page. mercury is configured using the Galaxy Communicator architecture (Seneff, Hurley, Lau, Schmid, & Zue, 1998), where a suite of servers interact via program control mediated by a central hub. Language generation is performed in two steps: response planning, or deep-structure generation, is carried out by the dialogue manager, and is well-integrated with other aspects of dialogue control; control flow is specified by a dialogue control table (Seneff & Polifroni, 2000a). Response generation, or surface-form generation, is executed by a separate language generation server, under the guidance of a set of recursive generation rules and an associated lexicon (Baptist & Seneff, 2000). The generation of the textual string for the graphical interface and the marked-up synthesis string for spoken outputs are controlled by a shared set of generation rules (Seneff & Polifroni, 2000b). Thus there is a direct meaning-to-speech mapping that eliminates the need to analyze linguistic structure for synthesis. To date, we have collected over 25 000 utterances from users interacting with the mercury system. We report here on both the results of user satisfaction studies conducted by the National Institute of Standards and Technology (NIST), and on our own tabulation of a number of different measures of dialogue success.",['S Seneff'],July–October 2002,Computer Speech & Language,[],Response planning and generation in the mercury flight reservation system☆
836,"In this paper, we describe a generation system for spoken dialogue that not only produces coherent, informative and responsive dialogue contributions, but also explicitly models human styles of interaction. This generation system is based on conversation acts theory. It has been implemented in the TRIPS spoken dialogue system, and includes components that plan content, perform surface generation for different modalities, and coordinate output production. We discuss our implementation, and describe an evaluation of the generation output.",['Amanda J Stent'],July–October 2002,Computer Speech & Language,[],A conversation acts model for generating spoken dialogue contributions
837,"A spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes. The system is trained by a ‘show-and-tell’ procedure in which visual scenes are paired with natural language descriptions. Learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure, word classes, and individual words. Using these structures, a planning algorithm integrates syntactic, semantic, and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes. The system generates syntactically well-formed compound adjective noun phrases, as well as relative spatial clauses. The acquired linguistic structures generalize from training data, enabling the production of novel word sequences which were never observed during training. The output of the generation system is synthesized using word-based concatenative synthesis drawing from the original training speech corpus. In evaluations of semantic comprehension by human judges, the performance of automatically generated spoken descriptions was comparable to human-generated descriptions. This work is motivated by our long-term goal of developing spoken language processing systems which grounds semantics in machine perception and action.",['Deb K. Roy'],July–October 2002,Computer Speech & Language,[],Learning visually grounded words and syntax for a scene description task
838,"We describe a corpus-based approach to natural language generation (NLG). The approach has been implemented as a component of a spoken dialog system and a series of evaluations were carried out. Our system uses n-gram language models, which have been found useful in other language technology applications, in a generative mode. It is not yet clear whether the simple n-grams can adequately model human language generation in general, but we show that we can successfully apply this ubiquitous modeling technique to the task of natural language generation for spoken dialog systems. In this paper, we discuss applying corpus-based stochastic language generation at two levels: content selection and sentence planning/realization. At the content selection level, output utterances are modeled by bigrams, and the appropriate attributes are chosen using bigram statistics. In sentence planning and realization, corpus utterances are modeled by n-grams of varying length, and new utterances are generated stochastically. Through this work, we show that a simple statistical model alone can generate appropriate language for a spoken dialog system. The results describe a promising avenue for using a statistical approach in future NLG systems.","['Alice H Oh', 'Alexander I Rudnicky']",July–October 2002,Computer Speech & Language,[],Stochastic natural language generation for spoken dialog systems
839,"In the past few years, as the number of dialogue systems has increased, there has been an increasing interest in the use of natural language generation in spoken dialogue. Our research assumes that trainable natural language generation is needed to support more flexible and customized dialogues with human users. This paper focuses on methods for automatically training the sentence planning module of a spoken language generator. Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e., the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences. The paper first presents SPoT, a trainable sentence planner, and a new methodology for automatically training SPoT on the basis of feedback provided by human judges. Our methodology is unique in neither depending on hand-crafted rules nor on the existence of a domain-specific corpus. SPoT first randomly generates a candidate set of sentence plans and then selects one. We show that SPoT learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan. We then experimentally evaluate SPoT by asking human judges to compare SPoT's output with a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners. We show that SPoT performs better than the rule-based systems and the baselines, and as well as the hand-crafted system.","['Marilyn A. Walker', 'Owen C. Rambow', 'Monica Rogati']",July–October 2002,Computer Speech & Language,[],Training a sentence planner for spoken dialogue using boosting
840,"We study how decisions for word ordering and word choice in surface natural language generation can be automatically learned from annotated data. We examine four trainable systems for surface natural language generation in the air travel domain, called NLG[1–4]. NLG1 is a lookup table which stores the most frequent phrase to express a concept, and is intended as a baseline system for comparison purposes. NLG2 and NLG3 attempt to find the highest probability word sequence with respect to a maximum entropy probability model. They differ in that NLG2 predicts words left-to-right, while NLG3 predicts words in dependency tree order. NLG4 requires a dependency-style grammar of phrase fragments and conditions on their use, and attempts to find the highest probability word sequence that is consistent with the rules and conditions of the grammar. NLG4 has been implemented in a dialog strategy for a prototype air travel conversational system, in which word order is dynamically modified to emphasize certain aspects of the run-time dialog state.",['Adwait Ratnaparkhi'],July–October 2002,Computer Speech & Language,[],Trainable approaches to surface natural language generation and their application to conversational dialog systems☆
841,"Prosody modeling is critical in developing a Concept-to-Speech (CTS) system where both Natural Language Generation (NLG) and Speech Synthesis are used to automatically generate natural, coherent speech. In this paper, we empirically verify the usefulness of various natural language features in prosody modeling. Three groups of features are investigated: semantic, syntactic, and surface features produced by SURGE, a general-purpose surface natural language generator for English, deep semantic, and discourse features that are available during the domain modeling and content planning phases of generation, and information-based measures statistically derived from text. Our experiments identify which of this large set of features are effective in prosody modeling. This work represents an important step towards building a comprehensive prosody model for CTS systems that employ general NLG. This investigation is conducted in the context of MAGIC, a medical application that involves automatic speech and graphics generation.","['Shimei Pan', 'Kathleen McKeown', 'Julia Hirschberg']",July–October 2002,Computer Speech & Language,[],Exploring features from natural language generation for prosody modeling
842,"In concept-to-speech systems, spoken output is generated on the basis of a text that has been produced by the system itself. In such systems, linguistic information from the text generation component may be exploited to achieve a higher prosodic quality of the speech output than can be obtained in a plain text-to-speech system. In this paper we discuss how information from natural language generation can be used to compute prosody in a concept-to-speech system, focusing on the automatic marking of contrastive accents on the basis of information about the preceding discourse. We discuss and compare some formal approaches to this problem and present the results of a small perception experiment that was carried out to test which discourse contexts trigger a preference for contrastive accent, and which do not. Finally, we describe a method for marking contrastive accent in a generic concept-to-speech system called D2S. In D2S, contrastive accent is assigned to generated phrases expressing different aspects of similar events. Unlike in previous approaches, there is no restriction on the kind of entities that may be considered contrastive. This is in line with the observation that, given the `right' context, any two items may stand in contrast to each other.",['Mariët Theune'],July–October 2002,Computer Speech & Language,[],Contrast in concept-to-speech generation
843,"In this paper, we describe how language generation and speech synthesis for spoken dialog systems can be efficiently integrated under a weighted finite state transducer architecture. Taking advantage of this efficiency, we show that introducing flexible targets in generation leads to more natural sounding synthesis. Specifically, we allow multiple wordings of the response and multiple prosodic realizations of the different wordings. The choice of wording and prosodic structure are then jointly optimized with unit selection for waveform generation in speech synthesis. Results of perceptual experiments show that by integrating the steps of language generation and speech synthesis, we are able to achieve improved naturalness of synthetic speech compared to the sequential implementation.","['Ivan Bulyko', 'Mari Ostendorf']",July–October 2002,Computer Speech & Language,[],Efficient integrated response generation from multiple targets using weighted finite state transducers
844,"This article describes investigations into the use of phonologically-constrained morphological analysis (PCMA) in language modelling for continuous speech recognition. PCMA provides a means for modelling text as a sequence of morphemes in a way that retains compatibility with the linear concatenative model of pronunciation used in conventional decoders. Experiments were performed in English exploiting the 100-million-word British National Corpus as source material. We show that PCMA leads to smaller but more generative pronunciation lexicons, and that it does not weaken the quality of the acoustic decoding measured in terms of recognition lattices. For trigram language models, perplexity figures are poorer for PCMA over words, as might be expected given the reduction in sentence span. However recognition results show small improvements in accuracy under some conditions, particularly when morph lattices are decoded with word-trigram models. We explore the capabilities for PCMA across vocabulary size, language model training size, and post-processing strategy. The best results show a 16% relative reduction in word error rate.","['Mark Huckvale', 'Alex Chengyu Fang']",April 2002,Computer Speech & Language,[],Regular ArticleUsing phonologically-constrained morphological analysis in continuous speech recognition
845,"Accurate prediction of segmental duration from text in a text-to-speech system is difficult for several reasons. One which is especially relevant is the great quantity of contextual factors that affect timing and it is difficult to find the right way to model them. There are many parameters that affect duration, but not all of them are always relevant and some can even be counterproductive because of the possibility of overtraining.The main motivation of this paper has been to reduce the error in the duration estimation.To this end, it is of the utmost importance to find the factors that most influence duration in a given language. The approach we have taken is to use a neural network, which is completely configurable, and experiment with the different combinations of parameters that yield the minimum error in the estimation.We have oriented our work mainly towards the following aspects: the most significant parameters that can be used as input to the automatic model, and the best way to code these parameters. We have studied first the effect of each parameter alone and, after that, we have included all parameters together to have our final system.Another important aspect of this study is the generation of a suite of software tools and design protocols that will be used in future tasks with different speakers and databases. The applications for automatic modelling are obvious: adapt the prosody to a new speaker, to a new environment, to “restricted-domain"" sentences, etc., in a fast, semi-automatic and inexpensive way. After the database labelling, it is a matter of minutes to prepare the inputs to the network for the new situation, and the network is trained in 1 h.The result has been a system that predicts duration with very good results (19 ms in RMS) and that clearly improves our previous rule-based system.","['Ricardo Córdoba', 'Juan M. Montero', 'Juana M. Gutiérrez', 'José A. Vallejo', 'Emilia Enriquez', 'José M. Pardo']",April 2002,Computer Speech & Language,[],Regular ArticleSelection of the most significant parameters for duration modelling in a Spanish text-to-speech system using neural networks
846,"A challenging scenario is addressed in which a distant-talking speech recognizer operates in a noisy office environment with model adaptation. The use of a single far microphone as well as that of a microphone array input are investigated.In addition to the benefits from the application of microphone array processing, system robustness is improved by training hidden Markov models (HMMs) with a contaminated version of a clean corpus. This artificial corpus is produced by exploiting information extracted from “real world"" acoustic scenarios. The resulting models are then used as a starting point for unsupervised incremental adaptation.Experimental results show that improvements in recognition accuracy due to multiple microphones, HMM training on contaminated speech and incremental adaptation are additive on a connected digits task. Moreover, the results show that unsupervised incremental adaptation receives the benefits of starting from models trained using contaminated speech. A final contribution of the paper refers to the influence of accuracy of speech activity detection, which seems to be relevant when moving towards real applications.","['Marco Matassoni', 'Maurizio Omologo', 'Diego Giuliani', 'Piergiorgio Svaizer']",April 2002,Computer Speech & Language,[],Regular ArticleHidden Markov model training with contaminated speech material for distant-talking speech recognition
847,"The most popular model used in automatic speech recognition is the hidden Markov model (HMM). Though good performance has been obtained with such models there are well known limitations in its ability to model speech. A variety of modifications to the standard HMM topology have been proposed to handle these problems. One approach is the factorial HMM. This paper introduces a new form of factorial HMM which makes use of transformation streams. The new scheme is a generalization of the standard factorial HMM and other related schemes in speech processing. A particular form of this model, theHMM error model (HEM) is described in detail. The HEM is evaluated on two standard large vocabulary speaker independent speech recognition tasks. On both tasks significant reductions in word error rate are obtained over standard HMM-based systems.",['M.J.F. Gales'],April 2002,Computer Speech & Language,[],Regular ArticleTransformation streams and the HMM error model
848,"In this paper we report on the application of across-word context dependent acoustic phoneme models in a single-pass large vocabulary continuous speech recognizer.Although across-word models are used by many groups today, only an outline of the recognizers is usually given in the publications. Implementation details are often missing.We present both a formal derivation of across-word model search and a detailed description of our implementation. The across-word model system is compared with a conventional within-word model system regarding word error rate and computational effort. Compared to the baseline within-word system a straightforward implementation of across-word model search results in a substantial increase of the computational effort. Therefore, several optimization steps are studied that result in a more efficient organization of the search space and a more efficient pruning. The effects of these optimizations are analysed in a detailed profiling. In combination they accelerate the straightforward implementation of across-word model search by nearly a factor of three.In addition we discuss the construction of word graphs during across-word model search. Starting from a word graph method based on within-word model search, we derive a formal specification of across-word word graphs. We show that the resulting word graphs are a good representation of the active search space.","['Achim Sixtus', 'Hermann Ney']",April 2002,Computer Speech & Language,[],Regular ArticleFrom within-word model search to across-word model search in large vocabulary continuous speech recognition
849,,"['Jean-Luc Gauvain', 'Renato De Mori', 'Lori Lamel']",January 2002,Computer Speech & Language,[],EditorialAdvances in Large Vocabulary Speech Recognition
850,"Transformation-based model adaptation techniques have been used for many years to improve robustness of speech recognition systems. While the estimation criterion used to estimate transformation parameters has been mainly based on maximum likelihood estimation (MLE), Bayesian versions of some of the most popular transformation-based adaptation methods have been recently introduced, like MAPLR, a maximum a posteriori(MAP) based version of the well-known maximum likelihood linear regression (MLLR) algorithm. This is in fact an attempt to constraint parameter estimation in order to obtain reliable estimation with a limited amount of data, not only to prevent overfitting the adaptation data but also to allow integration of prior knowledge into transformation-based adaptation techniques. Since such techniques require the estimation of a large number of transformation parameters when the amount of adaptation data is large, it is also required to define a large number of prior densities for these parameters. Robust estimation of these prior densities is therefore a crucial issue that directly affects the efficiency and effectiveness of the Bayesian techniques. This paper proposes to estimate these priors using the notion of hierarchical priors, embedded into the tree structure used to control transformation complexity. The proposed algorithm, called structural MAPLR (SMAPLR), has been evaluated on the Spoke3 1993 test set of the WSJ task. It is shown that SMAPLR reduces the risk of overtraining and exploits the adaptation data much more efficiently than MLLR, leading to a significant reduction of the word error rate for any amount of adaptation data.","['Olivier Siohan', 'Tor André Myrvoll', 'Chin-Hui Lee']",January 2002,Computer Speech & Language,[],Regular ArticleStructural maximum a posteriori linear regression for fast HMM adaptation
851,"This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models (HMMs). This paper concentrates on the maximum mutual information estimation (MMIE) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of discriminative training techniques for speech recognition of which the authors are aware. Details are given of the MMIE lattice-based implementation used with the extended Baum-Welch algorithm, which makes training of such large systems computationally feasible. Techniques for improving generalization using acoustic scaling and weakened language models are discussed. The overall technique has allowed the estimation of triphone and quinphone HMM parameters which has led to significant reductions in word error rate for the transcription of conversational telephone speech relative to our best systems trained using maximum likelihood estimation (MLE). This is in contrast to some previous studies, which have concluded that there is little benefit in using discriminative training for the most difficult large vocabulary speech recognition tasks. The lattice MMIE-based discriminative training scheme is also shown to out-perform the frame discrimination technique. Various properties of the lattice-based MMIE training scheme are investigated including comparisons of different lattice processing strategies (full search and exact-match) and the effect of lattice size on performance. Furthermore a scheme based on the linear interpolation of the MMIE and MLE objective functions is shown to reduce the danger of over-training. It is shown that HMMs trained with MMIE benefit as much as MLE-trained HMMs from applying model adaptation using maximum likelihood linear regression (MLLR). This has allowed the straightforward integration of MMIE-trained HMMs into complex multi-pass systems for transcription of conversational telephone speech and has contributed to our MMIE-trained systems giving the lowest word error rates in both the 2000 and 2001 NIST Hub5 evaluations.","['P.C. Woodland', 'D. Povey']",January 2002,Computer Speech & Language,[],Regular ArticleLarge scale discriminative training of hidden Markov models for speech recognition
852,"In this paper we present an approach to recognition confidence scoring and a set of techniques for integrating confidence scores into the understanding and dialogue components of a speech understanding system. The recognition component uses a multi-tiered approach where confidence scores are computed at the phonetic, word, and utterance levels. The scores are produced by extracting confidence features from the computation of the recognition hypotheses and processing these features using an accept/reject classifier for word and utterance hypotheses. The scores generated by the confidence classifier can then be passed on to the language understanding and dialogue modeling components of the system. In these components the confidence scores can be combined with linguistic scores and pragmatic constraints before the system makes a final decision about the appropriate action to be taken. To evaluate the system, experiments were conducted using the jupiter weather information system. An evaluation of the confidence classifier at the word-level shows that the system detects 66% of the recognizer’s errors with a false detection rate on correctly recognized words of only 5%. An evaluation was also performed at the understanding level using key-value pair concept error rate as the evaluation metric. When confidence scores were integrated into the understanding component of the system, a relative reduction of 35% in concept error rate was achieved.","['Timothy J. Hazen', 'Stephanie Seneff', 'Joseph Polifroni']",January 2002,Computer Speech & Language,[],Regular ArticleRecognition confidence scoring and its use in speech understanding systems
853,"We survey the use of weighted finite-state transducers (WFSTs) in speech recognition. We show that WFSTs provide a common and natural representation for hidden Markov models (HMMs), context-dependency, pronunciation dictionaries, grammars, and alternative recognition outputs. Furthermore, general transducer operations combine these representations flexibly and efficiently. Weighted determinization and minimization algorithms optimize their time and space requirements, and a weight pushing algorithm distributes the weights along the paths of a weighted transducer optimally for speech recognition. As an example, we describe a North American Business News (NAB) recognition system built using these techniques that combines the HMMs, full cross-word triphones, a lexicon of 40 000 words, and a large trigram grammar into a single weighted transducer that is only somewhat larger than the trigram word grammar and that runs NAB in real-time on a very simple decoder. In another example, we show that the same techniques can be used to optimize lattices for second-pass recognition. In a third example, we show how general automata operations can be used to assemble lattices from different recognizers to improve recognition performance.","['Mehryar Mohri', 'Fernando Pereira', 'Michael Riley']",January 2002,Computer Speech & Language,[],Regular ArticleWeighted finite-state transducers in speech recognition
854,"A number of decoding strategies for large vocabulary continuous speech recognition (LVCSR) are examined from the viewpoint of their search space representation. Different design solutions are compared with respect to the integration of linguistic and acoustic constraints, as implied by m -gram language models (LM) and cross-word (CW) phonetic contexts. This study is structured along two main axes: the network expansion and the search algorithm itself. The network can be expanded statically or dynamically while the search can proceed either time-synchronously or asynchronously which leads to distinct architectures. Three broad classes of decoding methods are briefly reviewed: the use of weighted finite state transducers (WFST) for static network expansion, the time-synchronous dynamic-expansion search and the asynchronous stack decoding. Heuristic methods for further reducing the search space are also considered. The main approaches are compared and some prospective views are formulated regarding possible future avenues.",['Xavier L. Aubert'],January 2002,Computer Speech & Language,[],Regular ArticleAn overview of decoding techniques for large vocabulary continuous speech recognition
855,"The last decade has witnessed substantial progress in speech recognition technology, with today’s state-of-the-art systems being able to transcribe unrestricted broadcast news audio data with a word error of about 20%. However, acoustic model development for these recognizers relies on the availability of large amounts of manually transcribed training data. Obtaining such data is both time-consuming and expensive, requiring trained human annotators and substantial amounts of supervision. This paper describes some recent experiments using lightly supervised and unsupervised techniques for acoustic model training in order to reduce the system development cost. The approach uses a speech recognizer to transcribe unannotated broadcast news data from the Darpa TDT-2 corpus. The hypothesized transcription is optionally aligned with closed-captions or transcripts to create labels for the training data. Experiments providing supervision only via the language model training materials show that including texts which are contemporaneous with the audio data is not crucial for success of the approach, and that the acoustic models can be initialized with as little as 10 min of manually annotated data. These experiments demonstrate that light or no supervision can dramatically reduce the cost of building acoustic models.","['Lori Lamel', 'Jean-Luc Gauvain', 'Gilles Adda']",January 2002,Computer Speech & Language,[],Regular ArticleLightly supervised and unsupervised acoustic model training
856,"In this paper we define two alternatives to the familiar perplexity statistic (hereafter lexical perplexity), which is widely applied both as a figure of merit and as an objective function for training language models. These alternatives, respectively acoustic perplexity and the synthetic acoustic word error rate, fuse information from both the language model and the acoustic model. We show how to compute these statistics by effectively synthesizing a large acoustic corpus, demonstrate their superiority (on a modest collection of models and test sets) to lexical perplexity as predictors of language model performance, and investigate their use as objective functions for training language models. We develop an efficient algorithm for training such models, and present results from a simple speech recognition experiment, in which we achieved a small reduction in word error rate by interpolating a language model trained by synthetic acoustic word error rate with a unigram model.","['Harry Printz', 'Peder A. Olsen']",January 2002,Computer Speech & Language,[],Regular ArticleTheory and practice of acoustic confusability
857,,[],October 2001,Computer Speech & Language,[],Contents and index to Volume 15Contents and index to Volume 15
858,,[],October 2001,Computer Speech & Language,[],AnnouncementAnnouncement New Editor of Computer Speech and Language
859,,[],October 2001,Computer Speech & Language,[],AnnouncementAnnouncement 2001 Computer Speech and Language Paper Award
860,"This paper describes an investigation on the possibility of adding new features to classical Mel Scaled Cepstral Coefficients (MFCC) and their time derivatives. A hybrid Automatic Speech Recognition (ASR) system is used based on a Neural Network (NN) and a collection of Hidden Markov Models (HMM). It is shown that the gravity centres (GC) of energies in the frequency bands of the first three formants and their first and second time derivatives can be added to the classical set of MFCCs and their first and second time derivatives, resulting in significant performance improvements. Nevertheless, in some cases, the added parameters may nave a negative effect on performance, because the parameters are reliable only for certain types of sounds as their values may exhibit large variations for the same sound in the presence of additive noise. Experiments have shown that one solution is that of introducing a reliability index indicating the importance the newly added parameters should have in describing a given frame. NNs appear to be suitable devices for taking this fact into account in the computation of observation probabilities. Experiments have also shown improvements when GCs are computed from zero-crossing intervals detected at the output of the filters of an ear model. Intensities are obtained by associating a nonlinear peak amplitude coding to each zero-crossing interval. Consistent improvements are observed when the above-mentioned solutions are applied with medium as well as large size lexicons in the presence of additive noise.","['R.De Mori', 'L. Moisa', 'R. Gemello', 'F. Mana', 'D. Albesano']",October 2001,Computer Speech & Language,[],Regular ArticleAugmenting standard speech recognition features with energy gravity centres
861,"This article describes an unrestricted vocabulary text-to-speech (TTS) conversion system for the synthesis of Standard Arabic (SA) speech. The system uses short phonetic clusters that are derived from the Arabic syllables to synthesize Arabic. Basic and phonetic variants of the synthesis units are defined after qualitative and quantitative analyses of the phonetics of SA. A speech database of the synthesis units and their phonetic variations is created and the units are tested to control their segmental quality. Besides the types of synthesis unit used, their enhancement with phonetic variants, and their segmental quality control, the production of good quality speech also depends on waveform analysis and the method used to concatenate the synthesis units together. Waveform analysis is needed to condition the selected synthesis units at their junctures to produce synthesized speech of better quality. The types of speech juncture between contiguous units, the phonetic characteristics of the sounds surrounding the junctures and the concatenation artifacts occurring across the junctures are important and will be discussed. The results of waveform analysis and smoothing algorithms will be presented. The intelligibility of synthesized Arabic by a standard intelligibility test method that is adapted to suit the Arabic phonetic characteristics and scoring the results of the tests will also be dealt with.",['Yousif A. El-Imam'],October 2001,Computer Speech & Language,[],Regular ArticleSynthesis of Arabic from short sound clusters
862,"In this paper a new approach to Temporal Decomposition (TD) of speech, called Spectral Stability Based Event Localizing Temporal Decomposition (S2BEL-TD), is presented. The original method of TD proposed by Atal (1983, Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, 81–84) is known to have the drawbacks of high computational cost, and the high parameter sensitivity of the number and locations of events. In S2BEL-TD, the event localization is performed based on a maximum spectral stability criterion. This overcomes the high parameter sensitivity of events of Atal’s method. Also, S2BEL-TD avoids the use of the computationally costly singular value decomposition routine used in Atal’s method, thus resulting in a computationally simpler algorithm for TD. Simulation results show that an average spectral distortion of about 1.5 dB can be achieved with line spectral frequencies as the spectral parameter. It is shown that the temporal pattern of the speech excitation parameters can also be well described using the S2BEL-TD technique.","['A.C.R. Nandasena', 'P.C. Nguyen', 'M. Akagi']",October 2001,Computer Speech & Language,[],Regular ArticleSpectral stability based event localizing temporal decomposition
863,"In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser–Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.",['Joshua T. Goodman'],October 2001,Computer Speech & Language,[],Regular ArticleA bit of progress in language modeling
864,"This paper describes an articulatory speech production model trained on an X-ray microbeam database, and presents results of using the model within a speech recognition framework. The system uses an explicit statistical model of co-articulation to increase the accuracy of articulator trajectories synthesized from time-aligned phonetic strings, as compared with X-ray traces. From these trajectories, spectral vector probability distributions are generated using a set of artificial neural networks. The production model is then used in combination with a hidden Markov model recognition system to re-scoreN -best utterance transcription lists. Relative reductions in the word error rate of between 11% and 18% are achieved on a small recognition task.","['C Simon Blackburn', 'Steve Young']",July 2001,Computer Speech & Language,[],Regular ArticleEnhanced speech recognition using an articulatory production model trained on X-ray data
865,"This paper proposes a new statistical approach, namely the probabilistic union model, for speech recognition subjected to unknown burst noise during the utterance. The model combines the local temporal information based on the union of random events, to reduce the dependence of the model on information about the noise. This paper describes the theory of the model, and an implementation based on hidden Markov modeling techniques. For the evaluation, we used the TIDIGITS database for both isolated and connected digit recognition. The utterances were corrupted by various types of abrupt noise with unknown, time-varying characteristics. The experimental results indicate that the new model offers robustness to partial duration corruption, requiring no prior knowledge about the noise. A combination of the proposed model and conventional noise-reduction techniques is discussed, which has been shown to be potentially capable of dealing with a mixture of stationary noise and random, abrupt noise.","['Ji Ming', 'F. Jack Smith']",July 2001,Computer Speech & Language,[],Regular ArticleUnion: a model for partial temporal corruption of speech
866,"This paper discusses the generation of context description for an MLP/HMM speech recognition system such that the time span of the context can be dynamic during training and recognition. The context description is obtained using a window sliding over the feature vector sequence and a codebook classifying the feature vectors in the window. The classification results are stored in a matrix initially containing only zeros and having as many elements as there are codevectors in the codebook. Since each matrix element corresponds only to one codevector, the classification results of the feature vectors inside the window can be indicated by setting the corresponding element values to one. The dynamic nature of the context is obtained when the number of the feature vectors in the window is larger than the number of the elements in the matrix that are allowed to be one at the same time. When the dynamic context was included in the MLP/HMM recognition system, the string recognition accuracy of the test set increased from 92.9 to 93.8 % on average. This test set contained 29 188 Finnish digit strings. Half of this test set had a signal-to-noise ratio (SNR) of 20 dB, while the rest of this set had a SNR of  − 1.7 dB on average.",['Petri Salmela'],July 2001,Computer Speech & Language,[],Regular ArticleApplying dynamic context into MLP/HMM speech recognition system
867,"Speaker adaptation is recognized as an essential part of today’s large-vocabulary automatic speech recognition systems. A family of techniques that has been extensively applied for limited adaptation data is transformation-based adaptation. In transformation-based adaptation we partition our parameter space in a set of classes, estimate a transform (usually linear) for each class and apply the same transform to all the components of the class. It is known, however, that additional gains can be made if we do not constrain the components of each class to use the same transform. In this paper two speaker adaptation algorithms are described. First, instead of estimating one linear transform for each class (as maximum likelihood linear regression (MLLR) does, for example) we estimate multiple linear transforms per class of models and a transform weights vector which is specific to each component (Gaussians in our case). This in effect means that each component receives its own transform without having to estimate each one of them independently. This scheme, termed maximum likelihood stochastic transformation (MLST) achieves a good trade-off between robustness and acoustic resolution. MLST is evaluated on the Wall Street Journal(WSJ) corpus for non-native speakers and it is shown that in the case of 40 adaptation sentences the algorithm outperforms MLLR by more than 13%. In the second half of this paper, we introduce a variant of the MLST designed to operate under sparsity of data. Since the majority of the adaptation parameters are the transformations, we estimate them on the training speakers and adapt to a new speaker by estimating the transform weights only. First we cluster the speakers in a number of sets and estimate the transformations on each cluster. The new speaker will use transformations from all clusters to perform adaptation. This method, termed basis transformation, can be seen as a speaker similarity scheme. Experimental results on the WSJ show that when basis transformation is cascaded with MLLR marginal gains can be obtained from MLLR only, for adaptation of native speakers.","['Constantinos Boulis', 'Vassilios Diakoloukas', 'Vassilios Digalakis']",July 2001,Computer Speech & Language,[],Regular ArticleMaximum likelihood stochastic transformation adaptation for medium and small data sets
868,"In addition to ordinary words and names, real text contains non-standard “words"" (NSWs), including numbers, abbreviations, dates, currency amounts and acronyms. Typically, one cannot find NSWs in a dictionary, nor can one find their pronunciation by an application of ordinary “letter-to-sound"" rules. Non-standard words also have a greater propensity than ordinary words to be ambiguous with respect to their interpretation or pronunciation. In many applications, it is desirable to “normalize"" text by replacing the NSWs with the contextually appropriate ordinary word or sequence of words. Typical technology for text normalization involves sets of ad hoc rules tuned to handle one or two genres of text (often newspaper-style text) with the expected result that the techniques do not usually generalize well to new domains. The purpose of the work reported here is to take some initial steps towards addressing deficiencies in previous approaches to text normalization. We developed a taxonomy of NSWs on the basis of four rather distinct text types—news text, a recipes newsgroup, a hardware-product-specific newsgroup, and real-estate classified ads. We then investigated the application of several general techniques including n-gram language models, decision trees and weighted finite-state transducers to the range of NSW types, and demonstrated that a systematic treatment can lead to better results than have been obtained by the ad hoc treatments that have typically been used in the past. For abbreviation expansion in particular, we investigated both supervised and unsupervised approaches. We report results in terms of word-error rate, which is standard in speech recognition evaluations, but which has only occasionally been used as an overall measure in evaluating text normalization systems.","['Richard Sproat', 'Alan W. Black', 'Stanley Chen', 'Shankar Kumar', 'Mari Ostendorf', 'Christopher Richards']",July 2001,Computer Speech & Language,[],Regular ArticleNormalization of non-standard words
869,,"['Mari Ostendorf', 'Jan van Santen', 'Mark Clements']",July 2001,Computer Speech & Language,[],ObituaryObituary: M. W. Macon 1969–2001
870,"We consider the enhancement of speech corrupted by additive white Gaussian noise. In a Bayesian inference framework, maximum a posteriori (MAP) estimation of the signal is performed, along the lines developed by Lim & Oppenheim (1978). The speech enhancement problem is treated as a signal estimation problem, whose aim is to obtain a MAP estimate of the clean speech signal, given the noisy observations. The novelty of our approach, over previously reported work, is that we relate the variance of the additive noise and the gain of the autoregressive (AR) process to hyperparameters in a hierarchical Bayesian framework. These hyperparameters are computed from the noisy speech data to maximize the denominator in Bayes formula, also known as the evidence. The resulting Bayesian scheme is capable of performing speech enhancement from the noisy data without the need for silence detection. Experimental results are presented for stationary and slowly varying additive white Gaussian noise. The Bayesian scheme is also compared to the Lim and Oppenheim system, and the spectral subtraction method.","['Gaafar M.K. Saleh', 'Mahesan Niranjan']",April 2001,Computer Speech & Language,[],Regular ArticleSpeech enhancement using a Bayesian evidence approach
871,"The aim of this work is to show the ability of stochastic regular grammars to generate accurate language models which can be well integrated, allocated and handled in a continuous speech recognition system. For this purpose, a syntactic version of the well-known n -gram model, called k -testable language in the strict sense (k -TSS), is used. The complete definition of a k -TSS stochastic finite state automaton is provided in the paper. One of the difficulties arising in representing a language model through a stochastic finite state network is that the recursive schema involved in the smoothing procedure must be adopted in the finite state formalism to achieve an efficient implementation of the backing-off mechanism. The use of the syntactic back-off smoothing technique applied to k -TSS language modelling allowed us to obtain a self-contained smoothed model integrating several k -TSS automata in a unique smoothed and integrated model, which is also fully defined in the paper. The proposed formulation leads to a very compact representation of the model parameters learned at training time: probability distribution and model structure. The dynamic expansion of the structure at decoding time allows an efficient integration in a continuous speech recognition system using a one-step decoding procedure. An experimental evaluation of the proposed formulation was carried out on two Spanish corpora. These experiments showed that regular grammars generate accurate language models (k -TSS) that can be efficiently represented and managed in real speech recognition systems, even for high values of k, leading to very good system performance.","['I. Torres', 'A. Varona']",April 2001,Computer Speech & Language,[],Regular Articlek-TSS language models in speech recognition systems
872,"This paper presents a new architecture for automatic speech recognition systems which is characterized by the division of the spectral domain of the speech signal into several independent frequency bands. This model is based on the psycho-acoustic work of Fletcher (1953) who proposed a similar principle for the human auditory system. Jont B. Allen published a paper in 1994 in which he summarized the work of Fletcher and also proposed to adapt the multi-band paradigm to automatic speech recognition (ASR) (Allen, 1994). Many researchers have then studied this principle and built such ASR systems. The goal of this paper is to analyse some of the most important issues in the design of a multi-band ASR system in order to determine which architecture it should have in which environment. Two other major problems are then considered: how to train multi-band systems and how to use them for continuous ASR.","['Christophe Cerisara', 'Dominique Fohr']",April 2001,Computer Speech & Language,[],Regular ArticleMulti-band automatic speech recognition
873,"We investigate a statistical model for integrating narrowband cues in speech. The model is inspired by two ideas in human speech perception: (i) Fletcher’s hypothesis (1953) that independent detectors, working in narrow frequency bands, account for the robustness of auditory strategies, and (ii) Miller and Nicely’s analysis (1955) that perceptual confusions in noisy bandlimited speech are correlated with phonetic features. We apply the model to detecting the phonetic feature [ +  /  − sonorant] that distinguishes vowels, approximants, and nasals (sonorants) from stops, fricatives, and affricates (obstruents). The model is represented by a multilayer probabilistic network whose binary hidden variables indicate sonorant cues from different parts of the frequency spectrum. We derive the Expectation-Maximization algorithm for estimating the model’s parameters and evaluate its performance on clean and corrupted speech.","['Lawrence K. Saul', 'Mazin G. Rahim', 'Jont B. Allen']",April 2001,Computer Speech & Language,[],Regular ArticleA statistical model for robust integration of narrowband cues in speech
874,"Language models are usually evaluated on test texts using the perplexity derived from the model likelihood function computed on these texts (test set perplexity). In order to use this measure in the framework of a comparative evaluation campaign, we have developed an alternative scheme for estimating the test set perplexity. The method is derived from the Shannon game and based on a gambling approach on the next word to come in a truncated sentence. We also study the entropy bounds proposed by Shannon and based on the rank of the correct answer, in order to estimate a perplexity interval for non-probabilistic language models. The relevance of the approach is validated on an example. We then report the results of a preliminary comparative evaluation using the proposed scheme.","['Frédéric Bimbot', 'Marc El-Bèze', 'Stéphane Igounet', 'Michèle Jardino', 'Kamel Smaili', 'Imed Zitouni']",January 2001,Computer Speech & Language,[],Regular ArticleAn alternative scheme for perplexity estimation and its assessment for the evaluation of language models
875,"The widely used maximum likelihood linear regression speaker adaptation procedure suffers from overtraining when used for rapid adaptation tasks in which the amount of adaptation data is severely limited. This is a well known difficulty associated with the expectation maximization algorithm. We use an information geometric analysis of the expectation maximization algorithm as an alternating minimization of a Kullback–Leibler-type divergence to see the cause of this difficulty, and propose a more robust discounted likelihood estimation procedure. This gives rise to a discounted likelihood linear regression procedure, which is a variant of maximum likelihood linear regression suited for small adaptation sets. Our procedure is evaluated on an unsupervised rapid adaptation task defined on the Switchboard conversational telephone speech corpus, where our proposed procedure improves word error rate by 1.6% (absolute) with as little as 5 seconds of adaptation data, which is a situation in which maximum likelihood linear regression overtrains in the first iteration of adaptation. We compare several realizations of discounted likelihood linear regression with maximum likelihood linear regression and other simple maximum likelihood linear regression variants, and discuss issues that arise in implementing our discounted likelihood procedures.","['Asela Gunawardana', 'William Byrne']",January 2001,Computer Speech & Language,[],Regular ArticleDiscounted likelihood linear regression for rapid speaker adaptation
876,"This paper explores the interaction between a language model’s perplexity and its effect on the word error rate of a speech recognition system. Much recent research has indicated that these two measures are not as well correlated as was once thought, and many examples exist of models which have a much lower perplexity than the equivalent N -gram model, yet lead to no improvement in recognition accuracy. This paper investigates the reasons for this apparent discrepancy. Perplexity’s calculation is based solely on the probabilities of words contained within the test text; it disregards the probabilities of alternative words which will be competing with the correct word within the decoder. It is shown that by considering the probabilities of the alternative words it is possible to derive measures of language model quality which are better correlated with word error rate than perplexity is. Furthermore, optimizing language model parameters with respect to these new measures leads to a significant reduction in the word error rate.","['Philip Clarkson', 'Tony Robinson']",January 2001,Computer Speech & Language,[],Regular ArticleImproved language modelling through better language model evaluation measures
877,"We introduce an exponential language model which models a whole sentence or utterance as a single unit. By avoiding the chain rule, the model treats each sentence as a “bag of features"", where features are arbitrary computable properties of the sentence. The new model is computationally more efficient, and more naturally suited to modeling global sentential phenomena, than the conditional exponential (e.g. maximum entropy) models proposed to date. Using the model is straightforward. Training the model requires sampling from an exponential distribution. We describe the challenge of applying Monte Carlo Markov Chain and other sampling techniques to natural language, and discuss smoothing and step-size selection. We then present a novel procedure for feature selection, which exploits discrepancies between the existing model and the training corpus. We demonstrate our ideas by constructing and analysing competitive models in the Switchboard and Broadcast News domains, incorporating lexical and syntactic information.","['Ronald Rosenfeld', 'Stanley F. Chen', 'Xiaojin Zhu']",January 2001,Computer Speech & Language,[],Regular ArticleWhole-sentence exponential language models: a vehicle for linguistic-statistical integration
878,"This study describes tree-based intonation modeling for text-to-speech systems. The procedure for generating intonation consists of two phases: assigning tonal labels and predicting fundamental frequency contours, which are done by using tree-structured predictors. The decision tree assigns tonal labels to the syllables, and 10 sampled pitch values per syllable are predicted by a vector-regression tree. Additionally, we apply a bootstrap aggregating technique to improve the performance of the trees and create a born again tree as a representer of the multiple tree predictor. We collected 500 Korean sentences and their corresponding speech corpus, trained trees on 300 sentences, and tested them on 200 sentences. The overall misclassification rates of the born again trees were about 37% for boundary tone prediction and about 19% for non-boundary tone prediction. In the experiment on fundamental frequency contour prediction, we compared the tree-based modeling and linear regression methods with objective and subjective measures. The correlation coefficient for the observed pitch values and those predicted by the born again tree was 0.812, while a linear regression method yielded 0.805. In the subjective test, native Korean subjects clearly preferred the intonation generated by the born again vector-regression tree to that by the linear regression method.","['Sangho Lee', 'Yung-Hwan Oh']",January 2001,Computer Speech & Language,[],Regular ArticleTree-based modeling of intonation
879,,"['Marilyn Walker', 'Owen Rambow']",January 2001,Computer Speech & Language,[],Regular ArticleCall for Papers Special Issue on Spoken Language Generation
