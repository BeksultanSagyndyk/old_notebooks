,Abstract,Author,Date,Jornal,Keywords,Title
2,This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate—word lattice rescoring—over the standard 3-gram language model.,"['Ciprian Chelba', 'Frederick Jelinek']",October 2000,Computer Speech & Language,[],Regular ArticleStructured language modeling
3,"We report work on the first component of a two-stage speech recognition architecture based onphonological features rather than phones. This paper reports experiments on three phonological feature systems: (1) the Sound Pattern of English (SPE) system which uses binary features, (2) amulti-valued (MV) feature system which uses traditional phonetic categories such as manner, place, etc., and (3)Government Phonology (GP) which uses a set of structured primes. All experiments used recurrent neural networks to perform feature detection. In these networks the input layer is a standard framewise cepstral representation, and the output layer represents the values of the features. The system effectively produces a representation of the most likely phonological features for each input frame. All experiments were carried out on the TIMIT speaker-independent database. The networks performed well in all cases, with the average accuracy for a single feature ranging from 86% and 93%. We describe these experiments in detail, and discuss the justification and potential advantages of using phonological features rather than phones for the basis of speech recognition.","['Simon King', 'Paul Taylor']",October 2000,Computer Speech & Language,[],Regular ArticleDetection of phonological features in continuous speech using neural networks
4,A new statistical language model is presented which combines collocational dependencies with two important sources of long-range statistical dependence: the syntactic structure and the topic of a sentence. These dependencies or constraints are integrated using the maximum entropy technique. Substantial improvements are demonstrated over a trigram model in both perplexity and speech recognition accuracy on the Switchboard task. A detailed analysis of the performance of this language model is provided in order to characterize the manner in which it performs better than a standard N -gram model. It is shown that topic dependencies are most useful in predicting words which are semantically related by the subject matter of the conversation. Syntactic dependencies on the other hand are found to be most helpful in positions where the best predictors of the following word are not within N -gram range due to an intervening phrase or clause. It is also shown that these two methods individually enhance an N -gram model in complementary ways and the overall improvement from their combination is nearly additive.,"['Sanjeev Khudanpur', 'Jun Wu']",October 2000,Computer Speech & Language,[],"Regular ArticleMaximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling"
5,"We describe a new framework for distilling information from word lattices to improve the accuracy of the speech recognition output and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of a set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.","['Lidia Mangu', 'Eric Brill', 'Andreas Stolcke']",October 2000,Computer Speech & Language,[],Regular ArticleFinding consensus in speech recognition: word error minimization and other applications of confusion networks☆
8,"This paper outlines ProSynth, an approach to speech synthesis which takes a rich linguistic structure as central to the generation of natural-sounding speech. We start from the assumption that the acoustic richness of the speech signal reflects linguistic structural richness and underlies the percept of naturalness. Naturalness achieved by paying attention to systematic phonetic detail in the spectral, temporal and intonational domains produces a perceptually robust signal that is intelligible in adverse listening conditions. ProSynth uses syntactic and phonological parses to model the fine acoustic–phonetic detail of real speech. We present examples of our approach to modelling systematic segmental, temporal and intonational detail and show how all are integrated in the prosodic structure. Preliminary tests to evaluate the effects of modelling systematic fine spectral detail, timing, and intonation suggest that the approach increases intelligibility and naturalness.","['Richard Ogden', 'Sarah Hawkins', 'Jill House', 'Mark Huckvale', 'John Local', 'Paul Carter', 'Jana Dankovičová', 'Sebastian Heid']",July 2000,Computer Speech & Language,[],"Regular ArticleProSynth: an integrated prosodic approach to device-independent, natural-sounding speech synthesis"
9,"Acoustic transmission in the vocal tract may be simulated in the time domain using the model of Kelly and Lochbaum. A disadvantage of this simulation is that a fixed number of fixed length sections must be used, so that it cannot be used to model variability in vocal tract length, caused by lip protrusion or larynx lowering. This paper describes a simple modification in which digital filters, derived from transmission line T -sections and including glottal and lip impedance models, are appended at each end of a Kelly–Lochbaum filter. The lengths of the sections may be made continuously variable, allowing the lip and larynx segments of the model to be varied, while maintaining a fixed sampling rate. This new technique is compared with the earlier method due to Strube and is found capable of longer extensions and reduced spectral amplitude distortion.",['C.C. Goodyear'],July 2000,Computer Speech & Language,[],Regular ArticleIncorporating lip protrusion and larynx lowering into a time domain model for articulatory speech synthesis
10,"We describe some new methods for constructing discrete acoustic phonetic hidden Markov models (HMMs) using tree quantizers having very large numbers (16–64 K) of leaf nodes and tree-structured smoothing techniques. We consider two criteria for constructing tree quantizers (minimum distortion and minimum entropy) and three types of smoothing (mixture smoothing, smoothing by adding 1 and Gaussian smoothing). We show that these methods are capable of achieving recognition accuracies which are generally comparable to those obtained with Gaussian mixture HMMs at a computational cost which is only marginally greater than that of conventional discrete HMMs. We present some evidence of superior performance in situations where the number of HMM distributions to be estimated is small compared with the amount of training data. We also show how our methods can accommodate feature vectors of much higher dimensionality than are traditionally used in speech recognition.","['M. Barszcz', 'W. Chen', 'G. Boulianne', 'P. Kenny']",July 2000,Computer Speech & Language,[],Regular ArticleTree-structured vector quantization for speech recognition
11,"HMM acoustic models are typically trained on a single set of cepstral features extracted over the full bandwidth of mel-spaced filterbank energies. In this paper, multi-resolution sub-band transformations of the log energy spectra are introduced based on the conjecture that additional cues for phonetic discrimination may exist in the local spectral correlates not captured by the full-band analysis. In this approach the discriminative contribution from sub-band features is considered to supplement rather than substitute for full-band features. HMMs trained on concatenated multi-resolution cepstral features are investigated, along with models based on linearly combined independent multi-resolution streams, in which the sub-band and full-band streams represent different resolutions of the same signal. For the stream-based models, discriminative training of the linear combination weights to a minimum classification error criteria is also applied. Both the concatenated feature and the independent stream modelling configurations are demonstrated to outperform traditional full-band cepstra for HMM-based acoustic phonetic modelling on the TIMIT database. Experiments on context-independent modelling achieve a best increase on the core test set from an accuracy of 62.3% for full-band models to a 67.5% accuracy for discriminately weighted multi-resolution sub-band modelling. A triphone accuracy of 73.9% achieved on the core test set improves notably on full-band cepstra and compares well with results previously published on this task.","['P.M. McCourt', 'S.V. Vaseghi', 'B. Doherty']",July 2000,Computer Speech & Language,[],Regular ArticleMulti-resolution sub-band features and models for HMM-based phonetic modelling
12,"In this paper, we present a stochastic language-modeling tool which aims at retrieving variable-length phrases (multigrams), assuming n -gram dependencies between them, hence the name of the model: n -multigram. The estimation of the probability distribution of the phrases is intermixed with a phrase-clustering procedure in a way which jointly optimizes the likelihood of the data. As a result, the language data are iteratively structured at both a paradigmatic and a syntagmatic level in a fully integrated way. We evaluate the 2-multigram model as a statistical language model on ATIS, a task-oriented database consisting of air travel reservations. Experiments show that the 2-multigram model allows a reduction of 10% of the word error rate on ATIS with respect to the usual trigram model, with 25% fewer parameters than in the trigram model. In addition, we illustrate the ability of this model to merge semantically related phrases of different lengths into a common class.","['Sabine Deligne', 'Yoshinori Sagisaka']",July 2000,Computer Speech & Language,[],Regular ArticleStatistical language modeling with a class-basedn-multigram model
14,"Based on the log-normal assumption, parallel model combination (PMC) provides an effective method to adapt the cepstral means and variances of speech models for noisy speech recognition. In addition, the log-add method has been derived to adapt the mean by ignoring the cepstral variance during the process of PMC. This method is efficient for speech recognition in a high signal-to-noise ratio (SNR) environment. In this paper, a new interpretation of the log-add method is proposed. This leads to a modified scheme for performing the adaptation procedure in PMC. This modified method is shown to be efficient in improving recognition accuracy in low SNR. Based on this modified PMC method, we derive a direct adaptation procedure for the variance of speech models in the cepstral domain. The proposed method is a fast algorithm because the computation for the transformation of the covariance matrix is no longer required. Three recognition tasks are conducted to evaluate the proposed method. Experimental results show that the proposed technique not only requires lower computational cost but it also outperforms the original PMC technique in noisy environments.","['Tai-Hwei Hwang', 'Hsiao-Chuan Wang']",April 2000,Computer Speech & Language,[],Regular ArticleA fast algorithm for parallel model combination for noisy speech recognition
15,"In this paper we report our recent research whose goal is to improve the performance of a novel speech recognizer based on an underlying statistical hidden dynamic model of phonetic reduction in the production of conversational speech. We have developed a path-stack search algorithm which efficiently computes the likelihood of any observation utterance while optimizing the dynamic regimes in the speech model. The effectiveness of the algorithm is tested on the speech data in the Switchboard corpus, in which the optimized dynamic regimes computed from the algorithm are compared with those from exhaustive search. We also present speech recognition results on the Switchboard corpus that demonstrate improvements of the recognizer’s performance compared with the use of the dynamic regimes heuristically set from the phone segmentation by a state-of-the-art hidden Markov model (HMM) system.","['Jeff Z. Ma', 'Li Deng']",April 2000,Computer Speech & Language,[],Regular ArticleA path-stack algorithm for optimizing dynamic regimes in a statistical hidden dynamic model of speech
16,"In this paper we address application of minimum Bayes-risk classifiers to tasks in automatic speech recognition (ASR). Minimum-risk classifiers are useful because they produce hypotheses in an attempt to be optimal under a specified task-dependent performance criterion. While the form of the optimal classifier is well known, its implementation is prohibitively expensive. We present efficient approximations that can be used to implement these procedures. In particular, anA *  search over word lattices produced by a conventional ASR system is described. This algorithm is intended to extend the previously proposed N -best list rescoring approximation to minimum-risk classifiers. We provide experimental results showing that both the A * and N -best list rescoring implementations of minimum-risk classifiers yield better recognition accuracy than the commonly used maximum a posteriori probability (MAP) classifier in word transcription and identification of keywords. TheA *  implementation is compared to the N -best list rescoring implementation and is found to obtain modest but significant improvements in accuracy at little additional computational cost. Another application of minimum-risk classifiers for the identification of named entities from speech is presented. Only the N -best list rescoring could be implemented for this task and was found to yield better named entity identification performance than the MAP classifier.","['Vaibhava Goel', 'William J Byrne']",April 2000,Computer Speech & Language,[],Regular ArticleMinimum Bayes-risk automatic speech recognition
17,"Conversational speech exhibits considerable pronunciation variability, which has been shown to have a detrimental effect on the accuracy of automatic speech recognition. There have been many attempts to model pronunciation variation, including the use of decision trees to generate alternate word pronunciations from phonemic baseforms. Use of pronunciation models during recognition is known to improve accuracy. This paper describes the incorporation of pronunciation models into acoustic model training in addition to recognition. Subtle difficulties in the straightforward use of alternatives to canonical pronunciations are first illustrated: it is shown that simply improving the accuracy of the phonetic transcription used for acoustic model training is of little benefit. Acoustic models trained on the most accurate phonetic transcriptions result in worse recognition than acoustic models trained on canonical baseforms. Analysis of this counterintuitive result leads to a new method of accommodating nonstandard pronunciations: rather than allowing a phoneme in the canonical pronunciation to be realized as one of a few distinct alternate phones, the hidden Markov model (HMM) states of the phoneme’s model are instead allowed to share Gaussian mixture components with the HMM states of the model(s) of the alternate realization(s). Qualitatively, this amounts to making a soft decision about which surface form is realized. Quantitatively, experiments show that this method is particularly well suited for acoustic model training for spontaneous speech: a 1.7 %(absolute) improvement in recognition accuracy on the Switchboard corpus is presented.","['Murat Saraçlar', 'Harriet Nock', 'Sanjeev Khudanpur']",April 2000,Computer Speech & Language,[],Regular ArticlePronunciation modeling by sharing Gaussian densities across phonetic models
18,"Between 1990 and 1998, the Speech Research Unit at the Defence Evaluation and Research Agency (DERA), and Hereford and Worcester County Council Education Department, a U.K. local education authority, conducted research into the use of speech recognition technology in an interactive computer-based pronunciation tutor for 5–7 year-old primary school children. The goal of the project was to develop a robust, autonomous system that would enable a child to practice the pronunciation of a given set of words by speaking them to a computer, which provided immediate feedback on whether the pronunciation was acceptable. This paper describes the development of the underlying speech recognition technology, the prototype real-time system which was developed, and the results of pilot trials of the system in a U.K. primary school.","['Martin Russell', 'Robert W. Series', 'Julie L. Wallace', 'Catherine Brown', 'Adrian Skilling']",April 2000,Computer Speech & Language,[],Regular ArticleThe STAR system: an interactive pronunciation tutor for young children
19,"Vowel harmony is a pervasive feature in Finnish. It is only rarely violated in slips of the tongue or in aphasic output. Thus it is a constraint which should be successfully simulated by any model aiming to account for word production in Finnish. Only a few neural network models for Finnish language processing have been developed so far, although Finnish should be a very challenging object for modeling because of its complex morphology. Our work is the first attempt to model Finnish vowel harmony using neural networks. We have introduced a tool, FinnPro, for building interactive spreading activation models and for simulating word production processes. Our tool produces Finnish nouns in different cases and forms and simulates both normal and damaged speech. In this paper, the vowel harmony control features of FinnPro are introduced. The control structure of FinnPro maintains vowel harmony even in damaged word production. The vowel harmony adjustment process in FinnPro follows the idea that the vowel harmony process in Finnish is phonological. Our model suggests that in disturbed word productions, the vowel harmony category of the output word is triggered by the whole set of vowels activated in the production.",['Anneli Tikkala'],January 2000,Computer Speech & Language,[],Regular ArticleA connectionist word production tool for Finnish nouns with a model for vowel harmony restrictions
20,This paper presents two look-ahead techniques for speeding up large vocabulary continuous speech recognition. These two techniques are the language model look-ahead and the phoneme look-ahead; both are incorporated into the pruning process of the time-synchronous one-pass beam search algorithm. The search algorithm is based on a tree-organized pronunciation lexicon in connection with a bigram language model. Both look-ahead techniques have been tested on the 20 000-word NAB’94 task (ARPA North American Business Corpus). The recognition experiments show that the combination of bigram language model look-ahead and phoneme look-ahead reduces the size of search space by a factor of about 30 and the computational effort by a factor of 5 without affecting the word recognition accuracy in comparison with no look-ahead pruning technique.,"['Stefan Ortmanns', 'Hermann Ney']",January 2000,Computer Speech & Language,[],Regular ArticleLook-ahead techniques for fast beam search☆
21,"This paper introduces a new form of observation distributions for hidden Markov models (HMMs), combining subvector quantization and mixtures of discrete distributions. Despite what is generally believed, we show that discrete-distribution HMMs can outperform continuous-density HMMs at significantly faster decoding speeds. Performance of the discrete HMMs is improved by using product-code vector quantization (VQ) and mixtures of discrete distributions. The decoding speed of the discrete HMMs is also improved by quantizing subvectors of coefficients, since this reduces the number of table lookups needed to compute the output probabilities. We present efficient training and decoding algorithms for the discrete-mixture HMMs (DMHMMs). Our experimental results in the air-travel information domain show that the high level of recognition accuracy of continuous-mixture-density HMMs (CDHMMs) can be maintained at significantly faster decoding speeds. Moreover, we show that when the same number of mixture components is used in DMHMMs and CDHMMs, the new models exhibit superior recognition performance.","['V. Digalakis', 'S. Tsakalidis', 'C. Harizakis', 'L. Neumeyer']",January 2000,Computer Speech & Language,[],Regular ArticleEfficient speech recognition using subvector quantization and discrete-mixture HMMS
22,"This paper describes the details of a fast, memory-efficient one-pass stack decoder for efficient evaluation of the search space for large vocabulary continuous speech recognition. A modern, efficient search engine is not based on a single idea, but is a rather complex collection of separate algorithms and practical implementation details, which only in combination make the search efficient in time and memory requirements. Being the core of a speech recognition system, the software design phase for a new decoder is often crucial for its later performance and flexibility. This paper tries to emphasize this point—after defining the requirements for a modern decoder, it describes the details of an implementation that is based on a stack decoder framework. It is shown how it is possible to handle arbitrary order N -grams, how to generate N -best lists or lattices next to the first-best hypothesis at little computational overhead, how to handle efficiently cross-word acoustic models of any context order, how to efficiently constrain the search with word graphs or word-pair grammars, and how to use a fast-match with delay to speed up the search, all in a single left-to-right search pass. The details of a disk-based representation of an N -gram language model are given, which make it possible to use language models (LMs) of arbitrary (file) size in only a few hundred kB of memory. On-demand N -gram smearing, an efficient improvement over the regular unigram smearing used as an approximation to the LM scores in a tree lexicon, is introduced. It is also shown how lattice rescoring, the generation of forced alignments and detailed phone-/state-alignments can efficiently be integrated into a single stack decoder. The decoder named “Nozomi""1was tested on a Japanese newspaper dictation task using a 5000 word vocabulary. Using computationally cheap models it is possible to achieve real-time performance with 89% word recognition accuracy at about 1% search error using only 4 MB of total memory on a 300 MHz Pentium II. With computationally more expensive acoustic models, which also cover for the Japanese language essential cross-word effects, more than 95% recognition accuracy2These are currently the best reported results on this task. is reached.",['Mike Schuster'],January 2000,Computer Speech & Language,[],Regular ArticleMemory-efficient LVCSR search using a one-pass stack decoder
24,"Confidence measures enable us to assess the output of a speech recognition system. The confidence measure provides us with an estimate of the probability that a word in the recognizer output is either correct or incorrect. In this paper we discuss ways in which to quantify the performance of confidence measures in terms of their discrimination power and bias. In particular, we analyze two different performance metrics: the classification equal error rate and the normalized mutual information metric. We then report experimental results of using these metrics to compare four different confidence measure estimation schemes. We also discuss the relationship between these metrics and the operating point of the speech recognition system and develop an approach to the robust estimation of normalized mutual information.","['Manhung Siu', 'Herbert Gish']",October 1999,Computer Speech & Language,[],Regular ArticleEvaluation of word confidence for speech recognition systems
25,"The Bell Labs multilingual text-to-speech system can be characterized as consisting of a set of language-independent modules. Any language-specific information is represented in, and at run-time retrieved from, precompiled tables, models and finite-state transducers. In this paper we present a detailed description of the German version of the Bell Labs text-to-speech system. We will first discuss aspects of text analysis and our solutions to the problems they pose. Some of these problems, such as the expansion of numbers and abbreviations, and proper name pronunciation, occur in many languages while others, such as productive compounding, are specific to German and several related languages. We will then report on the construction of models for segmental duration and intonation. Finally, we will explain the design and structure of the acoustic inventory for concatenative synthesis and the criteria and procedures that were used to build it.",['Bernd Möbius'],October 1999,Computer Speech & Language,[],Regular ArticleThe Bell Labs German text-to-speech system
26,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.","['Stanley F. Chen', 'Joshua Goodman']",October 1999,Computer Speech & Language,[],Regular ArticleAn empirical study of smoothing techniques for language modeling
27,"In this paper we introduce a set of related confidence measures for large vocabulary continuous speech recognition (LVCSR) based on local phone posterior probability estimates output by an acceptor HMM acoustic model. In addition to their computational efficiency, these confidence measures are attractive as they may be applied at the state-, phone-, word- or utterance-levels, potentially enabling discrimination between different causes of low confidence recognizer output, such as unclear acoustics or mismatched pronunciation models. We have evaluated these confidence measures for utterance verification using a number of different metrics. Experiments reveal several trends in “profitability of rejection"", as measured by the unconditional error rate of a hypothesis test. These trends suggest that crude pronunciation models can mask the relatively subtle reductions in confidence caused by out-of-vocabulary (OOV) words and disfluencies, but not the gross model mismatches elicited by non-speech sounds. The observation that a purely acoustic confidence measure can provide improved performance over a measure based upon both acoustic and language model information for data drawn from the Broadcast News corpus, but not for data drawn from the North American Business News corpus suggests that the quality of model fit offered by a trigram language model is reduced for Broadcast News data. We also argue that acoustic confidence measures may be used to inform the search for improved pronunciation models.","['Gethin Williams', 'Steve Renals']",October 1999,Computer Speech & Language,[],Regular ArticleConfidence measures from local posterior probability estimates
29,"A new system for the automatic segmentation and labeling of Mandarin speech is presented. The system is capable of labeling speech generated without requiring extensive linguistic knowledge or large training databases. In this work, a new approach using fuzzy implication was used to design a consonant/vowel segmentation method with a high accuracy rate and robustness to background noise for Mandarin syllable recognition systems. To find the abrupt spectral difference change of adjacent frames, we used the Lukasiewicz implication and the Gains implication to design a spectral distance measuring rule, which disregards small spectral differences and enhances large spectral differences. After measurement, the largest spectral difference of adjacent frames determines the segmentation of each consonant/vowel. The experimental results show that the accuracy of the obtained automatic segmentation is comparable to that of a human expert. The performance of our system appears to compare favorably with that of other systems.","['Ming-Tzaw Lin', 'Ching-Kuen Lee', 'Chin-Yi Lin']",July 1999,Computer Speech & Language,[],Regular ArticleConsonant/vowel segmentation for Mandarin syllable recognition
30,"This paper presents a new approach to speech synthesis in which a set of cross-word decision-tree state-clustered context-dependent hidden Markov models are used to define a set of subphone units to be used in a concatenation synthesizer. The models, trees, waveform segments and other parameters representing each clustered state are obtained completely automatically through training on a 1 hour single-speaker continuous-speech database. During synthesis the required utterance, specified as a string of words of known phonetic pronounciation, is generated as a sequence of these clustered states using a TD-PSOLA waveform concatenation synthesizer. The system produces speech which, though in a monotone, is both natural sounding and highly intelligible. A Modified Rhyme Test conducted to measure segmental intelligibility yielded a 5· 0 % error rate. The speech produced by the system mimics the voice of the speaker used to record the training database. The system can be retrained on a new voice in less than 48 hours, and has been successfully trained on four voices.","['R.E. Donovan', 'P.C. Woodland']",July 1999,Computer Speech & Language,[],Regular ArticleA hidden Markov-model-based trainable speech synthesizer
31,"This paper investigates low computational and memory cost approaches to open set keyword spotting for audio document retrieval. In many applications the document vocabulary will be fluid and user specific. An open, rather than pre-defined, keyword set is needed to retrieve these documents. Defining the keyword at the time of the search request raises two issues not found in a fixed keyword system, (i) how to model the new keyword and (ii) how to perform the word spotting pass to achieve a resonable retrieval response time. This paper presents Viterbi-based approaches to both of these. The keyword is modelled by one or more phonetic strings. A phone-level recognizer is used to determine the keyword phone string (KPS) from one or more spoken keyword examples. Generating multiple pronunciations using an n -best recognizer was found to give better word-spotting performance than using the 1-best KPS. A more robust KPS was created from 2 compared to 1 keyword samples. Three techniques are presented to reduce the time required to perform the word-spotting search: approximation of the full keyword plus filler recognition pass using the pre-computed Viterbi filler hypothesis; restricting the search space by dynamically matching the KPS against the filler path; and Gaussian Selection. An overall speed-up of  × 50 in retrieval time was achieved by combining these techniques.","['K.M. Knill', 'S.J. Young']",July 1999,Computer Speech & Language,[],Regular ArticleLow-cost implementation of open set keyword spotting
32,"Standard statistical language modeling techniques suffer from sparse-data problems in tasks where large amounts of domain-specific text are not available. In this paper, we focus on improving the estimation of domain-dependent n -gram models by the selective use of out-of-domain text data. Previous approaches for estimating language models from multi-domain data have not accounted for the characteristic variations of style and content across domains. In contrast, this work aims at differentially weighting subsets of the out-of-domain data according to style and/or content similarity to the given task, where “style"" is represented by part-of-speech statistics and “content"" by the particular choice of vocabulary items. In addition to n -gram estimation, the differential weights can be used for lexicon design. Recognition experiments are based on the Switchboard corpus of spontaneous conversations, with out-of-domain text drawn from the Wall Street Journal and Broadcast News corpora. The similarity weighting approach gives a 3–5% reduction in word error rate over a domain-specific n -gram language model, providing some of the largest language modeling gains reported for the Switchboard task in recent years.","['R. Iyer', 'M. Ostendorf']",July 1999,Computer Speech & Language,[],Regular ArticleRelevance weighting for combining multi-domain data for n-gram language modeling
33,"In this paper, we present a family of maximum likelihood (ML) techniques that aim at reducing an acoustic mismatch between the training and testing conditions of hidden Markov model (HMM)-based automatic speech recognition (ASR) systems. Our study is conducted in two phases. In the first phase, we evaluate two classes of robustness techniques; those that represent the acoustic mismatch for the entire utterance as a single additive bias and those that represent the mismatch as a non-stationary bias. In the second phase, we propose a codebook-based stochastic matching (CBSM) approach for bias removal both at the feature level and at the model level. CBSM associates each bias with an ensemble of HMM mixture components that share similar acoustic characteristics. It is integrated with hierarchical signal bias removal and further extended to account for n -best candidates. Experimental results on connected digits, recorded over a cellular network, shows that incorporating bias removal reduces both the word and string error rates by about 12% and 16%, respectively, when using a global bias, and 36% and 31%, respectively, when using a non-stationary bias.","['Craig Lawrence', 'Mazin Rahim']",July 1999,Computer Speech & Language,[],Regular ArticleIntegrated bias removal techniques for robust speech recognition☆
34,"While n-gram modeling is simple and dominant in speech recognition, it can only capture the short-distance context dependency within an n-word window where currently the largest practical n for natural language is three. However, many of the context dependencies in natural language occur beyond a three-word window. This paper proposes a new language modeling approach to capture the preferred relationships between words over a short or long distance through the concept of MI-Trigger pairs. Different MI-Trigger-based models are constructed in either a distance-dependent or a distance-independent way within a window from 1 to 10 words. This new MI-Trigger-based modeling is also compared and merged with word bigram modeling. It is found that the MI-Trigger-based modeling has better performance than word bigram modeling. It is also found that n-gram and MI-Trigger models have good complementarity and their proper merging can further increase the recognition rate when tested on Mandarin speech recognition. One advantage of MI-Trigger-based modeling is that the number of parameters needed for MI-Trigger modeling is much less than that of word bigram modeling. Another advantage is that the number of trigger pairs in an MI-Trigger model can be kept to a reasonable size without losing too much of its modeling power.","['Z. GuoDong', 'L. KimTeng']",April 1999,Computer Speech & Language,[],Regular ArticleInterpolation of n-gram and mutual-information based trigger pair language models for Mandarin speech recognition
35,"In this paper, we propose a POS (part-of-speech)-dependent multiple pronunciation dictionary generation method using HMM-state confusions spanning several phonemes. When used in a multi-pass search, a dictionary generated from the method makes it possible to recover missing words that are lost during the first pass of the search process in continuous speech recognition using a single pronunciation dictionary. The new pronunciations are added to a dictionary that considers the POS dependency of the confusion characteristics. Continuous word recognition experiments have confirmed that the best results are obtained when (1) confusions expressed by HMM-state sequences and (2) pronunciation variations considering the POS-dependent confusion characteristics are used.","['Y. Wakita', 'H. Singer', 'Y. Sagisaka']",April 1999,Computer Speech & Language,[],Regular ArticleMultiple pronunciation dictionary using HMM-state confusion characteristics
36,"The automatic derivation of word pronunciations from input text is a central task for any text-to-speech system. For general English text at least, this is often thought to be a solved problem, with manually-derived linguistic rules assumed capable of handling “novel” words missing from the system dictionary. Data-driven methods, based on machine learning of the regularities implicit in a large pronouncing dictionary, have received considerable attention recently but are generally thought to perform less well. However, these tentative beliefs are at best uncertain without powerful methods for comparing text-to-phoneme subsystems. This paper contributes to the development of such methods by comparing the performance of four representative approaches to automatic phonemization on the same test dictionary. As well as rule-based approaches, three data-driven techniques are evaluated: pronunciation by analogy (PbA), NETspeak and IB1-IG (a modifiedk-nearest neighbour method). Issues involved in comparative evaluation are detailed and elucidated. The data-driven techniques outperform rules in accuracy of letter-to-phoneme translation by a very significant margin but require aligned text-phoneme training data and are slower. Best translation results are obtained with PbA at approximately 72% words correct on a resonably large pronouncing dictionary, compared with something like 26% words correct for the rules, indicating that automatic pronunciation of text is not a solved problem.","['R.I. Damper', 'Y. Marchand', 'M.J. Adamson', 'K. Gustafson']",April 1999,Computer Speech & Language,[],Regular ArticleEvaluating the pronunciation component of text-to-speech systems for English: a performance comparison of different approaches
37,"We report our experience of applying a stochastic method for understanding natural language to a multilingual appointment scheduling task, in particular, to the English spontaneous speech task (ESST). The aim of the spoken language systems developed for this task is to translate spontaneous conversational speech among different languages. We have investigated the portability of a stochastic semantic analyser from a setting of human–machine interactions air travel information services (ATIS) and multimodal multimedia automated service kiosk (MASK) into the more open one of human-to-human interactions (ESST).","['W. Minker', 'M. Gavaldà', 'A. Waibel']",April 1999,Computer Speech & Language,[],Regular ArticleStochastically-based semantic analysis for machine translation
38,"A crucial issue in triphone-based continuous-speech recognition is the large number of parameters to be estimated against the limited availability of training data. This problem can be relieved by composing a triphone model from less context-dependent models. This paper introduces a new statistical framework, derived from Bayesian statistics, to perform such a composition. The potential power of this new framework is explored, both algorithmically and experimentally, by an implementation with hidden Markov modelling techniques. In particular, we describe an implementation based on the mixture-Gaussian hidden Markov models (HMMs) incorporating state-level parameter tying. This implemented model is applied to the recognition of the 39-phone set on the TIMIT database, achieving 74.4% and 75.6% accurate, respectively, on the core and complete test sets.","['Ji Ming', 'F.J. Smith']",April 1999,Computer Speech & Language,[],Regular ArticleA Bayesian triphone model
40,"“Segmental hidden Markov models” (SHMMs) are intended to overcome important speech-modelling limitations of the conventional-HMM approach by representing sequences (or segments) of features and incorporating the concept of trajectories to describe how features change over time. A novel feature of the approach presented in this paper is thatextra-segmentalvariability between different examples of a sub-phonemic speech segment is modelled separately fromintra-segmentalvariability within any one example. The extra-segmental component of the model is represented in terms of variability in the trajectory parameters, and these models are therefore referred to as “probabilistic-trajectory segmental HMMs” (PTSHMMs). This paper presents the theory of PTSHMMs using a linear trajectory description characterized by slope and mid-point parameters, and presents theoretical and experimental comparisons between different types of PTSHMMs, simpler SHMMs and conventional HMMs.Experiments have demonstrated that, for any given feature set, a linear PTSHMM can substantially reduce the error rate in comparison with a conventional HMM, both for a connected-digit recognition task and for a phonetic classification task. Performance benefits have been demonstrated from incorporating a linear trajectory description and additionally from modelling variability in the mid-point parameter.","['W.J. Holmes', 'M.J. Russell']",January 1999,Computer Speech & Language,[],Regular ArticleProbabilistic-trajectory segmental HMMs☆
41,"Our recent development of a computational cochlear-nucleus-like network model for the study of speech encoding mechanisms associated with parts of the auditory system central to the auditory nerve is described in this paper. This network model is based on physiological grounds, and is designed to gracefully interface with a cochlear model established earlier that incorporates a biophysically motivated dynamic nonlinearity in the basilar-membrane filtering function. This study addresses a longstanding issue in auditory research; that is, how the unique spatial–temporal patterns, shaped by the cochlea's nonlinear filtering, in the auditory nerve data in response to high-level speech sounds can be transformed into a rate-place code in the auditory system in a physiologically plausible manner? Detailed neural mechanisms implemented in the model include neural inhibition, coincidence detection, short-term temporal integration of post synaptic potentials for action potential generation, and a conjectural temporal-to-rate conversion mechanism requiring the membrane time constant of a neuron to monotonically decrease with the neuron's CF. Model simulation experiments using both synthetic and natural speech utterances demonstrate that the auditory rate-place code constructed at the output of the network model is capable of reliable representation, with possible modification and/or enhancement, of the prominent spectral characteristics of the utterances displayed in wideband spectrograms. The effectiveness of the rate-place code is demonstrated, with examples taken from TIMIT corpus, to be universal across all classes of speech sounds including vowels, liquids, fricatives, nasals, and stops.","['H. Sheikhzadeh', 'L. Deng']",January 1999,Computer Speech & Language,[],Regular ArticleA layered neural network interfaced with a cochlear model for the study of speech encoding in the auditory system
42,"Although the delta and RASTA methods have been widely used in extracting the temporal properties of stationary features for robust speech recognition, there is still a need to investigate new temporal features for better performance. In this paper, we present two new temporal features for robust processing of speech signals with emphasis on microphone variations. First, the temporal feature is derived from a bank of RASTA-like filters, in which the parameters of each filter in this bank are estimated according to the statistical properties of the speech signals. Secondly, a parametrized temporal filter (called a PTF) is proposed. The filter can be described by four parameters: the passband, the beginning transition, the ending transition and the smoothness of the magnitude of the filter response. Together, these parameters determine the magnitude of the frequency response of the PTF, and a transformation algorithm is then used to derive the temporal coefficients with real and causal characteristics. The discriminative ability of the PTF features can be further enhanced using the minimum classification error (MCE) algorithm. Experimental results show that the RASTA features are inferior to the PTF features both in quiet conditions and in the presence of microphone variations.","['Jia-lin Shen', 'Wen L. Hwang']",January 1999,Computer Speech & Language,[],Regular ArticleNew temporal features for robust speech recognition with emphasis on microphone variations
43,"This paper presents an approach of automatic selection of phonetically distributed sentence sets for speaker adaptation, and applies the concept to the task of Mandarin speech recognition with very large vocabulary. This is a different approach to the adaptation data selection problem. A computer algorithm is developed to select minimum sets of phonetically distributed training sentences from a text corpus defining the desired task. These sentence sets not only include an almost minimum number of words and sentences that cover the desired acoustic units, but also have statistical distributions of these acoustic phonetic units very close to that in the given text corpus defining the desired task. In this way, more frequently used units can be better trained with higher accuracy, thus improving the overall performance, but the new user needs to produce only a small number of meaningful sentences to train the recognizer. Different sets of sentences selected using different phonetic criteria taking into consideration the statistics of the different acoustic units in the given corpus can then be integrated into a multi-stage adaptation procedure. With this procedure, the recognition performance can be improved incrementally stage by stage using the adaptation data produced with these sentence sets. This proposed approach is applied to an example task of Mandarin speech recognition with a very large vocabulary, both in isolated syllable and continuous speech modes and includes different subject domains in continuous speech recognition. Although the primary results obtained in this paper are for this example task, it is believed that many of the concepts and techniques developed here will also be very useful for other speaker adaptation problems and other languages.","['Jia-lin Shen', 'Hsin-min Wang', 'Ren-yuan Lyu', 'Lin-shan Lee']",January 1999,Computer Speech & Language,[],Regular ArticleAutomatic selection of phonetically distributed sentence sets for speaker adaptation with application to large vocabulary Mandarin speech recognition
44,"This paper presents a language model based onn-grams of word groups (categories). The length of eachn-gram is increased selectively according to an estimate of the resulting improvement in predictive quality. This allows the model size to be controlled while including longer-range dependencies when these benefit performance. The categories are chosen to correspond to part-of-speech classifications in a bid to exploita priorigrammatical information. To account for different grammatical functions, the language model allows words to belong to multiple categories, and implicitly involves a statistical tagging operation which may be used to label new text. Intrinsic generalization by the category-based model leads to good performance with sparse data sets. However word-basedn-grams deliver superior average performance as the amount of training material increases. Nevertheless, the category model continues to supply better predictions for wordn-tuples not present in the training set. Consequently, a method allowing the two approaches to be combined within a backoff framework is presented. Experiments with the LOB, Switchboard and Wall Street Journal corpora demonstrate that this technique greatly improves language model perplexities for sparse training sets, and offers significantly improved size vs. performance tradeoffs when compared with standard trigram models.","['T.R. Niesler', 'P.C. Woodland']",January 1999,Computer Speech & Language,[],Regular ArticleVariable-length categoryn-gram language models
47,"I often say that when you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind: it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science, whatever the matter may be. Lord Kelvin,Popular Lectures and Addresses, (1889), vol. 1, p. 73.",['Robert Gaizauskas'],October 1998,Computer Speech & Language,[],Regular ArticleEvaluation in language and speech technology
48,"The ARPA1CSR programme was initiated in 1984 and the first full-scale evaluation began in 1989. This initiative subsequently developed into two parallel strands in the form of the CSR2and LVCSR3programmes. Overall they have encompassed interactive inquiry, read speech dictation, spontaneous speech transcription and broadcast news transcription. This paper describes these programmes, their design, the mechanics of their operation, the results scoring and analysis and the experiences gained from them.","['SJ Young', 'LL Chase']",October 1998,Computer Speech & Language,[],Regular ArticleSpeech recognition evaluation: a review of the U.S. CSR and LVCSR programmes☆☆☆
49,"The Message Understanding Conferences (MUCs) represent one of the earliest and longest running efforts to evaluate language understanding technology. This article reviews the history of the MUCs and their evolution towards the use of common training and blind test sets, automated scoring, task decomposition into modular building blocks and tools for portability across languages and applications. Now that evaluation has become an accepted part of the developer's toolkit, it is important to understand the interplay between evaluation methods and the state of research. MUC was successful in generating excitement about text processing problems and in attracting talented researchers to the area. It also provided a functional decomposition of the information extraction problem into a series of simpler problems, thus allowing researchers to demonstrate successful systems and to spin off commercial products. However, the ultimate goal of accurate information extraction has been elusive; systems have become faster and cheaper to build, the evaluations have become harder, but overall accuracy in information extraction has improved only modestly. The MUC experience contrasts with experiences in other evaluations. For example, the spoken evaluation in the Air Travel Information System (ATIS) has shown dramatic improvement in error rate over time, but those evaluations were limited to a single domain and the metrics did not address interaction, even though real-time interactive systems were available. Looking across the history of MUC in the context of related evaluations, we can draw important lessons about the need for evaluation to evolve with the technology it evaluates, to balance costs against benefits and to weigh the divergent needs of the multiple stake-holders— developers, funders and users—in order to provide continuity while also providing the next set of challenges to the research community.",['L Hirschman'],October 1998,Computer Speech & Language,[],Regular ArticleThe Evolution of evaluation: Lessons from the Message Understanding Conferences
50,"The Aupelf-Uref launched the Francil research network on language engineering in 1994. The ARC program based on the use of the evaluation paradigm was started in conjunction. It addresses both written and spoken language processing, on seven different topics: natural language access to textual information; (bi/multi)lingual corpus alignment; automated terminological database design; message understanding; voice dictation; vocal dialogue; and text-to-speech synthesis. Each topic involves an organizer, a coordinating committee, the corpus providers and the participants. This paper gives an overview of the complete programme, and a brief summary of the content and results of the first campaign (1995–97) for each topic. More information may be found in the references or can be obtained on the Francil web site (http://www.limsi.fr/Recherche/FRANCIL/frcl.html). It also introduces other actions which are related to the ARC and Francil initiatives: the Aupelf Cooperative Research Actions (ARP), the joint Aupelf-CNRS Silfide project, the CNRS CCIIL Grace project and IL programme.",['J Mariani'],October 1998,Computer Speech & Language,[],Regular ArticleSome evaluation-based language engineering actions for French
51,"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating and comparing the performance of spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviours, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different taks by normalizing for task complexity. After presenting PARADISE, we illustrate its application to two different spoken dialogue agents. We show how to derive a performance function for each agent and how to generalize results across agents. We then show that once such a performance function has been derived, it can be used both for making predictions about future versions of an agent, and as feedback to the agent so that the agent can learn to optimize its behaviour based on its experiences with users over time.","['MA Walker', 'DJ Litman', 'CA Kamm', 'A Abella']",October 1998,Computer Speech & Language,[],Regular ArticleEvaluating spoken dialogue agents with PARADISE: Two case studies
52,"What role should evaluation play in the development of natural language generation (nlg) techniques and systems? In this paper we describe what is involved in natural language generation, and survey how evaluation has figured in work in this area to date. We comment on the issues raised by this existing work and on how the problems ofnlgevaluation are different from the problems of evaluating work in natural languageunderstanding. The paper is concluded by suggesting a way forward by looking more closely at the component problems that are addressed in natural language generation research; a particular text generation application is examined and the issues that are raised in assessing its performance on a variety of dimensions are looked at.","['C Mellish', 'R Dale']",October 1998,Computer Speech & Language,[],Regular ArticleEvaluation in the context of natural language generation
53,"The measurement of the word error rate (WER) of a speech recognizer is valuable for the development of new algorithms but provides only the most limited information about the performance of the recognizer. We propose the use of a human reference standard to assess the performance of speech recognizers, so that the performance of a recognizer could be quoted as being equivalent to the performance of a human hearing speech which is subject to X dB of degradation. This approach should have the major advantage of being independent of the database and speakers used for testing. Furthermore, it would allow factors beyond the word error rate to be measured, such as the performance within an interactive speech system. In this paper, we report on preliminary work to explore the viability of this approach. This has consisted of recording a suitable database for experimentation, devising a method of degrading the speech in a controlled way and conducting two set of experiments on listeners to measure their responses to degraded speech to establish a reference. Results from these experiments raise several questions about the technique but encourage us to experiment with comparisons with automatic recognizers.","['SJ Cox', 'PW Linford', 'WB Hill', 'RD Johnston']",October 1998,Computer Speech & Language,[],Regular ArticleTowards speech recognizer assessment using a human reference standard
54,"This paper reports on a cooperative international evaluation of grapheme-to-phoneme (GP) conversion for text-to-speech synthesis in French. Test methodology and test corpora are described. The results for eight systems are provided and analysed in some detail. The contribution of this paper is twofold: on the one hand, it gives an accurate picture of the state-of-the-art in the domain of GP conversion for French, and points out the problems still to be solved. On the other hand, much room is devoted to a discussion of methodological issues for this task. We hope this could help future evaluations of similar systems in other languages.","['F Yvon', 'P Boula de Mareüil', 'C d»Alessandro', 'V Aubergé', 'M Bagein', 'G Bailly', 'F Béchet', 'S Foukia', 'J -F Goldman', 'E Keller', 'D O»Shaughnessy', 'V Pagel', 'F Sannier', 'J Véronis', 'B Zellner']",October 1998,Computer Speech & Language,[],Regular ArticleObjective evaluation of grapheme to phoneme conversion for text-to-speech synthesis in French
55,"An experiment with recent test suite and grammar (engineering) resources is outlined: a criticial assessment of the EU-fundedtsnlp(Test Suites for Natural Language Processing) package as a diagnostic and benchmarking facility for a distributed (multi-site) large-scalehpsggrammar engineering effort. This paper argues for a generalized, systematic, and fully automated testing and diagnosis facility as an integral part of the linguistic engineering cycle and gives a practical assessment of existing resources; both a flexible methodology and tools for competence and performance profiling are presented. By comparison to earlier evaluation work as reflected in the Hewlett-Packard test suite data, released exactly 10 years beforetsnlp, it is judged where test-suite-based evaluation has improved (and where not) over time.","['S Oepen', 'DP Flickinger']",October 1998,Computer Speech & Language,[],Regular ArticleTowards systematic grammar profiling.Test suite technology 10 years after☆
56,"Since intelligibility of synthetic speech is no longer the main criterion on which to base quality judgements, reliable methods for prosody evaluation become more important. We propose a method called PURR (Prosody Unveiling through Restricted Representation) to evaluate the prosodic component of a synthesis system without the interference of other system components. In PURR, the stimuli are reduced to their prosodic content. The method has proven to be suitable for test designs with naıve listeners. It can be used for comparative studies as well as for diagnostic analyses and is, therefore, a useful tool for basic research on the perception of prosodic phenomena. In this paper we first describe how the best signal manipulation method was determined using perception tests. The appropriateness of the resulting signal is further assessed in a recognition test of syntactic structure. We then report on further validations of the proposed method: different ways of synthetic prosody modelling are evaluated, both in comparison with human prosody and amongst different synthesis systems.","['Gerit P Sonntag', 'Thomas Portele']",October 1998,Computer Speech & Language,[],Regular ArticlePURR—A method for prosody evaluation and investigation
57,"There are now many computer programs for automatically determining the sense in which a word is being used. One would like to be able to say which are better, which worse, and also which words, or varieties of language, present particular problems to which algorithms. An evaluation exercise is required, and such an exercise requires a “gold standard” dataset of correct answers. Producing this proves to be a difficult and challenging task. In this paper I discuss the background, challenges and strategies, and present a detailed methodology for ensuring that the gold standard is not fool's gold.",['Adam Kilgarriff'],October 1998,Computer Speech & Language,[],Regular ArticleGold standard datasets for evaluating word sense disambiguation programs
58,"This paper outlines a model of accenting that is based on a combination of syntactic/metrical and semantic/pragmatic considerations. The model, called theblackboardmodel of accenting in an allusion to the blackboard architectures used in artificial intelligence, explains how the interaction of various factors (namely ««triggers»» as well as ««obstacles»» for accenting) determines the locations of sentence accents in speech. The interactions between factors related to novelty and contrast are studied in some detail. The paper focuses on the problem of accent location in the prosodic component of a concept-to-speech system.",['K. van Deemter'],June 1998,Computer Speech & Language,[],Regular ArticleA blackboard model of accenting
59,"Modelling the state duration of hidden Markov models (HMMs) can effectively improve the accuracy in decoding the state sequence of an utterance and result in an improvement of speech recognition accuracy. However, when a speech signal is contaminated by ambient noise, the decoded state sequence may be distorted. It may stay at some states too long or too short even with the help of state duration models. This paper presents a proportional alignment decoding (PAD) algorithm for retraining the HMMs. A task of multi-speaker isolated Mandarin digit recognition was conducted to demonstrate the effectiveness and robustness of the PAD-based variable duration hidden Markov model (VDHMM/PAD) method. Experimental results show that the discriminativity of VDHMM/PAD in a noisy environment has been significantly enhanced. Moreover, the proposed method outperforms those widely used state duration modelling methods, such as using Poisson, gamma, Gaussian, bounded and non-parametric probability density functions.","['Wei-Wen Hung', 'Hsiao-Chuan Wang']",June 1998,Computer Speech & Language,[],Regular ArticleImprovement of noisy speech recognition using a proportional alignment decoding algorithm in the training phase☆
60,"Language Understanding can be considered as the realization of a mapping from sentences of a natural language into a description of their meaning in an appropriate formal language. Under this viewpoint, the application of the Onward Subsequential Transducer Inference Algorithm (OSTIA) to Language Understanding is considered. The basic version of OSTIA is reviewed and a new version is presented in which syntactic restrictions of the domain and/or range of the target transduction can effectively be taken into account. For experimentation purposes, a task proposed by Feldman, Lakoff, Stolcke and Weber (1990) (International Computer Science Institute, Berkley, California) for assessing the capabilities of language learning and understanding systems has been adopted and three semantic coding schemes have been defined for this task with different sources of difficulty. In all cases the basic version of OSTIA has proved consistently to be able to learn very compact and accurate transducers from relativly small training sets of input–output examples of the task. Moreover, if the input sentences are corrupted with syntactic incorrectness or errors, the new version of OSTIA still provides understandable results that only degrade in a gradual and natural way.","['A. Castellanos', 'E. Vidal', 'M.A. Varó', 'J. Oncina']",June 1998,Computer Speech & Language,[],Regular ArticleLanguage understanding and subsequential transducer learning
61,"In this paper, we propose parameter estimation techniques for mixture density polynomial segment models (MDPSMs) where their trajectories are specified with an arbitrary regression order. MDPSM parameters can be trained in one of three different ways: (1) segment clustering; (2) expectation maximization (EM) training of mean trajectories; and (3) EM training of mean and variance trajectories. These parameter estimation methods were evaluated in TIMIT vowel classification experiments. The experimental results showed that modelling both the mean and variance trajectories is consistently superior to modelling only the mean trajectory. We also found that modelling both trajectories results in significant improvements over the conventional HMM.","['T. Fukada', 'K.K. Paliwal', 'Y. Sagisaka']",June 1998,Computer Speech & Language,[],Regular ArticleModel parameter estimation for mixture density polynomial segment models
62,"This paper examines the application of linear transformations for speaker and environmental adaptation in an HMM-based speech recognition system. In particular, transformations that are trained in a maximum likelihood sense on adaptation data are investigated. Only model-based linear transforms are considered, since, for linear transforms, they subsume the appropriate feature–space transforms. The paper compares the two possible forms of model-based transforms: (i) unconstrained, where any combination of mean and variance transform may be used, and (ii) constrained, which requires the variance transform to have the same form as the mean transform. Re-estimation formulae for all appropriate cases of transform are given. This includes a new and efficient full variance transform and the extension of the constrained model–space transform from the simple diagonal case to the full or block–diagonal case. The constrained and unconstrained transforms are evaluated in terms of computational cost, recognition time efficiency, and use for speaker adaptive training. The recognition performance of the two model–space transforms on a large vocabulary speech recognition task using incremental adaptation is investigated. In addition, initial experiments using the constrained model–space transform for speaker adaptive training are detailed.",['M.J.F. Gales'],April 1998,Computer Speech & Language,[],Regular ArticleMaximum likelihood linear transformations for HMM-based speech recognition☆
63,"This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text-to-speech synthesizer. Text is first converted into a sequence of part-of-speech tags. Next a Markov model is used to give the most likely sequence of phrase breaks for the input part-of-speech tags. In the Markov model, states represent types of phrase break and the transitions between states represent the likelihoods of sequences of phrase types occurring. The paper reports a variety of experiments investigating part-of-speech tag-sets, Markov model structure and smoothing. The best setup correctly identifies 79% of breaks in the test corpus.","['Paul Taylor', 'Alan W. Black']",April 1998,Computer Speech & Language,[],Regular ArticleAssigning phrase breaks from part-of-speech sequences☆
64,"The synthesis of speech from unrestricted text needs a phonemic transcription including syllabification and lexical stress for each word and symbol. Speech synthesizers currently use large lexicons to provide such transcriptions, but noteveryword has a lexical entry and a backup is required to produce transcriptions for novel words. In addition, synthesizers do not have an infinite amount of memory at their disposal, so it is not always possible continually to append supplementary lexemes for specialized applications in the hope of reducing the probability of encountering a novel word. Transcriptions for novel words are produced by implicit analogy with an existing lexicon. A data-driven technique of extracting context-dependent grapheme-to-phoneme rules with dynamically minimized context lengths from a training lexicon is proposed. Syllable boundary and lexical stress information is included in the transcriptions. The proposed system satisfies certain pragmatic constraints: it can produce transcriptions with sufficient rapidity to maintain real-time processing in a text-to-speech system; the rules take up a small amount of storage size (370 KBytes); and a pronunciation can be generated for any novel word. The quality of the transcription process enables 77·06% of lexemes formerly present in the training lexicon to be excluded, thus reducing the lexicon's memory requirements by 74·18% (of 3·57 MBytes).",['Paul C. Bagshaw'],April 1998,Computer Speech & Language,[],Regular ArticlePhonemic transcription by analogy in text-to-speech synthesis: Novel word pronunciation and lexicon compression
65,"An important problem in automatic speech understanding is the transport of an existing application system to a new language. Design choices are required to keep the cost and implementation time of the porting as low as possible. One of the bottlenecks in spoken language system development is represented by data collection. When moving an application from the original language to a new one, it is very important to exploit, as much as possible, the data collected in the first language. This paper discusses the construction of a speech-based Air Travel Information Service (ATIS) for Italian, starting from (American) English. The aim of the work is to maximize the use of data available in English for the ATIS task in the construction of the Italian system. All components will be examined and proposed strategies will be evaluated by experimental tests. By just using a small Italian corpus and the ATIS data available in English, an understanding error rate of 6·7% was obtained on Italian written transcriptions. By adapting to speakers acoustic models and by training a language model on translations of transcriptions, an understanding error rate of 17·9% was obtained by considering read as well as spontaneous Italian sentences.","['Mauro Cettolo', 'Anna Corazza', 'Renato De Mori']",January 1998,Computer Speech & Language,[],Regular ArticleLanguage portability of a speech understanding system
66,"We detail an algorithm (ORED) that transforms any higher-order hidden Markov model (HMM) to an equivalent first-order HMM. This makes it possible to process higher-order HMMs with standard techniques applicable to first-order models. Based on this equivalence, a fast incremental algorithm (FIT) is developed for training higher-order HMMs from lower-order models, thereby avoiding the training of redundant parameters. We also show that the FIT algorithm results in much faster training and better generalization compared to conventional high-order HMM approaches. This makes training of high-order HMMs practical for many applications.",['J.A. du Preez'],January 1998,Computer Speech & Language,[],Regular ArticleEfficient training of high-order hidden Markov models using first-order representations
67,"This paper proposes an instantaneous speaker adaptation method that uses N-best decoding for continuous mixture-density hidden-Markov-model-based speech-recognition systems. This method is effective even for speakers whose decoding using speaker-independent (SI) models are error-prone and for whom speaker adaptation techniques are truly needed. In addition, smoothed estimation and utterance verification are introduced into this method. The smoothed estimation is based on the likelihood values for adapted models of word sequences obtained by N-best decoding and improves the performance of error-prone speakers, and the utterance verification technique reduces the amount of calculation required. Performance evaluation using connected-digit (four-digit strings) recognition experiments performed over actual telephone lines showed a reduction of 36·4% in the error rates of speakers whose decoding using SI models are error-prone.","['Tomoko Matsui', 'Sadaoki Furui']",January 1998,Computer Speech & Language,[],Regular ArticleN-Best-based unsupervised speaker adaptation for speech recognition
68,"Speech recognition systems that are based on hidden Markov modelling (HMM) assume that the mean trajectory feature vector within a state is constant over time. In recent years, segment models that attempt to describe the dynamics of the speech signal within a phonetic unit have been proposed. Some of these models describe the mean trajectory over time as a random process. In this paper we present the concept of a scaled random trajectory segment model, which aims to overcome the modelling problem created by the fact that segment realizations of the same phonetic unit differ in length. The new model is supported by direct experimental evidence. It offers the following advantages over the standard (non-scaled) model. First, it shows improved performance compared to the non-scaled model. This is demonstrated using phone classification experiments. Second, it yields closed form expressions for the estimated parameters, unlike the previously suggested, non-scaled model, which requires more complicated iterative estimation procedures.","['Jacob Goldberger', 'David Burshtein']",January 1998,Computer Speech & Language,[],Regular ArticleScaled random trajectory segment models
69,No abstract,[],October 1997,Computer Speech & Language,[],Contents and IndexContents and Index to Volume 11
70,"Dialogue move recognition is cited as being representative of a class of problem which may be of concern in data driven natural language processing. The dialogue move recognition problem is formulated as a keyword-based topic identification problem, and is shown to be sensitive to the issue of unknown vocabulary. A model based on the multiple Poisson distribution is shown to alleviate the unknown vocabulary issue, subject to the assumption that the occurrence of keywords represents a small fraction of the data. A keyword selection strategy is derived to ensure this assumption is valid. It is shown that a modified version of Zipf's law provides a suitable prior probability distribution for keywords, and that its inclusion increases classification performance.",['Philip N. Garner'],October 1997,Computer Speech & Language,[],Regular ArticleOn topic identification and dialogue move recognition
71,"A corpus-based statistical-oriented Chinese word classification can be regarded as a fundamental step for automatic or non-automatic, monolingual natural processing systems. Word classification can solve the problems of data sparseness and have far fewer parameters. So far, much relative work about word classification has been done. All the work is based on some similarity metrics. We use average mutual information as the global similarity metric to do classification. The clustering process is top-down splitting and the binary tree is growing with splitting. In natural language, the effect of left neighbours and right neighbours of a word are asymmetric. To utilize this directional information, we induce the left–right binary and the right–left binary tree to represent this property. The probability is also introduced in our algorithm to merge the resulting classes from the left–right and the right–left binary tree. Also, we use the resulting classes to do experiments on a word class-based language model. Some classes' results and the perplexity of a word class-based language model are presented.","['Jun Gao', 'XiXian Chen']",October 1997,Computer Speech & Language,[],Regular ArticleProbabilistic word classification based on a context-sensitive binary tree method
72,"The objective of this paper is to present experiments and discussions of how some neural network algorithms can help to improve phoneme recognition using mixture density hidden Markov models (MDHMMs). In MDHMMs, the modelling of the stochastic observation processes associated with the states is based on the estimation of the probability density function of the short-time observations in each state as a mixture of Gaussian densities. The Learning Vector Quantization (LVQ) is used to increase the discrimination between different phoneme models both during the initialization of the Gaussian codebooks and during the actual MDHMM training. The Self-Organizing Map (SOM) is applied to provide a suitably smoothed mapping of the training vectors to accelerate the convergence of the actual training. The codebook topology which is obtained can also be exploited in the recognition phase to speed up the calculations to approximate the observation probabilities. The experiments with LVQ and SOMs show reductions both in the average phoneme recognition error rate and in the computational load compared to the maximum likelihood training and the Generalized Probabilistic Descent (GPD). The lowest final error rate, however, is obtained by using several training algorithms successively. Additional reductions from the online system of about 40% in the error rate are obtained by using the same training methods, but with advanced and higher dimensional feature vectors.",['Mikko Kurimo'],October 1997,Computer Speech & Language,[],Regular ArticleTraining mixture density HMMs with SOM and LVQ
73,"This paper proposes a novel approach to frame-level classification by the use of inductive inference (decision trees). The proposed system (Samouelian, 1994a) uses the C4.5 induction system (Quinlan, 1993, 1996) to capture the knowledge about the structure and characteristics of the speech signal explicitly from the database. The decision tree is generated automatically from the training speech database. The database contains labelled examples in the form of a feature vector and its corresponding label for each frame. The feature vector may consist of any number of different feature sets and the label may be at the phoneme, sub-word or word level. This approach allows the integration of features from existing signal processing techniques that are currently used in stochastic modelling such as hidden Markov models (HMMs), and acoustic–phonetic features, which have been the cornerstone of traditional knowledge-based techniques. The aim of this research is to demonstrate that induction systems can provide a viable alternative automatic speech recognition technique by allowing the combination of features from any of the above feature representations to achieve optimum classification. Using C4.5, the results on five experiments are reported. The first four experiments use a small corpus of Australian English consonants (plosives, liquids and nasals) and four different feature sets, and they report on frame-level classification results for speaker-dependent and independent modes. The fifth experiment uses the TIMIT database and the mel frequency cepstral coefficient (MFCC) feature set and reports on frame-level classification results for speaker-independent experiments on the training data and test data.",['Ara Samouelian'],July 1997,Computer Speech & Language,[],Regular ArticleFrame-level phoneme classification using inductive inference
74,"One problem faced by some model adaptation techniques is that only the parameters of those models which are observed in the adaptation data are updated. Hence, with small amounts of adaptation data most of the system parameters remain unchanged. In this paper, a technique called regression-based model prediction (RMP), which tries to overcome this problem, is presented. This technique tries to adapt the model parameters of a continuous density hidden Markov model set which has insufficient adaptation data when used with maximuma posteriori(MAP) estimation. The technique uses the parameters of better estimated models and a set of parameter relationships between the model parameters to update the parameters of models with insufficient adaptation data. The parameter relationships are found by applying linear regression to a number of speaker-specific model sets. Experiments using both MAP estimation and RMP are presented using the ARPA RM1 continuous speech database and RMP has been found to be useful in improving the system performance with as little as 3 s of adaptation speech. RMP has been shown to consistently improve the results obtained by MAP. When a very large number of adaptation sentences are used the error rates converge towards those of MAP. It is shown that RMP gives an improvement of 8·8% over the baseline error rate with a single adaptation sentence, and 27% with 40 adaptation sentences.","['SM Ahadi', 'PC Woodland']",July 1997,Computer Speech & Language,[],Regular ArticleCombined Bayesian and predictive techniques for rapid speaker adaptation of continuous density hidden Markov models
75,"Traditional hidden Markov model (HMM) word spotting requires both explicit HMM models of each desired keyword and a computationally expensive decoding pass. For certain applications, such as audio indexing or information retrieval, conventional word spotting may be too constrained or impractically slow. This paper presents an alternative technique, where a phone lattice—representing multiple phone hypotheses—is pre-computed prior to need. Given a phone decomposition of any desired keyword, the lattice may be rapidly searched to find putative occurrences of the keyword. Though somewhat less accurate, this can be substantially faster (orders of magnitude) and more flexible (any keyword may be detected) than previous approaches. This paper presents algorithms for lattice generation and scanning, and experimental results, including comparison with conventional keyword-HMM approaches. Finally, word spotting based on phone lattice scanning is demonstrated to be effective for spoken document retrieval.","['JT Foote', 'SJ Young', 'G J.F Jones', 'K Spärck Jones']",July 1997,Computer Speech & Language,[],Regular ArticleUnconstrained keyword spotting using phone lattices with application to spoken document retrieval
76,"By using context-dependent concept language models, we can significantly improve the performance of the speech understanding component of a dialogue system. We use several different modelling methods, such as bigrams, concept set probability estimations using graph theory and combinations of these. The contexts are selected both manually and automatically using mutual information. Using context-dependent bigrams, with contexts selected by the clustering algorithm, we establish a relative increase in performance of 10% on the attribute error rate on data obtained through the currently operational Swiss automatic train timetable information system.","['Erwin W Drenth', 'Bernhard Rüber']",July 1997,Computer Speech & Language,[],Regular ArticleContext-dependent probability adaptation in speech understanding
77,"A discrete wavelet transform algorithm segregates the operand data set sequentially. It generates computational intermediates which represent it at graded resolutions and leads to a reciprocal domain within which information is multiply resolved in terms of the time-frequency localization of the component wavelet basis vectors. Based on this we introduce the concept of wavelet subtransform domains and show that these can be used to selectively enhance acoustic events in a speech signal. Enhancement of phoneme classes improves segmentation and recognition performance for reasons that we have pointed out in the paper. As an experimental verification, we use subtransform domains to design a preprocessor for a Hindi database and evaluate the subsequent recognition performances using hidden Markov models and two standard parametrizations.","['R Singh', 'K Davis', 'P V.S Rao']",July 1997,Computer Speech & Language,[],Regular ArticleHidden Markov model-based speech recognition with intermediate wavelet transform domains
78,"Electronically steerable arrays of microphones have a variety of uses in speech data acquisition systems. Applications include teleconferencing, speech recognition and speaker identification, sound capture in adverse environments, and biomedical devices for the hearing impaired. An array of microphones has a number of advantages over a single-microphone system. It may be electronically aimed to provide a high-quality signal from a desired source location while simultaneously attenuating interfering talkers and ambient noise, does not necessitate local placement of transducers or encumber the talker with a hand-held or head-mounted microphone, and does not require physical movement to alter its direction of reception. Additionally, it has capabilities that a single microphone does not; namely automatic detection, localization and tracking of active talkers in its receptive area. This paper addresses the specific application of source localization algorithms for estimating the position of speech sources in a real-room environment given limited computational resources. The theoretical foundations of a speech source localization system are presented. This includes the development of a source–sensor geometry for talkers and sensors in the near-field environment as well as the evaluation of several error criteria available to the problem. Several practical algorithms necessary for real-time implementation are developed, specifically the derivation and evaluation of an appropriate time-delay estimator and a novel closed-form locator. Finally, results obtained from a real system are presented to illustrate the effectiveness of the proposed source localization techniques as well as to confirm the practicality of the theoretical models.","['Michael S. Brandstein', 'Harvey F. Silverman']",April 1997,Computer Speech & Language,[],Regular ArticleA practical methodology for speech source localization with microphone arrays
79,"This paper describes an on-line adaptation method that combines maximuma posteriori(MAP) estimation for intra-class training (the training scheme incorporates new training samples with prior information) with vector field smoothing (VFS) for inter-class smoothing. Results of experiments comparing recognition performance of MAP/VFS with MAP adaptation for speaker adaptation and simultaneous adaptation of speaker and telephone channel show that fast and incremental adaptation can be achieved even with a relatively small number of training samples (under 10 words) due to VFS's ability to consistently enhance MAP adaptation. High word error reduction rates, which in the experiments were 22% for speaker adaptation in a large-vocabulary isolated-word recognition task (vocabulary size=2575) and 48% for simultaneous adaptation of speaker and telephone channel in a 100-isolated-word recognition task, can be achieved through word-by-word incremental adaptation using 10-word data.","['Jun-ichi Takahashi', 'Shigeki Sagayama']",April 1997,Computer Speech & Language,[],Regular ArticleVector-field-smoothed Bayesian learning for fast and incremental speaker/telephone-channel adaptation
80,"In recent years, we have experienced an increasing demand for speech recognition technology to be utilized in various real-world applications, such as name dialling, message retrieval, etc. During this process, we have learned that the performance of speech recognition systems under laboratory environment cannot be duplicated in the actual service. Two major causes have been identified to this problem. The first is the lack ofrobustnesswhen the acoustic conditions in testing are different from those in training. The second is the lack offlexibilitywhen handling spontaneous speech input which often contains extraneous speech in addition to the desired speech segments of key phrases. This paper focuses on one aspect of achieving flexible speech recognition, namely, improving the ability to cope with naturally spoken utterances through discriminative utterance verification. We propose an algorithm for training utterance verification systems based on the minimum verification error (MVE) training framework. Experimental results on speaker-independent telephone-based connected digits show a significant improvement in verification accuracy when the discriminant function used in MVE training is made consistent with the confidence measure used in utterance verification. At a 10% rejection rate, for example, the new proposed method reduces the string error rate by a further 22·7% over our previously reported results in which the MVE-based discriminative training was not incorporated.","['Mazin G. Rahim', 'Chin-Hui Lee']",April 1997,Computer Speech & Language,[],Regular ArticleString-based minimum verification error (SB-MVE) training for speech recognition
81,"It is well known that language models are effective for increasing the accuracy of speech and handwriting recognizers, but large language models are often required to achieve low model perplexity (or entropy) and still have adequate language coverage. We study three efficient methods for variable order stochastic language modeling in the context of the stochastic pattern recognition problem. Two of these methods are previous techniques from recent literature, and one is a new method based on a successful text compression technique. We give results of a comparative analysis, and demonstrate that the best performance is achieved by extending one of the previous techniques using elements from the newly developed method.","['Jianying Hu', 'William Turin', 'Michael K. Brown']",January 1997,Computer Speech & Language,[],Regular ArticleLanguage modeling using stochastic automata with variable length contexts
82,"Modelling contextual variations of phones is widely accepted as an important aspect of a continuous speech recognition system, and HMM distribution clustering has been sucessfully used to obtain robust models of context through distribution tying. However, as systems move to the challenge of spontaneous speech, temporal variation also becomes important. This paper describes a method fordesigning HMM topologies that learn both temporal and contextual variation, extending previous work on successive state splitting (SSS). The new approach uses a maximum likelihood criterion consistently at each step, overcoming the previous SSS limitation to speaker-dependent training. Initial experiments show both performance gains and training cost reduction over SSS with the reformulated algorithm.","['M. Ostendorf', 'H. Singer']",January 1997,Computer Speech & Language,[],Regular ArticleHMM topology design using maximum likelihood successive state splitting
83,"This paper describes a method for the construction of a word graph (or lattice) for large vocabulary, continuous speech recognition. The advantage of a word graph is that a fairly good degree of decoupling between acoustic recognition at the 10-ms level and the final search at the word level using a complicated language model can be achieved. The word graph algorithm is obtained as an extension of the one-pass beam search strategy using word dependent copies of the word models or lexical trees. The method has been tested successfully on the 20 000-word NAB'94 task (American English, continuous speech, 20 000 words, speaker independent) and compared with the integrated method. The experiments show that the word graph density can be reduced to an average number of about 10 word hypotheses, i.e. word edges in the graph, per spoken word with virtually no loss in recognition performance.","['Stefan Ortmanns', 'Hermann Ney', 'Xavier Aubert']",January 1997,Computer Speech & Language,[],Regular ArticleA word graph algorithm for large vocabulary continuous speech recognition
84,"This paper describes the Sqaleproject in which the ARPA large vocabulary evaluation paradigm was adapted to meet the needs of European multilingual speech recognition development. It involved establishing a framework for sharing training and test materials, defining common protocols for training and testing systems, developing systems, running an evaluation and analysing the results. The specifically multilingual issues addressed included the impact of the language on corpora and test set design, transcription issues, evaluation metrics, recognition system design, cross-system and cross-language performance, and results analysis. The project started in December 1993 and finished in September 1995. The paper describes the evaluation framework and the results obtained. The overall conclusions of the project were that the same general approach to recognition system design is applicable to all the languages studied although there were some language specific problems to solve. It was found that the evaluation paradigm used within ARPA could be used within the European context with little difficulty and the consequent sharing amongst the sites of training and test materials and language-specific expertise was highly beneficial.","['S.J. Young', 'M. Adda-Dekker', 'X. Aubert', 'C. Dugast', 'J.L. Gauvain', 'D.J. Kershaw', 'L. Lamel', 'D.A. Leeuwen', 'D. Pye', 'A.J. Robinson', 'H.J.M. Steeneken', 'P.C. Woodland']",January 1997,Computer Speech & Language,[],Regular ArticleMultilingual large vocabulary speech recognition: the European SQALE project
85,No abstract,[],October 1996,Computer Speech & Language,[],Regular PaperContents and Index to Volume 10
86,"This paper investigates the modelling of the interframe dependence in a hidden Markov model (HMM) for speech recognition. First, a new observation model, assuming dependence on multiple previous frames, is proposed. This model represents such a dependence structure with a weighted mixture of a set of first-order conditional Gaussian densities, each mixture component accounting for a specific conditional frame. Next, an optimization in choosing the conditional frames/segment is performed in both training and recognition, thereby helping to remove the mismatch of the conditional segments due to different observation histories. An EM (Expectation–Maximization) iteration algorithm is developed for the estimation of the model parameters and for the optimization over the dependence structure. Experimental comparisons on a speaker-independent E-set database show that the new model, without optimization on the dependence structure, achieves better performance than the standard HMM, the bigram HMM and the linear-predictive HMM, all in comparable or smaller parameter sizes. The optimization over the dependence structure leads to further improvement in the performance.","['Ji Ming', 'F.Jack Smith']",October 1996,Computer Speech & Language,[],Regular PaperModelling of the interframe dependence in an HMM using conditional Gaussian mixtures
87,"One of the key issues for adaptation algorithms is to modify a large number of parameters with only a small amount of adaptation data. Speaker adaptation techniques try to obtain near speaker-dependent (SD) performance with only small amounts of speaker-specific data, and are often based on initial speaker-independent (SI) recognition systems. Some of these speaker adaptation techniques may also be applied to the task of adaptation to a new acoustic environment. In this case an SI recognition system trained in, typically, a clean acoustic environment is adapted to operate in a new, noise-corrupted, acoustic environment. This paper examines the maximum likelihood linear regression (MLLR) adaptation technique. MLLR estimates linear transformations for groups of model parameters to maximize the likelihood of the adaptation data. Previously, MLLR has been applied to the mean parameters in mixture-Gaussian HMM systems. In this paper MLLR is extended to also update the Gaussian variances and re-estimation formulae are derived for these variance transforms. MLLR with variance compensation is evaluated on several large vocabulary recognition tasks. The use of mean and variance MLLR adaptation was found to give an additional 2% to 7% decrease in word error rate over mean-only MLLR adaptation.","['M.J.F. Gales', 'P.C. Woodland']",October 1996,Computer Speech & Language,[],Regular PaperMean and variance adaptation within the MLLR framework
88,"Stochastic language models are widely used in spoken language understanding to recognize and interpret the speech signal: the speech samples are decoded into word transcriptions by means of acoustic and syntactic models and then interpreted according to a semantic model. Both for speech recognition and understanding, search algorithms use stochastic models to extract the most likely uttered sentence and its correspondent interpretation. The design of the language models has to be effective in order to mostly constrain the search algorithms and has to be efficient to comply with the storage space limits. In this work we present the VariableN-gram Stochastic Automaton (VNSA) language model that provides a unified formalism for building a wide class of language models. First, this approach allows for the use of accurate language models for large vocabulary speech recognition by using the standard search algorithm in the one-pass Viterbi decoder. Second, the unified formalism is an effective approach to incorporate different sources of information for computing the probability of word sequences. Third, the VNSAs are well suited for those applications where speech and language decoding cascades are implemented through weighted rational transductions. The VNSAs have been compared to standard bigram and trigram language models and their reduced set of parameters does not affect by any means the performances in terms of perplexity. The design of a stochastic language model through the VNSA is described and applied to word and phrase class-based language models. The effectiveness of VNSAs has been tested within the Air Travel Information System (ATIS) task to build the language model for the speech recognition and the language understanding system.","['Giuseppe Riccardi', 'Roberto Pieraccini', 'Enrico Bocchieri']",October 1996,Computer Speech & Language,[],Regular PaperStochastic automata for language modeling
89,"We describe a new search algorithm for speech recognition which applies the monotone graph search procedure to the problem of building a word graph. A first backward pass provides a method for estimating the word boundary times and phone segment boundary times needed to build the word graph using either the 1-phone or 2-phone lookahead assumptions. It also provides a heuristic for the search which satisfies the monotonicity condition. A second backward pass applies forward–backward pruning to the word graph. We show how the search can be made to run very quickly if the 1-phone lookahead assumption holds. We present the results of experiments performed on the 5000-word speaker-independentWall Street Journaltask under both the 1-phone and 2-phone lookahead assumptions. These results show that the 1-phone lookahead assumption leads to unacceptably large error rates for speaker-independent recognition using current acoustic phonetic modelling techniques. Finally, we give an account of the methods we have developed to process speech data in successive blocks so as to address the real-time issue and to control the memory requirements of the search.","['Z. Li', 'G. Boulianne', 'P. Labute', 'M. Barszcz', 'H. Garudadri', 'P. Kenny']",October 1996,Computer Speech & Language,[],Regular PaperBi-directional graph search strategies for speech recognition
90,"Higher quality speech synthesis is required to make text-to-speech technologies useful in more applications, and prosody is one component of synthesis technology with the greatest need for improvement. This paper describes computational models for the prediction of abstract prosodic labels for synthesis—accent location, symbolic tones and relative prominence level—from text that is tagged with part-of-speech labels and marked for prosodic constituent structure. Specifically, the model uses multiple levels of a prosodic hierarchy and at each level combines decision tree probability functions with Markov sequence assumptions. An advantage of decision trees is the ability to incorporate linguistic knowledge in an automatic training framework, which is needed for building systems that reflect particular speaking styles. Studies of accent and tone variability across speakers are reported and used to motivate new evaluation metrics. Prediction experiments show an improvement in accuracy of prominence location prediction over simple decision trees, with accuracy similar to the level of variability observed across speakers.","['K. Ross', 'M. Ostendorf']",July 1996,Computer Speech & Language,[],Regular PaperPrediction of abstract prosodic labels for speech synthesis
91,"An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32–39% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10–14%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework.",['Ronald Rosenfeld'],July 1996,Computer Speech & Language,[],Regular PaperA maximum entropy approach to adaptive statistical language modelling
92,"A numerical simulation of laryngeal flow was developed to study flow patterns and pressure and velocity waveforms in a model of the oscillating glottis. The unsteady Navier–Stokes equations were solved with a finite volume method using a nonuniform staggered grid. The numerical method was tested against published experimental data. In this study of glottal aerodynamics, the vocal folds independently and sinusoidally were moved from a converging to a diverging and back to a converging shape, and the input airflow sinusoidally varied from zero to a maximum and back to zero. The typical results were obtained for a Reynolds number of 2000 and for an oscillation frequency of 100 Hz. Results indicate that with this simulation of the entire flow field, periodic velocity and pressure fields exist throughout the laryngeal duct. The airflow separates within the glottis, creating intraglottal (and downstream) asymmetric flow throughout the glottal cycle, with formation of eddies downstream of the glottis. The observed maximum velocity delays due to the glottal wall movement would contribute to the well-known glottal volume velocity skewing during phonation.","['Fariborz Alipour', 'Chenwu Fan', 'Ronald C. Scherer']",April 1996,Computer Speech & Language,[],Regular PaperA numerical simulation of laryngeal flow in a forced-oscillation glottal model
93,"In this paper, by using the formulation of the missing-data problem, a general framework for statistical acoustic modelling of speech is presented. With the motivation of utilizing bi-directional contextual dependence in acoustic modelling, a bi-directional hidden Markov modelling approach for speech recognition is studied and the importance of the bi-directional contextual dependence for speech recognition is identified by a series of comparative experiments. Furthermore, hidden Markov random field (MRF)-based acoustic modelling techniques using our previously proposed contextual vector quantization (CVQ) method and iterated conditional modes (ICM) algorithm, which is very suitable for parallel processing implementation, are also attempted. Their viability is confirmed by a series of preliminary experiments in a speaker-independent isolated English letter recognition task.","['Qiang Huo', 'Chorkin Chan']",April 1996,Computer Speech & Language,[],Regular PaperA study on the use of bi-directional contextual dependence in Markov random field-based acoustic modelling for speech recognition
94,"This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.","['Tomoko Matsui', 'Tomohito Kanno', 'Sadaoki Furui']",April 1996,Computer Speech & Language,[],Regular PaperSpeaker recognition using HMM composition in noisy environments
95,"This paper proposes a novel speaker adaptation algorithm that enables adaptation with a small amount of speech data. This algorithm consists of two blocks. One is a parameter adaptation algorithm that utilizes the information of a well-trained initial model. The other is an initial model generation algorithm that is based on speaker clustering. The former algorithm is based on two speaker adaptation techniques, that is, maximum a posteriori (MAP) estimation and transfer vector field smoothing (VFS). This MAP–VFS algorithm unifies both techniques efficiently to avoid the weaknesses of the methods used individually, and can interpolate and smooth untrained or insufficiently trained parameters by taking into consideration the reliability of each estimated parameter. A higher phoneme recognition performance was obtained by using this algorithm than with the individual methods (MAP and VFS), showing the superiority of the proposed algorithm. With this algorithm, the phoneme recognition error rate was reduced from 22·0% to 19·1% for a speaker-independent model with a total of 6 s of adaptation speech. Then, in order to obtain a more efficient initial model for the MAP–VFS algorithm, the initial model generation algorithm was added. This algorithm generates an initial model by using the speech of a selected speaker cluster based on speaker similarity in order to geta prioriknowledge concerning the characteristics of the target speaker. It was found that adaptation using this initial model reduces the phoneme recognition error rate from 22·0% to 17·7%, showing the effectiveness of using speaker similarity information asa prioriinformation.","['Masahiro Tonomura', 'Tetsuo Kosaka', 'Shoichi Matsunaga']",April 1996,Computer Speech & Language,[],Regular PaperSpeaker adaptation based on transfer vector field smoothing using maximum a posteriori probability estimation
96,"This paper introduces and reviews stochastic phonographic transduction (SPT), a trainable (“data-driven”) technique for letter-to-phoneme conversion based on formal language theory, as well as describing and detailing one particularly simple realization of SPT. The spellings and pronunciations of English words are modelled as the productions of a stochastic grammar, inferred from example data in the form of a pronouncing dictionary. The terminal symbols of the grammar are letter–phoneme correspondences, and the rewrite (production) rules of the grammar specify how these are combined to form acceptable English word spellings and their pronunciations. Given the spelling of a word as input, a pronunciation can then be produced as output by parsing the input string according to the letter-part of the terminals and selecting the “best” sequence of corresponding phoneme-parts according to some well-motivated criteria. Although the formalism is in principle very general, restrictive assumptions must be made if practical, trainable systems are to be realized. We have assumed at this stage that the grammar is regular. Further, word generation is modelled as a Markov process in which terminals (correspondences) are simply concatenated. The SPT learning task then amounts to the inference of a set of correspondences and estimation from the training data of their associated transition probabilities. Transduction to produce a pronunciation for a word given its spelling is achieved by Viterbi decoding, using a maximum likelihood criterion. Results are presented for letter–phoneme alignment and transduction for the dictionary training data, unseen dictionary words, unseen proper nouns and novel (pseudo-)words. Two different ways of inferring correspondences are described and compared. It is found that the provision of quite limited information about the alternating vowel/consonant structure of words aids the inference process significantly. Best transduction performance obtained on unseen dictionary words is 93·7% phonemes correct, conservatively scored. Automatically inferred correspondences also consistently out-perform a published set of manually derived correspondences when used for SPT. Although the comparison is difficult to make, we believe that current results for letter-to-phoneme conversion are at least as good as the best reported so far for a data-driven approach, while being comparable in performance to knowledge-based approaches.","['R.W.P. Luk', 'R.I. Damper']",April 1996,Computer Speech & Language,[],Regular PaperStochastic phonographic transduction for English☆
97,"When considering the speech as a non-stationary signal and giving up the quasi-stationary approach, the wavelet transform is a possible analysis method. Though this technique has been applied to a wide variety of speech processing tasks, the problem of how to derive the speech-tailored wavelets is not solved definitely. Therefore, in this paper the so-called perceptual wavelet transform and the corresponding speech representation are introduced as possible solutions. Although the proposed transform has been derived heuristically—namely, to be optimal in the perceptual frequency scale in Gábor-sense and to perform a 1 CB speech analysis—it appears that this is a self-invertible, overcomplete, shiftable transform. This is an important fact in the light of recent results in the so-called wavelet-frame-based denoising; moreover, the concept of soft-thresholding of the latter can also be introduced heuristically by applying the auditory masking phenomenon. The recent paper describes the perceptual wavelet functions in detail, presents the properties of this method in a time–domain analysis example, and two novel speech representations are given, which are somewhat similar to each other and to the conventional spectrogram. Finally, a new speech enhancement method is proposed, which consists of three stages: a perceptual wavelet-decomposition, followed by a compressive non-linearity (with adjustable parameters to the noise process), and summing.",['István Pintér'],January 1996,Computer Speech & Language,[],Regular articlePerceptual wavelet-representation of speech signals and its application to speech enhancement
98,"In this work we extend our previously proposed stochastic mixture trajectory models to modelling time correlation. To achieve this extension we explicitly model the time evolution of an observed trajectory by the sum of a first order AR process and a mean component. This approach generalizes that employed in Digalakiset al., by using a mixture of trajectories to represent a phone in a parameter space. This generalization is necessary—from our experience—to account for different contextual variants of a phone. Optimum parameter estimates are obtained by two embedded EM-algorithms. Evaluated on an 850-word vocabulary continuous speech recognition task, the new method reduced the recognition error rate by about 25%.","['Mohamed Afify', 'Yifan Gong', 'Jean-Paul Haton']",January 1996,Computer Speech & Language,[],Regular articleEstimation of mixtures of stochastic dynamic trajectories: application to continuous speech recognition
99,"An Automatic Language Identification (LID) approach is presented. The baseline LID system consists of three parts: (1) hidden Markov model (HMM) based context-independent phone recognizers, (2) language identification score generators and (3) a linear language classifier. The system exploits language-dependent phonotactic constraints and prosodic information. Four methods are proposed to improve the system performance. Two bigram-based interpolated N-gram language models (forward and backward) are used to model the phone sequence constraints of different spoken languages. A context-dependent duration model interpolated by a context-independent duration model is used to capture the duration information. Comparison experiments between the linear classifier and neural network-based final classifiers were conducted. Finally, optimization of language model based on back propagation is proposed. The improved system was evaluated on an 11-language task, and performance reached 13·3% and 26·2% (error rate) for utterances averaging 45 s duration and 10 s duration, respectively. Compared with the baseline system performance, it shows the importance of the issues addressed in this paper for language identification.","['Yonghong Yan', 'Etienne Barnard', 'Ronald A. Cole']",January 1996,Computer Speech & Language,[],Regular articleDevelopment of an approach to automatic language identification based on phone recognition
100,"We have already proposed the application of tree-structured speaker clustering to supervised speaker adaptation. This paper proposes its application to unsupervised speaker adaptation and speaker-independent (SI) speech recognition. This clustering involves the selection of a speaker cluster from among multiple reference speaker clusters arranged in a tree structure. Cluster selection, unlike parameter training, enables quick adaptation using only a small amount of training data. This method was applied to a hidden Markov network (HMnet) and evaluated in Japanese phoneme and phrase recognition experiments. Results show effective unsupervised speaker adaptation using only 5 s calibration speech. In the SI speech recognition experiments, the method reduced the error rate by 8·5% compared with the conventional speaker-independent speech recognition method.","['Tetsuo Kosaka', 'Shoichi Matsunaga', 'Shigeki Sagayama']",January 1996,Computer Speech & Language,[],Regular articleSpeaker-independent speech recognition based on tree-structured speaker clustering
101,"The method of Parallel Model Combination (PMC) has been shown to be a powerful technique for compensating a speech recognizer for the effects of additive noise. In this paper, the PMC scheme is extended to include the effects of convolutional noise. This is done by introducing a modified “mismatch” function which allows an estimate to be made of the difference in channel conditions or tilt between training and test environments. Having estimated this tilt, Maximum Likelihood (ML) estimates of the corrupted speech model may then be obtained in the usual way. The scheme is evaluated using the NOISEX-92 database where the performance in the presence of both interfering additive noise and convolutional noise shows only slight degradation compared with that obtained when no convolutional noise is present.","['M.F.J. Gales', 'S.J. Young']",October 1995,Computer Speech & Language,[],Regular PaperRobust speech recognition in additive and convolutional noise using parallel model combination
102,"This paper describes a set of modeling techniques for detecting a small vocabulary of keywords in running conversational speech. The techniques are applied in the context of a hidden Markov model (HMM) based continuous speech recognition (CSR) approach to keyword spotting. The word spotting task is derived from the Switchboard conversational speech corpus, and involves unconstrained conversational speech utterances spoken over the public switched telephone network. The utterances in this task contain many of the artifacts that are characteristic of unconstrained speech as it appears in many telecommunications based automatic speech recognition (ASR) applications. Results are presented for an experimental study that was performed on this task. Performance was measured by computing the percentage correct keyword detection over a range of false alarm rates evaluated over 2·2 h of speech for a 20 keyword vocabulary. The results of the study demonstrate the importance of several techniques. These techniques include the use of decision tree based allophone clustering for defining acoustic subword units, different representations for non-vocabulary words appearing in the input utterance, and the definition of simple language models for keyword detection. Decision tree based allophone clustering resulted in a significant increase in keyword detection performance over that obtained using tri-phone based subword units while at the same time reducing the size of the inventory of subword acoustic models by 40%. More complex representations of non-vocabulary speech were also found to significantly improve keyword detection performance; however, these representations also resulted in a significant increase in computational complexity.",['R.C. Rose'],October 1995,Computer Speech & Language,[],Regular PaperKeyword detection in conversational speech utterances using hidden Markov model based continuous speech recognition
103,"In this paper, a speech segment network approach for the construction of a suitable synthesis unit set with which high-quality speech can be synthesized, and yet which is of small enough size to be practical, is proposed. The speech segment network approach selects a synthesis unit set in which segmental and/or inter-segmental distortions are minimized by using combinatorial optimization methods such as iterative improvement and simulated annealing. Experimental results using diphone segments have shown that the suitable diphone unit sets, with total or maximum of inter-segmental distortion reduced by about 35 and 30%, respectively, can be constructed using this method. This reduction rate was enhanced as the segment candidate population increased. Effectiveness of this unit set design was also perceptually confirmed by a listening test, using speech synthesized with the selected diphone unit set.","['Naoto Iwahashi', 'Yoshinori Sagisaka']",October 1995,Computer Speech & Language,[],Regular PaperSpeech segment network approach for optimization of synthesis unit set
104,"This paper considers the problems of estimating bigram language models and of efficiently representing them by a finite state network, which can be employed by a hidden Markov model based, beam-search, continuous speech recognizer. A review of the best known bigram estimation techniques is given together with a description of the original Stacked model. Language model comparisons in terms of perplexity are given for three text corpora with different data sparseness conditions, while speech recognition accuracy tests are presented for a 10 000-word real-time, speaker independent dictation task. The Stacked estimation method compares favourably with the others, by achieving about 93% of word accuracy. If better language model estimates can improve recognition accuracy, representations better suited to the search algorithm can improve its speed as well. Two static representations of language models are introduced: linear and tree-based. Results show that the latter organization is better exploited by the beam-search algorithm as it provides a five times faster response with same word accuracy. Finally, an off-line reduction algorithm is presented that cuts the space requirements of the tree-based topology to about 40%.The proposed solutions presented here have been successfully employed in a real-time, speaker independent, 10 000-word real-time dictation system for radiological reporting.","['Marcello Federico', 'Mauro Cettolo', 'Fabio Brugnara', 'Giuliano Antoniol']",October 1995,Computer Speech & Language,[],Regular PaperLanguage modelling for efficient beam-search
105,"Keywords that are characteristic of a topic are ranked by a criterion of usefulness, and a relatively small number (typically 10) of the best keywords are implemented in a hidden Markov model (HMM)-based word-spotting algorithm. Statistical models of counts of occurrences of keywords are used to discriminate topic from non-topic speech. In the simplest model, distinct keywords are treated as independent. Particular combinations of keywords can often improve discrimination, however, and linear-logistic and log-linear models enable higher-order dependencies of the topic on keywords to be detected, leading to enhanced performance. Models involving a small number of parameters can be estimated from modest amounts of training data. Heuristic procedures point the way to good models but assume statistical consistency between training and test data. The approach is tested using a broadcast radio database.","['Jerry H. Wright', 'Michael J. Carey', 'Eluned S. Parris']",October 1995,Computer Speech & Language,[],Regular PaperTopic discrimination using higher-order statistical models of spotted keywords
106,"A text-based and spoken language processing framework based on the constraint dependency grammar (CDG) developed by Maruyama is discussed. The scope of CDG is expanded to allow for the analysis of sentences containing lexically ambiguous words, to allow feature analysis in constraints, and to efficiently process multiple sentence candidates that are likely to arise in spoken language processing. The benefits of the CDG parsing approach are summarized. Additionally, the development of CDG grammars using our grammar tools and parser is discussed.","['Mary P. Harper', 'Randall A. Helzerman']",July 1995,Computer Speech & Language,[],Regular PaperExtensions to constraint dependency parsing for spoken language processing
107,"A system of computer assisted grammar construction (CAGC) is presented in this paper. The CAGC system is designed to generate broad-coverage grammars for large natural language corpora by utilizing both an extended inside–outside algorithm and an automatic phrase bracketing (AUTO) system which is designed to provide the extended algorithm with constituent information during learning. This paper demonstrates the capability of the CAGC system to deal with realistic natural language problems and the usefulness of the AUTO system for constraining the inside–outside based grammar re-estimation. Performance results, including coverage, recall and precision, are presented for a grammar constructed for theWall Street Journal(WSJ) corpus using the Penn Treebank.","['H-H. Shih', 'S.J. Young', 'N.P. Waegner']",July 1995,Computer Speech & Language,[],Regular PaperAn inference approach to grammar construction
108,"A new quantitative model of tonal perception for continuous speech is described. The paper illustrates its ability for automatic stylization of pitch contours, with applications to prosodic analysis and speech synthesis in mind, and evaluates it in a perception experiment.After a discussion of the psycho-acoustics of tonal perception, and an overview of existing tonal perception models and systems for automatic analysis of intonation, the model and its computer implementation are described in detail. It includes parameter extraction, segmentation into syllables, perceptual integration of short term pitch change, tonal segment computation, and pitch contour stylization.This is followed by a perception experiment in which subjects are asked to distinguish original signals from resynthesized signals with automatically stylized pitch contours. The aim of this experiment is to show the usefulness of the model as a basis for intonation representation, and to study the influence of the model parameters. It is shown that the stylization obtained with the model is an economic representation of intonation which can be useful for speech synthesis and prosodic analysis.","['Christophe d»Alessandro', 'Piet Mertens']",July 1995,Computer Speech & Language,[],Regular PaperAutomatic pitch contour stylization using a model of tonal perception
109,"A speaker-independent automatic speech recognition system is developed using hidden Markov models (HMMs). Simulated annealing and randomized search are used to optimize discrete features of the system, including topologies, parameter ties, context clusters, and the sizes of mixture densities. Domain knowledge is used to initialize and to constrain the search, which optimizes recognition performance while reducing the number of model parameters. System performance results for new types of discrete and continuous HMMs measured on the TIMIT corpus are reported. The small set of context-independent phoneme HMMs produced is competitive with much larger systems of context-dependent models.","['Renato De Mori', 'Michael Galler', 'Fabio Brugnara']",April 1995,Computer Speech & Language,[],Regular PaperSearch and learning strategies for improving hidden Markov models
110,"In recent years there is much interest in word co-occurrence relations, such as n-grams, verb–object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co-occurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models. A background survey is included, covering issues of lexical co-occurrence, data sparseness and smoothing, word similarity and clustering, and mutual information.","['Ido Dagan', 'Shaul Marcus', 'Shaul Markovitch']",April 1995,Computer Speech & Language,[],Regular PaperContextual word similarity and estimation from sparse data
111,"A frequency-domain-based delay estimator is described, designed specifically for speech signals in a microphone-array environment. It is shown to be capable of obtaining precision delay estimates over a wide range of signal-to-noise ratio conditions and is computationally simple enough to make it practical for real-time systems. A location algorithm based upon the delay estimator is then developed. With this algorithm it is possible to localize talker positions to a region only a few centimetres in diameter (not very different from the size of the source), and to track a moving source. Experimental results using data from a real 16-element array are presented to indicate the true performance of the algorithms.","['Michael S. Brandstein', 'John E. Adcock', 'Harvey F. Silverman']",April 1995,Computer Speech & Language,[],Regular PaperA practical time-delay estimator for localizing speech sources with a microphone array
112,"A method of speaker adaptation for continuous density hidden Markov models (HMMs) is presented. An initial speaker-independent system is adapted to improve the modelling of a new speaker by updating the HMM parameters. Statistics are gathered from the available adaptation data and used to calculate a linear regression-based transformation for the mean vectors. The transformation matrices are calculated to maximize the likelihood of the adaptation data and can be implemented using the forward–backward algorithm. By tying the transformations among a number of distributions, adaptation can be performed for distributions which are not represented in the training data. An important feature of the method is that arbitrary adaptation data can be used—no special enrolment sentences are needed.Experiments have been performed on the ARPA RM1 database using an HMM system with cross-word triphones and mixture Gaussian output distributions. Results show that adaptation can be performed using as little as 11 s of adaptation data, and that as more data is used the adaptation performance improves. For example, using 40 adaptation utterances, a 37% reduction in error from the speaker-independent system was achieved with supervised adaptation and a 32% reduction in unsupervised mode.","['C.J. Leggetter', 'P.C. Woodland']",April 1995,Computer Speech & Language,[],Regular PaperMaximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models
113,"A major problem with most speaker adaptation schemes is that they rely on the speaker providing at least one example of each acoustic unit (word, phone, triphone, etc.) in the vocabulary in order to adapt the appropriate model. Rapid adaptation is difficult to achieve and some sounds may never be adapted because they are never heard. In this paper, a technique of adapting all the speech models to a new speaker's voice when he has given an incomplete set of the vocabulary is presented. The technique is based upon using the training-set to obtain estimates of correlations between sounds. Given some sounds from a new speaker at recognition time, these correlations are used to obtain estimates of unheard sounds which are used to adapt the speech models. The technique was applied to a database of 104 speakers speaking the English alphabet. When speakers spoke half of the vocabulary for enrollment prior to recognition, the technique gave a 78% decrease in error.",['Stephen Cox'],January 1995,Computer Speech & Language,[],Regular PaperPredictive speaker adaptation in speech recognition
114,"This research characterizes the spontaneous spoken disfluencies typical of human–computer interaction, and presents a predictive model accounting for their occurrence. Data were collected during three empirical studies in which people spoke or wrote to a highly interactive simulated system as they completed service transactions. The studies involved within-subject factorial designs in which the input modality and presentation format were varied. Spoken disfluency rates during human–computer interaction were documented to be substantially lower than rates typically observed during comparable human–human speech. Two separate factors, both associated with increased planning demands, were statistically related to higher disfluency rates: (1) length of utterance; and (2) lack of structure in the presentation format. Regression techniques demonstrated that a linear model based simply on utterance length accounted for over 77% of the variability in spoken disfluencies. Therefore, design methods capable of guiding users» speech into briefer sentences have the potential to eliminate the majority of spoken disfluencies. In this research, for example, a structured presentation format successfully eliminated 60–70% of all disfluent speech. The long-term goal of this research is to provide empirical guidance for the design of robust spoken language technology.",['Sharon Oviatt'],January 1995,Computer Speech & Language,[],Regular PaperPredicting spoken disfluencies during human–computer interaction
115,"To reduce the estimation error introduced by insufficient training data, the parameters of probabilistic models are usually smoothed by different techniques, such as Good–Turing smoothing and back-off smoothing. However, the discriminative power of the model cannot be significantly enhanced simply with the smoothing techniques. Therefore, in this paper an adaptive learning method is adopted to enhance the discrimination power of a probabilistic model. Also, a novel tying scheme is proposed to tie the unreliable parameters which never or rarely occurred in the training data, so that those unreliable parameters can have more chance to be adjusted by the learning procedure. In the task of tagging Brown Corpus, this approach greatly reduces the number of parameters from 578 759 to 27 947 and reduces the error rate of theambiguous words(i.e. the words with more than one possible part of speech) from 5?48 to 4?93%, corresponding to 10?4% error reduction rate. Furthermore, a probabilistic model is usually simplified to enable reliable estimates of its parameters using the limited amount of training data. As a consequence, the modelling error is increased because some discriminative features are sacrificed while simplifying that model. Therefore, a probabilistic classification model is proposed to reduce the modelling error by better using the discriminative features selected by theClassification and Regression Treemethod. This proposed model achieves 19?16% error reduction rate for the top 30 error-contributing words, which contribute 31?64% of the overall tagging errors.","['Y.-C. Lin', 'T.-H. Chiang', 'K.-Y. Su']",January 1995,Computer Speech & Language,[],"Regular PaperThe effects of learning, parameter tying and model refinement for improving probabilistic tagging"
116,"In this paper we report our development of a new class of hidden Markov models (HMMs) with each state characterized by a time series model which is non-stationary up to the second order. A close-form solution for the model parameter estimation is obtained based on the EM algorithm and on the matrix-calculus implementation technique. In the first set of evaluation experiments, we adopt the residual square sum, over states and over time frames within state bounds, as a quantitative measure for goodness of fit between the model and the speech data. It is observed that inclusion of state-conditioned second-order non-stationarity, implemented by use of time-varying regression coefficients, has substantially greater effects on reducing data-fitting error than increase of the regression terms while maintaining the coefficients of each term constant. In the second set, isolated-word recognition experiments, it is found that use of mix of first-order and second-order non-stationarities consistently produces higher recognition accuracy than the conventional, stationary-state HMMs.","['L. Deng', 'C. Rathinavelu']",January 1995,Computer Speech & Language,[],A Markov model containing state-conditioned second-order non-stationarity: application to speech recognition
117,"In this paper we present a general approach to identifying non-linguistic speech features from the recorded signal using phone-based acoustic likelihoods. The basic idea is to process the unknown speech signal by feature-specific phone model sets in parallel, and to hypothesize the feature value associated with the model set having the highest likelihood. This technique is shown to be effective for text-independent gender, speaker and language identification. Text-independent speaker identification accuracies of 98·8% on TIMIT (168 speakers) and 99·2% on BREF (65 speakers), were obtained with one utterance per speaker, and 100% with two utterances for both corpora. Experiments in which speaker-specific models were estimated without using the phonetic transcriptions for the TIMIT speakers had the same identification accuracies as those obtained with the use of the transcriptions. French/English language identification is better than 99% with 2 s of read, laboratory speech. For spontaneous telephone speech from the OGI corpus, the language can be identified as French or English with 82% accuracy with 10 s of speech. The ten language identification rate using the OGI corpus was 59·7% with 10 s of signal.","['Lori F Lamel', 'Jean-Luc Gauvain']",January 1995,Computer Speech & Language,[],A phone-based approach to non-linguistic speech feature identification
119,"The line spectrum pair (LSP) is one of the most popular and efficient parameters for representing the short-time spectrum of speech signal. About 34 bits/frame is needed for direct scalar quantization of LSP parameters to maintain a good quality. Based on the spectral-weighted Euclidean distance of LSP parameters, a modified transform coding of LSP parameters which takes advantage of both strong interframe and intraframe correlations is proposed in this paper. This method incorporated with the DPCM scheme does not introduce further buffering delay. It utilizes the spectral-weighted LSP parameters in transform coding instead of LSP vector itself and needs only to quantize the prediction residuals in transformed domain. The much smaller dynamic ranges and symmetrically bell-shaped distributions of those residuals explain the success of this scheme. With the frame period of 10 ms, the spectral distortion limen of 1 dB2 can be reached at 18 bits/frame with inside training test data and 19 bits/frame with outside training test data.","['Fu-Rong Jean', 'Chih-Chung Kuo', 'Hsiao-Chuan Wang']",October 1994,Computer Speech & Language,[],Regular ArticleOptimal transform coding for speech line spectrum pair parameters based on spectral-weighted error criterion
120,"Although the ability of human listeners to perceptually segregate concurrent sounds is well documented in the literature, there have been few attempts to exploit this research in the design of computational systems for sound source segregation. In this paper, we present a segregation system that is consistent with psychological and physiological findings. The system is able to segregate speech from a variety of intrusive sounds, including other speech, with some success.The segregation system consists of four stages. Firstly, the auditory periphery is modelled by a bank of bandpass filters and a simulation of neuromechanical transduction by inner hair cells. In the second stage of the system, periodicities, frequency transitions, onsets and offsets in auditory nerve firing patterns are made explicit by separate auditory representations. The representations, auditory maps, are based on the known topographical organization of the higher auditory pathways. Information from the auditory maps is used to construct a symbolic description of the auditory scene. Specifically, the acoustic input is characterized as a collection of time-frequency elements, each of which describes the movement of a spectral peak in time and frequency.In the final stage of the system, a search strategy is employed which groups elements according to the similarity of their fundamental frequencies, onset times and offset times. Following the search, a waveform can be resynthesized from a group of elements so that segregation performance may be assessed by informal listening tests. The system has been evaluated using a database of voiced speech mixed with a variety of intrusive noises such as music, ""office"" noise and other speech. A technique for quantitative evaluation of the system is described, in which the signal-to-noise ratio (SNR) is compared before and after the segregation process. After segregation, an increase in SNR is obtained for each noise condition. Additionally, the performance of our system is significantly better than that of the frame-based segregation scheme described by Meddis and Hewitt (1992).","['Guy J. Brown', 'Martin Cooke']",October 1994,Computer Speech & Language,[],Regular ArticleComputational auditory scene analysis
121,"A new n-gram model of natural language designed to aid speech recognition is presented in which the probabilities are calculated as a weighted average of maximum likelihood probabilities obtained from a training corpus. This simple approach produces a model that can be constructed quickly and is easily adapted either by changing the weights or by changing the training corpus. The model is compared with two other models; the first is based on Turing-Good estimates and uses Katz's back-off approach. The second model is a deleted estimate model which combines different probability distributions in approximately optimal proportions.We introduce a new measure for language models based on their performance when predicting words removed randomly from samples of unseen text. The performance of all three models using both this new measure and the existing measure of perplexity have been compared. Results indicate that the performance of the new model is close to the performance of the deleted estimate model, while both are superior to the Turing-Good model.","[""P. O'Boyle"", 'M. Owens', 'F.J. Smith']",October 1994,Computer Speech & Language,[],Regular ArticleA weighted average n-gram model of natural language
122,"In previous work we reported high classification rates for learning vector quantization (LVQ) networks trained to classify phoneme tokens shifted in time. It has since been shown that the framework of minimum classification error (MCE) and generalized probabilistic descent (GPD) can treat LVQ as a special case of a general method for gradient descent on a rigorously defined classification loss measure that closely reflects the misclassification rate. This framework allows us to extend LVQ into a prototype-based minimum error classifier (PBMEC) appropriate for the classification of various speech units which the original LVQ was unable to treat. Speech categories are represented using a prototype-based multi-state architecture incorporating a dynamic time warping procedure. We present results for the difficult E-set task, as well as for isolated word recognition for a vocabulary of 5240 words, that reveal clear gains in performance as a result of using PBMEC. In addition, we discuss the issue of smoothing the loss function from the perspective of increasing classifier robustness.","['Erik McDermott', 'Shigeru Katagiri']",October 1994,Computer Speech & Language,[],Regular ArticlePrototype-based minimum classification error/generalized probabilistic descent training for various speech units
123,"A key problem in the use of context-dependent bidden Markov models is the need to balance the desired model complexity with the amount of available training data. This paper describes a method which uses a simple agglomerative algorithm to cluster and tie acoustically similar states. The main properties of the algorithm are explored using phone recognition on the TIMIT database where it is shown that there is an optimum between the clustering extrema of an untied context-dependent system and a fully tied monophone system. At this optimum, phone recognition performance was 76·7% correct and 72·3% accuracy. The use of state-tying in the HTK continuous speech recognition system is then described and results are presented using the Resource Management database. The average error rate across the Feb '89, Oct '89 and Feb '91 test sets was less than 4·3% and this was achieved without cross-word triphones. Gender-dependent models were also compared to gender-independent models but found to give little improvement.","['S.J. Young', 'P.C. Woodland']",October 1994,Computer Speech & Language,[],Regular ArticleState clustering in hidden Markov model-based continuous speech recognition
124,"Fundamental frequency (f0) contours derived from the speech of 35 mothers to their 4-month-old infants were quantified for two experimental conditions, one in which the mother was instructed to seek her infant's attention and a second in which the mother was instructed to express approval of her infant's action. In addition to conventional descriptions (e.g. mean and standard deviation of f0, and utterance duration) the contours were subjected to modelling using 16 equations (1 linear and 15 non-linear) selected to reflect customary qualitative descriptions of f0 contours (e.g. Gaussian, rising, falling). Curve-fitting results confirmed that these infant-directed utterances were fit extremely well by at least one function. The average maximum R2 value obtained across all 16 equations was 0·83. Furthermore, discriminant analysis demonstrated that these two utterance types could be differentiated with 76% accuracy by these curve-fit results alone. Discrimination improved to 92% accuracy, however, when additional, more global descriptors were included in the discrimination function (e.g. utterance duration, mean fundamental). These results suggest that infants may be responsive to specific prosodic stimuli, which may involve distinct voice dynamics or more general speech signal characteristics, such as overall pitch, pitch variability, or utterance duration.","['Christopher A. Moore', 'Jeffrey F. Cohn', 'Gary S. Katz']",October 1994,Computer Speech & Language,[],Regular ArticleQuantitative description and differentiation of fundamental frequency contours
125,"It has been observed that humans can translate nearly four times as quickly with little loss in accuracy simply by dictating, as opposed to typing, their translations. In this paper, we consider the integration of speech recognition into a translator's workstation. In particular, we show how to combine statistical models of speech, language and translation into a single system that decodes a sequence of words in a target language from a sequence of words in a source language together with an utterance of the target language sequence. Results are provided which demonstrate that the difficulty of the speech recognition task can be reduced by making use of information contained in the source text being translated.","['P.F. Brown', 'S.F. Chen', 'S.A. Della Pietra', 'V.J. Della Pietra', 'A.S. Kehler', 'R.L. Mercer']",July 1994,Computer Speech & Language,[],Regular ArticleAutomatic speech recognition in machine-aided translation
126,"A system for automatic speech recognition (ASR) based on a new neural network design and a theory of articulatory phonology is presented. This system operates in two stages. In the first, speech acoustics are mapped by a neural network onto the movements of the tongue and lips that produced those acoustics (the neural networks are trained on X-ray microbeam recordings of actual articulatory movements); in the second stage, gestures are recovered from those movements. The neural network is built around a new objective function, Correlational + Scaling Error (COSE). When compared to a traditional neural network system, the COSE system trains faster, produces output which better represents the shape of the articulatory movements, and yields higher recognition rates for vowel gestures. After training on two speakers, recognition rates up to 96% for tokens from the training set and 87% for tokens spoken by a novel speaker were achieved.","['Jeff Zacks', 'Timothy R. Thomas']",July 1994,Computer Speech & Language,[],Regular ArticleA new neural network for articulatory speech recognition and its application to vowel identification
127,"In this paper we present a training method and a network architecture for estimating context-dependent observation probabilities in the framework of a hybrid hidden Markov model (HMM)/multi layer perceptron (MLP) speaker-independent continuous speech recognition system. The context-dependent modeling approach we present here computes the HMM context-dependent observation probabilities using a Bayesian factorization in terms of context-conditioned posterior phone probabilities which are computed with a set of MLPs, one for every relevant context. The proposed network architecture shares the input-to-hidden layer among the set of context dependent MLPs in order to reduce the number of independent parameters. Multiple states for phone models with different context dependence for each state are used to model the different context effects at the beginning and end of phonetic segments. A new training procedure that ""smooths"" networks with different degrees of context dependence is proposed to obtain a robust estimate of the context-dependent probabilities. We have used this new architecture to model generalized biphone phonetic contexts. Tests with the speaker-independent DARPA Resource Management database have shown average reductions in word error rates of 28% using a word-pair grammar, compared to our earlier context-independent HMM/MLP hybrid.","['Horacio Franco', 'Michael Cohen', 'Nelson Morgan', 'David Rumelhart', 'Victor Abrash']",July 1994,Computer Speech & Language,[],Regular ArticleContext-dependent connectionist probability estimation in a hybrid hidden Markov model-neural net speech recognition system
128,"In this work we demonstrate that explicit modeling of correlations between spectral parameters in speech recognition improves speech models both in terms of their descriptive power (higher likelihoods) and classification accuracy.Most large-vocabulary speech recognition systems are based on some form of hidden Markov models (HMMs) modeling sub-word speech segments. Most of the time speech segments are represented using short term spectra. In this work we employ three-state left-to-right phone models and LPC cepstral parameters including their first and second order time differentials. We investigate the importance of modeling correlations between cepstral parameters for high accuracy phone recognition.Several different types of distributions for each HMM state are compared. The simplest uses a single multivariate Gaussian distribution with a full covariance matrix. The next uses a weighted mixture of multivariate Gaussian distributions with diagonal covariances. It uses implicit rather than explicit modeling of parameter correlations. The most elaborate model employs a mixture of Gaussian distributions, just like the previous model, but in addition it uses a parameter space rotation which is specific to a given state in an HMM. It thus explicitly models parameter correlations in exactly the same way as the simplest model which uses a single distribution per state.The highest phone accuracy on the DARPA Resource Management task Feb 89 test set is obtained using the most elaborate model, with mixtures and space rotation - 82·4% phone accuracy. The next best result was achieved using single distributions, which also explicitly model parameter correlations, with 80·8% phone accuracy. The worst result was obtained using distributions which only implicitly model parameter correlations, achieving 78·7% phone accuracy. These results clearly demonstrate the importance of explicitly modeling parameter correlations for improving speech recognition performance.",['Andrej Ljolje'],July 1994,Computer Speech & Language,[],Regular ArticleThe importance of cepstral parameter correlations in speech recognition
129,A system was developed for the automatic segmentation of German words into morphs. The main linguistic knowledge sources used by the system are a word syntax and a morph dictionary. The syntax is written in the formalism of right linear regular grammars and comprises approximately 1400 rules describing the set of those sequences of morph classes which underlie syntactically well-formed words. The morph dictionary contains almost 11000 morphs. Each morph is assigned to up to six morph classes. Statistical evaluations with 6000 test words showed that more than 99% of the segmented words get a correct segmentation.,"['Thomas Pachunke', 'Oliver Mertineit', 'Klaus Wothke', 'Rudolf Schmidt']",July 1994,Computer Speech & Language,[],Regular ArticleThe linguistic knowledge in a morphological segmentation procedure for German
130,This paper describes work on the phrase-level recognition of British English intonation contours using continuous density hidden Markov models (CDHMMs) in conjunction with the 1961 O'Connor and Arnold intonation description scheme. The scheme is presented briefly together with the feature estimation and model optimization processes. The paper includes a presentation of methods for handling two fundamental problems in modelling F0-contours: unvoiced segments and F0-halving/doubling. Recognition performance is reported for both speech material recorded especially for these investigations and material extracted from the EUROM0 database.,"['U. Jensen', 'R.K. Moore', 'P. Dalsgaard', 'B. Lindberg']",July 1994,Computer Speech & Language,[],Regular ArticleModelling intonation contours at the phrase level using continuous density hidden Markov models
131,"In a text-to-speech synthesis system, input words not found in the system's lexicon are passed to letter-to-sound rules, which derive the word's pronunciation. In Welsh, the letter-to-sound rules must be applied in three passes; firstly, to add epenthetic vowels, secondly, to determine stress and vowel location, and thirdly, to perform grapheme-to-phoneme conversion. To begin with, all these letter-to-sound rules were written in the form of context-sensitive rewrite rules, and were evaluated, giving a 96% success rate. The rules for the second pass were then rewritten in the form of two-level rules, using the PCKIMMO software package. The output was identical to that produced by the second block of rewrite rules. The two-level formalism had advantages in simplifying rules. However, there were difficulties due to the need to force the rules to operate in a deterministic fashion. In a practical text-to-speech system, the rewrite rule formalism would be favoured, despite the greater number of rules and their greater clumsiness, since the critical ordering of rewrite rules easily introduces the necessary determinism.",['Briony Williams'],July 1994,Computer Speech & Language,[],Regular ArticleWelsh letter-to-sound rules: rewrite rules and two-level rules compared
132,"This paper describes np, a component of the AT&T Bell Laboratories English text-to-speech system that computes accentuation for constructions such as applecake, applepie and sumppump factor. Np uses both rule-based and statistical 'corpus-based' methods. These methods are discussed, and their benefits and shortcomings enumerated. The various components of np are evaluated, and it is shown that the overall performance of np significantly reduces the error rate in accent assignment over some simple-minded 'baseline' approaches. The paper concludes by outlining some future areas of research.",['Richard Sproat'],April 1994,Computer Speech & Language,[],Regular ArticleEnglish noun-phrase accent prediction for text-to-speech
133,"In natural speech, durations of phonetic segments are strongly dependent on contextual factors. For synthetic speech to sound natural, the module for computing segmental duration (the duration system) must mimic these contextual effects as closely as possible. Construction of a duration system is obstructed by two facets of segmental duration: (1) interactions between contextual factors, and (2) sparsity of training data. This paper describes a new duration system in which a central role is played by duration models, in the form of equations consisting of sums and products such as in: duration (/i/, voiced, stressed) = A(/i/ + B (voiced) &Times; C(stressed). These models, which we call sums-of-products models, can capture the types of interaction patterns often found in duration data, where one factor typically amplifies—but does not reverse—the effects of other factors. Yet, these models are mathematically sufficiently tractable for robust parameter estimation in the presence of severe sparsity. The overall architecture of the system consists of a category structure, or tree, that divides the space into similar-behaved cases; for each of these categories a separate sums-of-products model is developed and its parameters are estimated. Perceptual evaluation results are reported for an implementation in the AT&T Bell Laboratories text-to-speech system.",['Jan P.H. van Santen'],April 1994,Computer Speech & Language,[],Regular ArticleAssignment of segmental duration in text-to-speech synthesis
134,"A new phone recognizer has been implemented which extends the (phonotactic) decoding constraint to sequences of three phones. It is based on a structure similar to a second order ergodic hidden Markov model (HMM). This kind of a model assumes direct correspondence between the model states and phones, thus constraints on possible state sequences are equivalent to phonotactic constraints.Very high coverage by both left and right context-dependent phone models has been achieved using two methods. The first assumes that some contexts have the same or very similar effect on the phone in question. Thus they are merged into the same contextual class. The outcome is a set of 19 left context classes and 18 right context classes. The second assumes that left context mostly influences the beginning of a phone, whereas the right context influences the end of the phone. Each phone (a state in an ergodic HMM) is represented by a sequence of three probability density functions (pdfs), which is similar to a three state left-to-right HMM. We generate acoustic models such that the first pdf in the model is conditioned on the left context, the middle pdf is context independent (or it can also be context dependent), and the last pdf is conditioned on the right context. A large number of such quasi-triphonic acoustic models can be generated, thus providing a good triphone coverage for a given task, efficiently utilizing the available training data set.The current implementations of the recognizer described here have been applied to the DARPA Resource Management Task to demonstrate feasibility of performing phone (not phoneme) recognition using an untranscribed database, and the TIMIT database, for comparison to existing phone recognition systems. Since true phone sequences for the training utterances are not available for the RM database, they are estimated from text using a phone realization classification tree trained on the TIMIT database transcriptions. The estimates of the true phone sequences are used in training the models and generating reference phone sequences for scoring. The best phone recognition match between the most likely path through the classification tree and the phone recognizer output for the DARPA February 89 test set was 80·5% accurate and 84·0% correct. The best result obtained using the same recognizer structure on the TIMIT database is 69·4% accurate and 74·8% correct, which is a significant improvement over the best published result, when they are both reduced to the same phone set.",['Andrej Ljolje'],April 1994,Computer Speech & Language,[],Regular ArticleHigh accuracy phone recognition using context clustering and quasi-triphonic models
135,"Language models for speech recognition frequently differ in the way they deal with unknown words or in the size of their vocabulary. We show that this can distort the resulting perplexity, the measure commonly used to evaluate language models. We argue that this should generally be kept in mind when comparing language models using this measure and we also present some solutions to this problem. The main goal of this paper, however, is to propose a way of evaluating and improving existing language models for speech recognition. To this end, it introduces a method to measure which parts of a language model perform particularly well or poorly. In our view, this information is very important and can be a valuable help in steering future research efforts. We apply the proposed method to a commonly used language model (bi-pos) and present the results that will be helpful in directing future work. We also introduce a new modeling for unknown words which leads to a reduction in perplexity of at least 14%.",['Joerg Ueberla'],April 1994,Computer Speech & Language,[],Regular ArticleAnalysing a simple language model·some general conclusions for language models for speech recognition
136,"In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.","['Hermann Ney', 'Ute Essen', 'Reinhard Kneser']",January 1994,Computer Speech & Language,[],Regular ArticleOn structuring probabilistic dependences in stochastic language modelling
137,"In this paper, we propose the use of a tree-trellis search scheme for the task of large vocabulary Mandarin polysyllabic word recognition. Usually, the task of large vocabulary word recognition is computationally intractable by whole-word based approach. We convert this task into a tree network search problem by considering basic syllables as matching units. A vocabulary of 22 088 polysyllabic words is represented by a tree network linked with basic syllables. Some implementations of tree-trellis search scheme for this special task are investigated. In comparison with other search algorithms, the experimental results show that the tree-trellis search algorithm is a very promising one for this special task.","['Eng-Fong Huang', 'Frank K. Soong', 'Hsiao-Chuan Wang']",January 1994,Computer Speech & Language,[],Regular ArticleThe use of tree-trellis search for large-vocabulary Mandarin polysyllabic word speech recognition
138,"Statistical association measures play an important role in many areas of natural language research. An impediment to reliable estimation of such associations is the problem of small-sample statistics, since in any natural corpus there will be many infrequently occurring words.This paper proposes an alternative method for estimating such associations, which circumvents the small-sample issues. The idea is to view sentence/meaning pairs as algebraic equations, rather than as observations of a pattern in some class. Associations are estimated via solving these equations rather than via relative frequency estimates of mutual information. We develop a theoretical foundation for the dual algebraic/statistical nature of associations, proving two uniqueness theorems. We then exploit this theory to provide an algorithmic solution to the estimation problem. This algorithm is experimentally evaluated on 1494 natural languages messages from a rudimentary Data Retrieval experiment. One striking result is that the algebraic algorithm can often provide reliable estimates even for words which occur only once in a corpus.","['Naftali Tishby', 'Allen Gorin']",January 1994,Computer Speech & Language,[],Regular ArticleAlgebraic learning of statistical associations for language acquisition
141,"mu+ is a system for corpus based speech research that can be used to retrieve and analyse segments and their associated signal files from a large speech corpus. The segments can occur at many different levels (acoustic-phonetic, phonemic, intonational, prosodic), while the signal files can include the acoustic speech waveform, analysis parameters derived from the speech waveform (e.g. formant frequencies), and various articulatory measurements (e.g. kinematic parameters from lip and jaw movement). Most combinations of segment types, together with their boundary times and the speech signal files with which they are associated, can be retrieved hierarchically (all phonemes that occur in certain words), sequentially (all phonemes that occur in a particular triphone) or hierarchically and sequentially (e.g. all phonemes that occur in content words which are preceded by an intonational phrase of a particular type). The segments and their associated signal files that are retrieved from the speech database can be analysed subsequently using a wide range of statistical primitives and digital-signal-processing routines. The system has been developed to provide a common environment for experimentation in numerous facets of corpus based speech and language research including: articulatory and acoustic phonetics, prosodic analysis, speech technology research, and linguistic corpus development.","['J. Harrington', 'S. Cassidy', 'J. Fletcher', 'A. Mc Veigh']",October 1993,Computer Speech & Language,[],Regular ArticleThe mu + system for corpus based speech research
143,"In this paper, we first extended the semicontinuous hidden Markov model to incorporate multiple code-books. The robustness of the semicontinuous output probability is enhanced by the combination of multiple codewords and multiple codebooks. In addition, we compared the semicontinuous model with the continuous mixture model and the discrete model in a large-vocabulary speaker-independent continuous speech recognition (DARPA resource management) task. The model assumption and parameter size issues are addressed in particular through these experiments. When the acoustic parameters are not well modelled by the continuous probability density, the model assumption problems may cause the recognition accuracy of the semicontinuous model or the continuous mixture model to be inferior to the discrete model. We also found that the SCHMM can have a large number of free parameters in comparison with the discrete HMM because of its smoothing ability. With explicit male and female clustered models and for conditional feature sets, we were able to reduce the error rate of discrete-model-based SPHINX by more than 20%.","['X.D. Huang', 'H.W. Hon', 'M.Y. Hwang', 'K.F. Lee']",October 1993,Computer Speech & Language,[],"Regular ArticleA comparative study of discrete, semicontinuous, and continuous hidden Markov models"
144,"In this paper the Speech Maker Formalism (SMF) is described. Speech Maker is a general framework for text-to-speech synthesis (van Leeuwen & te Lindert, 1993. Computer Speech and Language 7, 149-167), and SMF is operational in this context. SMF has been developed as a general-purpose rule formalism for the application domain of text-to-speech synthesis. A large range of symbolic transformations that a linguist may want to specify in this domain can be formulated in SMF. A central characteristic of SMF is that it is two-dimensional and that the rules closely resemble the two-dimensional data structure they operate on. An extensive and detailed description is given of the possibilities of SMF, concerning both the specification of patterns in the data structure and the alteration of its contents. In particular, the completeness and the expressive power of the formalism are discussed.",['Hugo C. van Leeuwen'],October 1993,Computer Speech & Language,[],"Regular ArticleSpeech Maker Formalism: a rule formalism operating on a multi-level, synchronized data structure"
145,"Prosody is used by human listeners to disambiguate spoken language and, in particular, the relative size and location of prosodic phrase boundaries provides an important cue for resolving syntactic ambiguity. Therefore, automatically detected prosodic phrase boundaries should provide information useful in speech understanding for choosing among several candidate parses. Here, we propose two scoring algorithms to rank candidate parses, both based on an analysis/synthesis approach that compares the recognized prosodic phrase structure (analysis) with the predicted structure (synthesis) for each candidate parse. The two scoring algorithms, one rule-based and one using a probabilistic model, yield similar overall results when evaluated in experiments with a corpus of ambiguous sentences read by FM radio announcers. To decouple the performance of the analysis and synthesis components, we have used the scoring algorithms with hand-labeled breaks, which results in disambiguation performance comparable to the performance of human subjects in perceptual experiments. Performance degrades somewhat using automatically recognized breaks.","['M. Ostendorf', 'C.W. Wightman', 'N.M. Veilleux']",July 1993,Computer Speech & Language,[],Regular ArticleParse scoring with prosodic information: an analysis/synthesis approach
146,"On simple intelligibility measures, high-quality synthesiser output now scores almost as well as natural speech. Nevertheless, it is widely agreed that perception of synthetic speech is a harder task for listeners than perception of natural speech; in particular, it has been hypothesized that listeners have difficulty identifying phonemes in synthetic speech. If so, a simple measure of the speed with which a phoneme can be identified should prove a useful tool for comparing perception of synthetic and natural speech. The phoneme detection task was here used in three experiments comparing perception of natural and synthetic speech. In the first, response times to synthetic and natural targets were not significantly different, but in the second and third experiments response times to synthetic targets were significantly slower than to natural targets. A speed-accuracy tradeoff in the third experiment suggests that an important factor in this task is the response criterion adopted by subjects. It is concluded that the phoneme detection task is a useful tool for investigating phonetic processing of synthetic speech input, but subjects must be encouraged to adopt a response criterion which emphasizes rapid responding. When this is the case, significantly longer response times for synthetic targets can indicate a processing disadvantage for synthetic speech at an early level of phonetic analysis.","['Andrew J. Nix', 'Gita Mehta', 'Julie Dye', 'Anne Cutler']",July 1993,Computer Speech & Language,[],Regular ArticlePhoneme detection as a tool for comparing perception of natural and synthetic speech
147,"Over the last several years, a major factor in reducing the error rate on most speech recognition systems has been the addition of new feature components to the frame vectors. However, because of the larger dimensionality of the frame feature vector, the number of model parameters and the computational requirements have also increased. To improve recognition performance, it is not feasible to indefinitely increase the size of the frame feature vectors, nor is it satisfactory or practical to run experiments on combinatorially chosen subsets of the feature set to pick the best performing subspace with the desired number of dimensions. It becomes clearly desirable to understand which components of the frame feature vector provide the greatest contribution to recognition performance and to discard the least useful components. Our feature ordering method allows new sets of features to be selectively incorporated into existing signal analysis methods.Discriminative analysis has been successfully used for hidden Markov model (HMM) parameter estimation. In this study, we use discriminative methods to perform feature selection of the frame feature space. The components of the feature vectors are rank ordered according to an objective criterion and only the most ""significant"" are used for recognition. The proposed feature reduction method has been applied to a 38 dimension vector consisting of 1st and 2nd order time derivatives of the frame energy and of the cepstral coefficients with their 1st and 2nd derivatives. Speaker independent recognition experiments with reduced feature sets were performed on three databases of high quality non-telephone speech and a data base of telephone-based speech recorded during a field trial. The dimension of the frame feature vectors, and hence the number of model parameters, were greatly reduced (by a factor of 2) without a significant loss of recognition performance.","['E.L. Bocchieri', 'J.G. Wilpon']",July 1993,Computer Speech & Language,[],Regular ArticleDiscriminative feature selection for speech recognition
148,"Feature parameters describing spectral transitions of speech signals have been properly integrated with the instantaneous features in many different approaches proposed for speech recognition, and significant performance improvements have been attained. Most of these methods are designed for recognition systems based on dynamic time warping (DTW) or discrete hidden Markov models (HMM). However, it has been experimentally shown that for the difficult problem of recognizing the highly confusing Mandarin syllables with limited amount of training data, the performances of DTW and discrete HMM techniques are much worse than that of continuous HMMs. In this paper, the performance of continuous HMMs using one type of transitional features in speaker-dependent recognition of the highly confusing Mandarin syllables is first evaluated and discussed in detail under the constraint of very limited training data. Three approaches are then proposed to integrate the instantaneous and transitional features for recognition systems based on continuous hidden Markov models. They are the most straightforward concatenation—integration approach in which the instantaneous and transitional feature vectors are simply concatenated, the two-maximization approach in which the output distribution functions for the instantaneous and transitional feature vectors are maximized separately, and the two-model approach in which two HMMs respectively for instantaneous and transitional feature vectors are independently trained but the log likelihoods are summed up with proper weighting. After extensive experiments and careful analysis, it is found that the three approaches respectively provide attractive performance under different conditions. For example, with the two-maximization approach a recognition rate (93·89%) only slightly lower than the highest achievable rate for the concatenation-integration approach (94·36% for M = 5) can be obtained at a much smaller number of mixtures (M = 2).","['Yumin Lee', 'Lin-shan Lee']",July 1993,Computer Speech & Language,[],Regular ArticleContinuous hidden Markov models integrating transitional and instantaneous features for Mandarin syllable recognition
149,"This paper describes a speech recognizer based on an HMM representation of quantized articulatory features and presents experimental results for its evaluation. Traditional schemes for HMM representation of speech have attempted to model a set of disjoint time segments. In order to create a more robust speech recognition system, the speech production system is characterized by a set of articulatory features, each of which are allowed to vary over a range of discrete values. Each configuration of the articulatory system is characterized by a particular combination of feature values. ""Target configurations"" of the articulatory system are those configurations which produce the distinctive homogeneous segments in the acoustic signal. These feature values are permitted to vary independently and asynchronously (with appropriate constraints) as the production system moves from one target configuration to the next (such intermediate feature combinations are referred to as ""transitional configurations""). This avoids the abrupt model changes inherent in non-overlapping segment modeling. The feature value combinations that occur while in transit between target configurations represent the coarticulation intervals between the two targets. This scheme is implemented using an ergodic HMM to control the evolution of the feature values as the system moves from one target configuration to the next. Speech recognition results show that the new system outperforms the traditional HMM approaches in small tasks. Examination of the source of error, using Viterbi analysis, in both the new model and in traditional HMM recognition schemes suggests that this new scheme is able to achieve better modelling of the acoustic transitions and coarticulation in speech.","['Kevin Erler', 'Li Deng']",July 1993,Computer Speech & Language,[],Regular ArticleHidden Markov model representation of quantized articulatory features for speech recognition
150,"In this paper, we describe some features of the fundamental frequency (F0) contours of speech in Hindi and propose an approach to represent and activate this intonation knowledge for an unrestricted text-to-speech system for Hindi. As in most languages, the declarative sentences in Hindi show a declining pattern of F0 contour, whereas interrogative sentences show a rising F0 contour. The backdrop declining or rising pattern is characterized by local falls and rises which are determined by the phonological pattern of the constituent words. In complex declarative sentences the F0 contour resets at major syntactic boundaries. Experiments to obtain the inherent F0 of segments are described. The intonation knowledge derived from analysis of speech is coded in a production system format. Intelligibility and naturalness of the synthesized speech improved significantly after incorporation of the intonation rules.","['A.S. Madhukumar', 'S. Rajendran', 'B. Yegnanarayana']",July 1993,Computer Speech & Language,[],Regular ArticleIntonation component of a text-to-speech system for Hindi
151,"In most implementations of hidden Markov models (HMMs) a state is assumed to be a stationary random sequence of observation vectors whose mean and covariance are estimated. Successive observations in a state are assumed to be independent and identically distributed. These assumptions are reasonable when each state represents a short segment of the speech signal. When states represent longer portions of the signal (e.g. phonemes, diphones, etc.) both assumptions are inaccurate. Recently, some attempts have been made to incorporate correlations between successive observations in a state. But to our knowledge, non-stationarity has not been dealt with. We propose an alternative representation in which a state of an HMM is defined as a template, i.e. a ""typical"" sequence of observations. The template for a state is derived from an ensemble of segments corresponding to that state. In our present implementation, the observations are 11th-order cepstrum vectors plus energy, states represent diphones and ensembles of the diphones are obtained from a hand-labeled speaker-dependent database of 2000 sentences spoken fluently. The probability of a test sequence being generated in a given state is obtained by time-warping the test utterance to the template, and assuming the differences between the corresponding observations to have a joint distribution. Tests on 50 sentences (outside the training set) indicate a correct recognition rate for phonemes of about 70%.","['Oded Ghitza', 'M.Mohan Sondhi']",April 1993,Computer Speech & Language,[],Regular ArticleHidden Markov models with templates as non-stationary states: an application to speech recognition
152,"In order to achieve real-time performance, the spatio-temporal resolution of preprocessed data entering typical speech recognition systems is limited to a level which is approximately three orders of magnitude less than that required to avoid significant loss of speech information. The problem of reducing high-resolution data to a manageable level while minimizing the loss of speech information is therefore a key issue in the design of improved recognition systems. When we look at the natural auditory system, one of the first processes applied to the auditory nerve signal is the detection of sudden onsets and offsets in signal energy such as those which are associated with consonantal closure and burst release. Depending on the degree to which such events can successfully locate concentrations of information for stop consonants and other phonemes, on/off detectors could provide a basis for reducing the data-rate in the output from high-resolution auditory models while preserving essential speech information. In this paper we use the information theoretic measure of mutual information to investigate the distribution of phonetic information across the on/off aligned auditory spectrogram for a corpus of vowel-plosive-vowel utterances. Automatic recognition is then used to test to what extent small high information samples are sufficient for plosive discrimination.","['Andrew Morris', 'Jean-Luc Schwartz', 'Pierre Escudier']",April 1993,Computer Speech & Language,[],Regular ArticleAn information theoretical investigation into the distribution of phonetic information across the auditory spectrogram
153,"In order for speech recognizers to deal with increased task perplexity, speaker variation, and environment variation, improved speech recognition is critical. Steady progress has been made along those three dimensions at Carnegie Mellon. In this paper, we review the SPHINX-II speech recognition system and summarize our recent efforts on improved speech recognition.","['Xuedong Huang', 'Fileno Alleva', 'Hsiao-Wuen Hon', 'Mei-Yuh Hwang', 'Kai-Fu Lee', 'Ronald Rosenfeld']",April 1993,Computer Speech & Language,[],Regular ArticleThe SPHINX-II speech recognition system: an overview
154,"In this paper, a text-to-speech system for Dutch, called Spraakmaker, is described. It is based on a flexible underlying framework which has been devised for building text-to-speech systems. This framework is called Speech Maker. Central to this framework is a multi-level, synchronized data structure, based on the work of Hertz, Kadin and Karplus (1985), which we call grid . Its purpose is to contain all linguistic information relevant to the text-to-speech conversion process.First of all, we explain why we chose the grid data structure as the core of our framework. Then we describe how the general framework Speech Maker is constructed: apart from the grid data structure, a user interface to interact with the structure and a rule formalism to algorithmically manipulate it are also available. Finally, we discuss our implementation for the Dutch language, Spraakmaker, explaining which linguistic units we chose to represent in the grid, and how eight major linguistic modules operate on the grid in terms of these units.","['Hugo C. van Leeuwen', 'Enrico te Lindert']",April 1993,Computer Speech & Language,[],"Regular ArticleSpeech Maker: a flexible and general framework for text-to-speech synthesis, and its application to Dutch"
155,"In order to formalize the information used in spectrogram reading, a knowledge-based system for identifying spoken stop consonants was developed. Speech spectrogram reading involves interpreting the acoustic patterns in the image to determine the spoken utterance. One must selectively attend to many different acoustic cues, interpret their significance in light of other evidence, and make inferences based on information from multiple sources. The evidence, obtained from both spectrogram reading experiments and from teaching spectrogram reading, indicates that the process can be modeled with rules. Formalizing spectrogram reading entails refining the language used to describe acoustic events in the spectrogram, selecting a set of relevant acoustic events that distinguish among phonemes, and developing rules which map these acoustic attributes into phonemes. One way to assess how well the knowledge used by experts has been captured is by embedding the rules in a computer program. A knowledge-based system was selected because the expression and use of knowledge are explicit. The emphasis was in capturing the acoustic descriptions and modeling the reasoning used by human spectrogram readers. In this paper, the knowledge acquisition and knowledge representation, in terms of descriptions and rules, are described. A performance evaluation and error analysis are also provided, and the performance of an HMM-based phone recognizer on the same test data is given for comparison.",['Lori F. Lamel'],April 1993,Computer Speech & Language,[],Regular ArticleA knowledge-based system for stop consonant identification based on speech spectrogram reading
156,"We describe a method for the computer entry of Chinese characters using speech recognition with no vocabulary restrictions. The method exploits the fact that spoken Chinese has only about 400 syllables, and the syllable structure is extremely simple: either consonant/vowel group or vowel group alone. To each of the 23 consonants and 35 vowel groups that comprise a syllable, a code word, or Chinese phonetic alphabet representation (CPAR), is associated. All Chinese characters can be ""spelled-out"" using the 58 highly discriminable CPARs, thus allowing accurate transcription by means of current speech recognition technology.The CPAR system and the speech input method were tested with a discrete word speech recognizer in an experiment with four human subjects. On average, a subject could learn the method with 22 hours of training over a period of 1 or 2 weeks. Subjects were able to translate vocally about 40 characters of Chinese text into CPARs per minute with an accuracy in excess of 97%. This compares roughly to 31 English words per minute. The speech recognition system used was not fast enough to accommodate this rate of speaking, and in actual practice only 24 syllables, or the equivalent of 19 English words, per minute could be entered. Recently developed discrete speech recognition systems can now handle the higher rate; however, such equipment was not available at the time of the experiments reported here.","['Di Han', 'Robert D. Rodman', 'Michael G. Joost']",January 1993,Computer Speech & Language,[],Regular ArticleA voice system for the computer entry of Chinese characters
157,"The problem of recognizing strings of connected digits is crucial to a number of applications such as voice dialing of telephone numbers, automatic data entry, credit card entry, PIN (personal identification number) entry, entry of access codes for transactions, etc. Algorithms for connected digit recognition, based on whole-word reference patterns, have become increasingly sophisticated and have been shown capable of achieving high-recognition performance. Much of this complexity is derived from the design of specialized word models suitable solely for connected digit recognition. For example, in Doddington (1989), context-dependent modeling for the digits two and four and confusion class models for likely digit confusions were used to give high-performance digit recognition.Historically, the training and modeling techniques developed for connected digit recognition have been improved and successfully incorporated into large-vocabulary recognition systems. Based largely on these techniques, there have been proposed and implemented a number of systems for large vocabulary speech recognition which have achieved high word recognition accuracy. In this paper we reverse the direction of technology flow; namely, we show how we can apply the improved acoustic modeling techniques (using a continuous density hidden Markov model framework), developed for large-vocabulary speech recognition applications, to the problem of connected digit recognition with no changes made to the basic modeling techniques and with no vocabulary-specific information used.The improved modeling techniques adopted in this study include an improved feature analysis procedure, that incorporates higher-order cepstral and log energy time derivatives, and an improved acoustic resolution procedure, that uses more Gaussian mixture components per state to characterize the acoustic variability in each state of the model. Using these techniques, string accuracies of 98·6% for unknown length strings and 99·2% for known length strings were achieved on the standard Texas Instruments connected digits database. These string accuracies are a factor of 2 better than those previously reported using the same modeling procedures (Rabiner, et al., 1989b), and are even somewhat better than those reported by Doddington using specialized modeling techniques for the digits (Doddington, 1989).","['J.G. Wilpon', 'C.-H. Lee', 'L.R. Rabiner']",January 1993,Computer Speech & Language,[],Regular ArticleConnected digit recognition based on improved acoustic resolution
158,"An algorithm is described for synthesizing intonation in the absence of syntactic information as part of an experimental system for text-to-speech conversion for Dutch. It contains several improvements of an existing algorithm. The improvements are based on analyses of speech data collected from a professional reader. They mainly concern the intonational realization of intonation phrases. The performance of the algorithm has been evaluated in a perceptual test, in which listeners were asked to judge the naturalness of the intonation. For isolated utterances, the rule-based intonation is judged more natural than the intonation generated with the older algorithm, and as natural as the human intonation. For coherent text, the rule-based intonation is judged more natural than the intonation generated with the older algorithm, but less natural than the human intonation. These findings suggest that the decomposition of the utterance into intonation phrases by means of pitch makes an essential contribution to the naturalness of synthetic speech. In addition, they support earlier findings, reported in the literature, which point to the perceptual relevance of text intonation.",['Jacques Terken'],January 1993,Computer Speech & Language,[],Regular ArticleSynthesizing natural-sounding intonation for Dutch: rules and perceptual evaluation
159,"This paper describes perceptual methods for diagnosing problems in text-to-speech systems. Special attention is paid to two issues. First, coverage of the domain of a text-to-speech system. Since this domain involves an enormous range of contexts, it is criticial for diagnostics, and also for overall evaluation, that test materials cover this range to the fullest extent possible. Automatic text generation algorithms that make extensive use of ""greedy"" algorithms are described that serve this purpose. Second, speech generated by text-to-speech systems tends to have a great variety of problems . A battery of experimental paradigms is discussed that address different facets of speech quality and intelligibility. Included are: (a) ""word pointing"" method for detection of problematic concatenative units, (b) ""minimal pairs intelligibility test""—an expanded diagnostic rhyme test; (c) automatically scored orthographic name transcription task; (d) mean opinion score paradigm with problem categorization; and (e) paired comparison paradigm with strength-of-choice rating. The methods are applied in a series of experiments on high-end text-to-speech systems.",['Jan P.H. van Santen'],January 1993,Computer Speech & Language,[],Regular ArticlePerceptual experiments for diagnostic testing of text-to-speech systems
161,"A two-stage approach to phoneme label alignment is presented. A self-organizing neural network is employed in the first stage. The second stage performs the label alignment of an independently given input phoneme string to the corresponding speech signal. The first stage transforms signal parameters into a set of continuously valued acoustic-phonetic features. The second stage uses the Viterbi decoding/level building technique to position the label boundaries.The validity of the feature transformation approach in stage one is demonstrated in a detailed experimental analysis, the results of which are used to derive a multi-dimensional probability density model for all individual phonemes. These models are used in the second stage label alignment process.Results are given in two parts. The first provides the experimental evidence to support the use of probability density functions based on acoustic-phonetic features, in the form of histograms for a number of vocalic and consonantal Danish and British English phonemes. The second gives the results from the label alignment process. Here, differences between reference time boundaries from a manually labelled test speech corpus and time boundaries from the alignment process are presented in histograms showing the label alignment time differences for a number of selected phoneme paris for Danish and British English. The results show an overall accuracy of the label alignment of 85% and 43% for Danish and British English, respectively.",['Paul Dalsgaard'],October 1992,Computer Speech & Language,[],Phoneme label alignment using acoustic-phonetic features and Gaussian probability density functions
162,"We apply a trigram language model to an 86 000-word vocabulary speech recognition task. The recognition task consists of paragraphs chosen arbitrarily from a variety of sources, including newspapers, books, magazines, etc.The trigram language model parameters correspond to probabilities of words conditioned on the previous two words. The number of parameters to be estimated is enormous: 86 0003 parameters in our case. Even a training set consisting of 60 million words is too small to estimate these parameters reliably. Parameter estimates using relative frequencies would assign a value of zero to a large fraction of the parameters. Many algorithms have been proposed to estimate probabilties of events not observed in the training text. We propose here a simple algorithm for estimating the probabilities of such events using Turing's formula.The resulting trigram language model reduces the acoustic recognition errors by 60%. We also show that the effectiveness of the trigram language model for correcting an acoustic word recognition error depends on whether or not the neighbouring word contexts occur in the training text corpus for the language model.","['V. Gupta', 'M. Lennig', 'P. Mermelstein']",October 1992,Computer Speech & Language,[],A language model for very large-vocabulary speech recognition
163,"Many acoustic misrecognitions in our 86 000-word speaker-trained isolated-word recognizer are due to phonemic hidden Markov models (phoneme models) mapping to short segments of speech. When we force these models to map to longer segments corresponding to the observed minimum durations for the phonemes, then the likelihood of the incorrect phoneme sequences drops dramatically. This drop in the likelihood of the incorrect words results in significant reduction in the acoustic recognition1 error rate. Even in cases where acoustic recognition performance is unchanged, the likelihood of the correct word choice improves relative to the incorrect word choices, resulting in significant reduction in recognition error rate with the language model. On nine speakers, the error rate for acoustic recognition reduces from 18·6 to 17·3%, while the error rate with the language model reduces from 9·2 to 7·2%.We have also improved the phoneme models by correcting the segmentation of the phonemes in the training set. During training, the boundaries between phonemes are not marked accurately. We use energy to correct these boundaries. Application of an energy threshold improves the segment boundaries between stops and sonorants (vowels, liquids and glides), between fricatives and sonorants, between affricates and sonorants and between breath noise and sonorants. Training the phoneme models with these segmented phonemes results in models which increase recognition accuracy significantly. On two speakers, the error rate for acoustic recognition reduces from 26·5 to 23·1%, while the error rate with the language model reduces from 11·3 to 8·8%. This reduction in error rate is in addition to the error rate reductions obtained by imposing minimum duration constraints. The overall reduction in errors for these two speakers using minimum durations and energy thresholds is from 27·3 to 23·1% for acoustic recognition, and from 14·3 to 8·8% with the language model.","['V. Gupta', 'M. Lennig', 'P. Mermelstein', 'P. Kenny', 'P.F. Seitz', ""D. O'Shaughnessy""]",October 1992,Computer Speech & Language,[],Use of minimum duration and energy contour for phonemes to improve large vocabulary isolated-word recognition☆
164,"A two-dimensional cepstrum (TDC) approach for speech recognition is investigated in this paper. The TDC coefficients explicitly incorporate the correlations of neighbouring frames and well represent the spectral information of utterances. Only a small part of the TDC is necessary for speech recognition. Some experiments are conducted on the recognition of isolated Mandarin syllable finals and digits. The results show that the recognition rates of the TDC method is very close to those of the HMM method. However, the TDC method has the advantages of simple computation and less storage space.","['Hsiao-Fen Pai', 'Hsiao-Chuan Wang']",October 1992,Computer Speech & Language,[],A study of the two-dimensional cepstrum approach for speech recognition
165,"Hidden Markov models are commonly used for speech unit modelling. This type of model is composed of a non-observable or “hidden” process, representing the temporal structure of the speech unit, and an observation process linking the hidden process with the acoustic parameters extracted from the speech signal.Different types of hidden processes (Markov chain, semi-Markov chain, “expanded-state” Markov chain) as well as different types of observation processes (discrete, continuous, semi-continuous—multiple processes) are reviewed, showing their relationships. The maximum likelihood estimation of two-stage stochastic process parameters is presented in an a posteriori probability formalism. An intepretation of the expectation-maximization algorithm is proposed and the practical learning algorithms for hidden Markov models and hidden semi-Markov models are compared in terms of computation structure, probabilistic justification and complexity.This presentation is illustrated by experiments on a multi-speaker 130 isolated word recognition system. The implementation techniques are detailed and the different combinations of state occupancy modelling techniques and observation modelling techniques are studied from a practical point of view.",['Yann Guédon'],October 1992,Computer Speech & Language,[],Review of several stochastic speech unit models
166,"Word juncture coarticulation is one of the major sources of acoustic variability for initial and final word segments when spoken in fluent speech. One way to improve characterization of word pronuciations in continuous speech is to include inter-word contexts in lexical representations, similar to the way intra-word contexts are utilized. In this paper we investigate the issues related to the modeling of this set of inter-word, context-dependent units using continuous density hidden Markov models. Under such a modeling framework, it is usually required to have enough training tokens available for each unit in order reliably to estimate the parameters of the unit. Therefore, each context-dependent unit is included in the set of units to be modeled only when its frequency of occurrence in the training data exceeds a prescribed threshold. Testing such a unit selection and modeling strategy on the DARPA resource management task, it was found that the incorporation of inter-word units gave a 15–25% word error reduction compared to the base-line continuous speech recognition system using only intra-word units.","['E.P. Giachin', 'C.-H. Lee', 'L.R. Rabiner', 'A.E. Rosenberg', 'R. Pieraccini']",July 1992,Computer Speech & Language,[],On the use of inter-word context-dependent units for word juncture modeling
167,We present a fast method for identifying a short list of candidate words that match well with some acoustic input to serve as an initial matching stage in a large vocabulary isolated speech recognition system that uses hidden Markov models (HMM) and maximum likelihood decoding. This method is admissible in that the short list returned by it is guaranteed to contain the maximum likelihood word given by the HMMs. Given HMMs for all the words in the vocabulary we derive a class of algorithms that are faster than a detailed likelihood computation using these models by constructing an estimator of the likelihood. Using such an estimator we produce a list of candidate words that match well with the given acoustic input. We describe several possible estimators that can be used to construct such an admissible algorithm.,"['L.R. Bahl', 'P.S. Gopalakrishnan', 'D.S. Kanevsky', 'D. Nahamoo']",July 1992,Computer Speech & Language,[],A fast admissible method for identifying a short list of candidate words
168,"A system for part-of-speech tagging is described. It is based on a hidden Markov model which can be trained using a corpus of untagged text. Several techniques are introduced to achieve robustness while maintaining high performance. Word equivalence classes are used to reduce the overall number of parameters in the model, alleviating the problem of obtaining reliable estimates for individual words. The context for category prediction is extended selectively via predefined networks, rather than using a uniformly higher-order conditioning which requires exponentially more parameters with increasing context. The networks are embedded in a first-order model and network structure is developed by analysis of erros, and also via linguistic considerations. To compensate for incomplete dictionary coverage, the categories of unknown words are predicted using both local context and suffix information to aid in disambiguation. An evaluation was performed using the Brown corpus and different dictionary arrangements were investigated. The techniques result in a model that correctly tags approximately 96% of the text. The flexibility of the methods is illustrated by their use in a tagging program for French.",['Julian Kupiec'],July 1992,Computer Speech & Language,[],Robust part-of-speech tagging using a hidden Markov model☆
169,"The quantization property of layered neural networks is studied in this paper. We first review the layered neural network-based coders or quantizers developed in recent years and show that their poor performances is due to their independent training schemes for each component in the coder. Then an alternative model named a codebook-excited neural network is proposed, where an encoded vector is approximated by the output of the network driven by one vector selected from an excitation codebook. The network and the excitation codebook are jointly trained with the error back-propagation algorithm. Simulations with a Gauss-Markov source demonstrate that the quantization performance of the codebook-excited feedforward neural network is not worse than that of the connectionist vector quantizer formed by a set of single-layer neural units which satisfied the optimal quantization conditions, and that the performance of the codebook-excited recurrent neural network is very close to the asymptotic performance bound of block quantizers.The codebook-excited neural network is applicable with any distortion measure. For a zero-mean, unit variance, memory-less Gaussian source and a squared-error measure, a 1 bit/sample two-dimensional quantizer with a codebook-excited feedforward neural network is found always to escape from the local minima and converge to the best one of the three local minima which are known to exist in the vector quantizer designed using the LBG algorithm. Moreover, due to its conformal mapping characteristic, the codebook-excited neural network can be applied to designing the vector quantizer with any required structural form on its codevectors.","['Lizhong Wu', 'Frank Fallside']",July 1992,Computer Speech & Language,[],Source coding and vector quantization with codebook-excited neural networks
170,"This research outlines the predominant dialogue and performance characteristics of three-person interpreted telephone speech during service-oriented dialogues, in comparison with those of two-person non-interpreted dialogues. An empirical study was conducted in which 12 native English speakers each made one telephone call through an experienced telephone interpreter to a Japanese confederate who did not speak English, and a second call to a Japanese confederate fluent in English. In total, 24 dialogues were collected, each one containing two successfully completed service tasks, or 48 tasks in total. This paper reports on comparisons performed between three-person interpreted and two-person non-interpreted speech, based on the same pool of tasks and English subjects. The unique characteristics of interpreted telephone dialogues are outlined, including structural and referential features, miscommunications and other performance characteristics, confirmatory language and linguistic indirection. In addition, an analysis is presented of interpreters' strategic management of turn shifts, and of the content, sequencing and chunking of information passed among speakers. The long-term goal of this exploratory research is the theoretical account and modeling of human dialogue, the specification of preliminary target requirements for future automatic systems, and the optimization of human performance within those systems.","['Sharon L. Oviatt', 'Philip R. Cohen']",July 1992,Computer Speech & Language,[],Spoken language in interpreted telephone dialogues☆
171,"We report on some recent improvements to an HMM-based, continuous speech recognition system which is being developed at AT&T Bell Laboratories. These advances, which include the incorporation of inter-word, context-dependent units and an improved feature analysis, lead to a recognition system which gives a 95% word accuracy for speaker-independent recognition of the 1000-word DARPA resource management task using the standard word-pair grammar (with a perplexity of about 60). It will be shown that the incorporation of inter-word units into training results in better acoustic models of word juncture coarticulation and gives a 20% reduction in error rate. The effect of an improved set of spectral and log-energy, features is further to reduce word error rate by about 30%. Since we use a continuous density HMM to characterize each subword unit, it is simple and straightforward to add new features to the feature vector (initially a 24-element vector, consisting of 12 cepstral and 12 delta cepstral coefficients). We investigate augmenting the feature vector with 12 second difference (delta-delta) cepstral coefficients and with first (delta) and second difference (delta-delta) log energies, thereby giving a 38-element feature vector. Additional error rate reductions of 11% and 18% were achieved, respectively. With the improved acoustic modeling of subword units, the overall error rate reduction was over 42%. We also found that the spectral vectors, corresponding to the same speech unit, behave differently statistically, depending on whether they are at word boundaries or within a word. The results suggest that intra-word and inter-word units should be modeled independently, even when they appear in the same context. Using a set of subword units which included variants for intra-word and inter-word, context-dependent phones, an additional decrease of about 6–10% in word error rate resulted.","['C.-H. Lee', 'E. Giachin', 'L.R. Rabiner', 'R. Pieraccini', 'A.E. Rosenberg']",April 1992,Computer Speech & Language,[],Improved acoustic modeling for large vocabulary continuous speech recognition
172,"A microphone array system for speech data input must include a robust algorithm for determining the location of the desired talker. Here, a two-stage talker location algorithm based on filtered cross-correlation is introduced. At each stage, maximization of a sum-of-independent-cross-correlations functional is used to establish talker position. Suitable accuracy is obtained at low cost by using multirate interpolation. Experimental evidence has shown that a two-stage procedure improves performance; in the first stage, closely spaced microphone pairs are used to determine the x location of the talker (x0), and more broadly spaced pairs are used in the second stage to find y0 for a restricted range of x. Substantive results, based on real data, are presented to indicate performance. An efficient, global, non-linear optimization technique, stochastic region contraction (SRC), is briefly introduced and is shown to make this algorithm feasible in real time.","['Harvey F. Silverman', 'Stuart E. Kirtman']",April 1992,Computer Speech & Language,[],A two-stage algorithm for determining talker location from linear microphone array data☆
173,"Computational models of the auditory periphery exist, but progress beyond the periphery requires appropriate representations of auditory data. This paper describes a novel time-frequency representation of speech derived from a characterization of synchronous activity in an auditory model. Both spatial and temporal groupings of activity are made explicit. The representation, which we call synchrony strands, is the result of tackling an auditory temporal correspondence problem. The adequacy of the representation has been informally tested by resynthesis from a wide variety of speech and non-speech material. The application of synchrony strands in tasks such as computational auditory scene analysis is discussed.",['M.P. Cooke'],April 1992,Computer Speech & Language,[],An explicit time-frequency characterization of synchrony in an auditory model☆
174,"The relationship between the intonational characteristics of an utterance and other features inferable from its text represents an important source of information both for speech recognition, to constrain the set of allowable hypotheses, and for speech synthesis, to assign intonational features appropriately from text. This work investigates the usefulness of a number of textual features and additional intonational features in predicting the location of one particular intonational feature—intonational phrase boundaries—in natural speech. The corpus for this investigation is 298 utterances from the 774 in the DARPA-collected Air Travel Information Service (ATIS) database. For statistical modeling, we employ classification and regression tree (CART) techniques. We achieve success rates of just over 90%, representing a major improvement over previous attempts at boundary prediction for spontaneous speech.","['Michelle Q. Wang', 'Julia Hirschberg']",April 1992,Computer Speech & Language,[],Automatic classification of intonational phrase boundaries
177,"A multi-level, multi-tiered approach to the labelling of speech recordings is proposed. Five levels of segmental labelling are defined, at differing degrees of abstraction with reference to the physical speech signal, ranging from the physical level of acoustic parameters to the segmental sound structure of the citation forms of the words in the utterance. The issue of prosodic labelling is also addressed and related to the levels suggested for segmental labelling.","['W.J. Barry', 'A.J. Fourcin']",January 1992,Computer Speech & Language,[],Levels of labelling☆
178,"In recent years there has been a growing interest amongst the speech research community into the use of spectral estimators which circumvent the traditional quasi-stationary assumption and provide greater time-frequency (t-f) resolution than conventional spectral estimators, such as the short time Fourier power spectrum (STFPS). One distribution in particular, the Wigner distribution (WD), has attracted considerable interest. However, experimental studies have indicated that, despite its improved t-f resolution, employing the WD as the front end of speech recognition system actually reduces recognition performance; only by explicitly re-introducing t-f smoothing into the WD are recognition rates improved. In this paper we provide an explanation for these findings. By treating the spectral estimation problem as one of optimization of a bias variance trade off, we show why additional t-f smoothing improves recognition rates, despite reducing the t-f resolution of the spectral estimator.A practical adaptive smoothing algorithm is presented, whicy attempts to match the degree of smoothing introduced into the WD with the time varying quasi-stationary regions within the speech waveform. The recognition performance of the resulting adaptively smoothed estimator is found to be comparable to that of conventional filterbank estimators, yet the average temporal sampling rate of the resulting spectral vectors is reduced by around a factor of 10.","['D. Rainton', 'S.J. Young']",January 1992,Computer Speech & Language,[],Time-frequency spectral estimation of speech
179,"It is often conjectured that articulatory modelling of speech is desirable, if training data can be obtained. Articulatory parameters are likely to interpolate over long intervals, and reproduce many acoustic details from simple constraints governed by physical laws. For text-to-speech application, it may be easier to formulate rules in the articulatory domain.An analysis synthesis scheme for estimating the phoneme-level articulatory parameters to obtain best fits to natural speech, in the context of a text-to-speech (TTS) system, is presented. The working units of optimization are the parameters of an articulatory model (one vector per phoneme) and vectors of time and speed of transition for each parameter. The TTS system is used to initialize these parameters.Problems encountered in analysis synthesis are due to the following: (1) Automatic optimization methods work well only when the model is capable of matching the data closely. Otherwise, best solutions according to an objective error criteria are not necessarily sensible compromises according to perception. (2) Very different vocal tract shapes can produce speech spectra that are very similar (non-uniqueness). (3) Local minima can result from artifacts of the comparison strategy or due to the non-uniqueness in the spectrum-to-area transformation.Our solutions mainly consist of the following: (1) In each phoneme, adapt only those variables that have a large effect on the acoustic result (critical variables). (2) Use multiple error criteria sequentially to match large-scale spectral features in the first stage, and then, match the finer details using a high resolution criterion.It is demonstrated that good quality speech synthesis is possible through a phoneme-level control of an articulatory model. The synthesis is devoid of any artifacts, even in transitions, due to parameterization at the phoneme level. The quality of synthetic speech is comparable (at least in non-nasal voiced segments) to that of a good quality, frame-by-frame linear predictive coder. The models need to be embellished to reproduce other phonemes closely.","['S. Parthasarathy', 'Cecil H. Coker']",January 1992,Computer Speech & Language,[],On automatic estimation of articulatory parameters in a text-to-speech system
180,"Suprasegmental phenomena in synthetic speech should reflect the linguistic structure of the input text. An algorithm is described, which establishes the prosodic sentence structure (PSS). This can be achieved without exhaustive syntactic parsing, using a dictionary of 550 function words. Subsequently, phrase and accent locations are derived from the PPS; accentuation is also affected by some semantic and contextual information. Comparison of the resulting sentence prosody with that of a human (professional) speaker shows that more detailed syntactic analysis may be necessary. Most of the accentuation errors are caused by semantic, pragmatic and contextual factors. These factors can only partly be imitated (using heuristics), since the relations between linguistic representations and real-world knowledge are not yet fully understood.","['Hugo Quené', 'René Kager']",January 1992,Computer Speech & Language,[],The derivation of prosody for text-to-speech from prosodic sentence structure☆
184,"Near-term spoken language systems will likely be limited in their interactive capabilities. To design them, we shall need to model how limitations on speaker interaction influence spoken discourse patterns in different types of tasks. Speaker interaction is a central feature of human dialogue, one presumed to have a powerful influence on its discourse structure and performance efficiency. The present study examined two speech modalities that represent opposites on the spectrum of speaker interaction—the telephone dialogue and audiotape monologue. Experts provided spontaneous instructions by either telephone or audiotape as their novice partner completed an assembly task. Within this task framework, a comprehensive analysis is provided of the basic differences in discourse organization, referential characteristics and performance efficiency for these two spoken modalities. The outlined distinctions are interpreted with special reference to the role of confirmation feedback in promoting dialogue efficiency. Implications are discussed for the design of non-interactive speech technology and limited interaction spoken language systems.","['Sharon L. Oviatt', 'Philip R. Cohen']",October 1991,Computer Speech & Language,[],Discourse structure and performance efficiency in interactive and non-interactive spoken modalities☆
185,"Approximate maximum likelihood (ML) hidden Markov modeling using the most likely state sequence (MLSS) is examined and compared with the exact ML approach that considers all possible state sequences. It is shown that for any hidden Markov model (HMM), the difference between the approximate and the exact normalized likelihood functions cannot exceed the logarithm of the number of states divided by the dimension of the output vectors (frame length), which is negligible for typically used values of vector dimension (128–256) and number of states (2–30). Furthermore, for Gaussian HMMs and a given observation sequence, the MLSS is typically the sequence of nearest neighbor states in the Itakura-Saito sense, and the posterior probability of any state sequence which departs from the MLSS in a single time instant decays exponentially with the frame length. Hence, for a sufficiently large frame length the exact and approximate ML approaches provide similar model estimates and likelihood values. The results and their implications on speech recognition are demonstrated in a set of experiments.","['Neri Merhav', 'Yariv Ephraim']",October 1991,Computer Speech & Language,[],Hidden Markov modeling using a dominant state sequence with application to speech recognition
186,A method for constructing isomorphic context-specific connectionist networks for phoneme recognition is introduced. It is shown that such networks can be merged into a single context-modulated network that makes use of second-order unit interconnections. This is accomplished by computing a minimal basis for the set of context-specific weight vectors using the singular value decomposition algorithm. Compact networks are thus obtained in which the phoneme discrimination surfaces are modulated by phonetic context. These methods are demonstrated on a small but non-trivial vowel recognition problem. It is shown that a context-modulated network can achieve a lower error rate than a context-independent network by a factor of 7. Similar results are obtained using optimized rather than constructed networks.,['Raymond L. Watrous'],October 1991,Computer Speech & Language,[],Context-modulated vowel discrimination using connectionist networks☆
187,"In many applications, Chinese information is very often provided in the form of phonetic symbol sequences, and it is desired to decode such sequences into the corresponding Chinese character sequences (sentences) as the output. Phonetic input of Chinese characters into computers is a typical example. The problem is due primarily to the high degree of ambiguities caused by the large number of homonyms in Mandarin Chinese. In this paper, Markov models for Mandarin Chinese are developed to solve effectively the above decoding problem, and an efficient algorithm suitable for parallel processing based on dynamic programming is further proposed to search fully the solution space of exponential size in polynomial time. Extensive experiments were performed and the results show that appropriate models with proper training conditions can effectively solve the above problem, and the techniques developed here are also suitable for real-time applications.","['Hung-yan Gu', 'Chiu-yu Tseng', 'Lin-shan Lee']",October 1991,Computer Speech & Language,[],Markov modeling of Mandarin Chinese for decoding the phonetic sequence into Chinese characters
188,"This paper proposes a method for hypothesizing word boundaries in Hindi speech. The method is based on the observation that function words such as case markers, pronouns and conjunctions occur frequently in Hindi text and spotting of these frequently occurring patterns is proposed as a means for hypothesizing word boundaries in a speech-to-text conversion system for Hindi. Initially, the idea was tested on a correct text with all word boundaries (except sentence boundaries) removed; the results showed that nearly 67% of the word boundaries were correctly hypothesized. Later, experiments with input containing errors simulated to represent speech environment showed that the proposed method is effective even at error levels as high as 50%. The implications of these results in the development of a speech-to-text conversion system for Hindi are discussed.","['G.V. Ramana Rao', 'B. Yegnanarayana']",October 1991,Computer Speech & Language,[],Word boundary hypothesization in Hindi speech
189,"The Kohonen self-organizing feature map has been widely used for the design of connectionist vector quantizers (VQ). One of the features of the Kohonen algorithm is that the weight update gain sequence η(m) is a decreasing function of the number of iterations, and if incorrectly chosen can lead to very long training times. Here we derive the time-optimal gain sequence and demonstrate its efficacy for a number of cases. The performance is demonstrated using a Gauss-Markov source and compared with a VQ designed using the LBG algorithm. It is demonstrated that the new method is time optimal and that its performance tends to that of the VQ with the LBG algorithm. The special case of a connectionist VQ for linear predictive or auto-regressive data is analysed. The optimal design is derived for the Itakura distance measures. It is again compared with the LBG design.Two connectionist models for finite-state vector quantizers (FSVQ) are proposed. These models learn not only the distribution but also the conditional distribution of a source. The connectionist FSVQs exploit the redundancy between the vectors (frames) of a highly correlated source, and further improve the rate-distortion performance. Their architectures and learning algorithms are presented.To evaluate the performance of the proposed quantizers, comparisons between quantized and unquantized speech spectra are presented. The connectionist VQ and FSVQ for linear predictive data are finally applied to a multipulse linear predictive speech coder using data from the TIMIT database. Comparisons are made of waveforms and rate-distortion functions.","['Lizhong Wu', 'Frank Fallside']",July 1991,Computer Speech & Language,[],On the design of connectionist vector quantizers
190,"For hidden Markov model (HMM) based speech recognition where the basic speech unit is smaller than the recognizer's output unit, the standard full Baum-Welch re-estimation procedure for the HMM training is very costly in computation. This is because it requires evaluation of the HMM output densities and of the forward/backward probabilities in the entire region of the state-frame trellis. In this paper, we present an algorithm which exploits the fact that the entries of the trellis are essentially zero except near the block diagonal and hence achieves significant computational saving. The algorithm is evaluated in experiments with a large vocabulary word recognizer based on mixture-density HMM representation of phonemes. The HMM parameters trained with the new algorithm are essentially identical to those trained with the full Baum-Welch algorithm in that the resulting HMMs have nearly the same likelihood values on the same set of training data. Identical word recognition accuracies are yielded using the HMMs trained with the two algorithms. However, the new algorithm is shown to be about an order of magnitude faster than the full Baum-Welch algorithm.",['Li Deng'],July 1991,Computer Speech & Language,[],The semi-relaxed algorithm for estimating parameters of hidden Markov models
191,"This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.","['K. Lari', 'S.J. Young']",July 1991,Computer Speech & Language,[],Applications of stochastic context-free grammars using the Inside-Outside algorithm
192,"This paper describes a speaker-independent phoneme and word recognition system based on a recurrent error propagation network (REPN) trained on the TIMIT database.The REPN is a fully recurrent error propagation network trained by the propagation of the gradient signal backwards in time. A variation of the stochastic gradient descent procedure is used which updates the weights by an adaptive step size in the direction given by the sign of the gradient.Phonetic context is stored internal to the network and the outputs are estimates of the probability that a given frame is part of a segment labelled with a context-independent phonetic symbol.During recognition, a dynamic programming match is made to find the most probable string of symbols. The one pass algorithm is used for phoneme and word recognition.The phoneme recognition rate for all 61 TIMIT symbols is 70·0% correct (63·5% accuracy including insertion errors) and on a reduced 39-symbol set the recognition rate is 76·5% correct (69·8%). This compares favourably with the results of other methods, such as HMMs, on the same database [K. F. Lee & H. W. Hon 1989. IEEE Transactions on Acoustics, Speech and Signal Processing, 37, 1641–1648; S. E. Levinson, M. Y. Liberman, A. Ljolje & L. G. Miller 1989. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. Glasgow, pp. 441–444].Analysis of the phoneme recognition results shows that information available from bigram and durational constraints is adequately handled within the network allowing for efficient parsing of the network output. For comparison, there is less computation involved in the resulting scheme than in a one-state-per-phoneme HMM system. This is demonstrated by applying the recognizer to the DARPA 1000-word resource management task. Parsing the network output to the word level with no grammar and no pruning can be carried out in faster than real time on a SUN 4330 workstation.","['Tony Robinson', 'Frank Fallside']",July 1991,Computer Speech & Language,[],A recurrent error propagation network speech recognition system☆☆☆
193,"The problem of automatic speech recognition in an adverse environment has attracted the attention of many researchers. The main reason is that the performance of existing speech recognition systems, whose designs are predicated on assumptions about the environment conditions, such as low noise or low interference, degrades rapidly in the presence of noise and distortion.In this paper, we review several promising methods that were proposed in the past few years to deal with this problem. We discuss methods or algorithms in six categories: signal enhancement preprocessing; special transducer arrangements; noise masking; stress compensation; robust distortion measures; and novel speech representations. We explain each type of approach and provide a summary of the performance improvements each method is able to achieve. This type of information is helpful in making a technical decision for the actual recognizer design to be deployed in adverse environments.",['B.H. Juang'],July 1991,Computer Speech & Language,[],Speech recognition in adverse environments
195,"At present, automatic speech recognition technology is based upon constructing models of the various levels of linguistic structure assumed to compose spoken language. These models are either constructed manually or automatically trained by example. A major impediment is the cost, or even the feasibility, of producing models of sufficient fidelity to enable the desired level of performance.The proposed alternative is to build a device capable of acquiring the necessary linguistic skills in the course of performing its task. We call this learning by doing, and contrast it with learning by example. The purpose of this paper is to describe some basic principles and mechanisms upon which such a device might be based, and to recount a rudimentary experiment evaluating their utility.Spoken language, the original natural language, evolved in order for humans to convey importnat messages to each other. A first principle, then, is that the primary function of language is to communicate. A consequence of this principle is that language acquisition involves gaining the capability of decoding the message. This is in contrast to much of the research on automated language acquisition, which focuses on discovering syntactic structure, often specifically to the exclusion of meaning. Our first principle leads us to investigate a language acquisition mechanism based on connectionist methods, in which the network builds associations between messages and meaningful responses to them.People learn while performing a task by receiving feedback as to the appropriateness of their actions. A second principle, then, is that the actual construction of the mapping from messages to meaning should be governed by a feedback control system where the error signal is at the level of meaning. This is in contrast to some learning research, which is governed by providing input/output pairs, and where the error signal is a parameter-space distortion measure. Our second principle leads us to investigate a mechanism for human-machine interaction based on control-theory methods, where the system input is the message and the error signal is a measure of appropriateness of the machine's response.The utility of these principles is demonstrated and evaluated by applying them to an elementary inward-call-management task, the object of which is to connect a caller to the department of a large organization appropriate to his inquiry. Initially, the system knows nothing about the language for its task, that is no vocabulary, no grammer, and no semantic associations. In the course of directing incoming calls, the system acquires a vocabulary, learns the meaning of words and some rudimentary grammatical relationships relevant to its task. The mechanism used is a particular connectionist network embedded in a feedback control system which adjusts the connection weights of the network based on the success or failure of the machine's behavior, as evaluated by the caller's reaction to it. This mechanism has several intriguing mathematical properties.An experimental evaluation of the system has been conducted using typed rather than spoken input. The system was tested by 12 subjects over a 2-month period. Over 1000 conversations were held, during which the machine acquired a vocabulary of over 1500 words. Subsequent tests showed that the learning was stable, in that it retained 99% of the knowledge it had acquired in the interactions.Although the experiments conducted thus far are of a rudimentary nature, we consider them to be the early stages in a long-term study of automatic acquisition of intelligence by machines through interaction with a complex environment.","['A.L. Gorin', 'S.E. Levinson', 'A.N. Gertner', 'E. Goldman']",April 1991,Computer Speech & Language,[],Adaptive acquisition of language
196,"Automatic speech recognition based on segments is preferred for its flexibility and power, despite the difficulty of extracting segmental information from the acoustic speech stream. The paper argues that successful segment labeling will not bring all the expected benefits, and that some higher-level knowledge is needed. An implementation of phonological knowledge, using finite state transducers (FSTs) and based on the “two-level” morphological parser of Koskenniemi [Koskenniemi, K. (1983). Two-level morphology. Texas Linguistic Forum, 22, 1–167] and Ritchie et al. [Ritchie et al. (1987). The Edinburgh-Cambridge Morphological Analyser and Dictionary System (Prototype: Version 2.4) User Manual. Cambridge University Computer Manual] is introduced. This provides a means of displaying the advantages and remaining difficulties of phonology in an ASR system. A disadvantage of this approach is the over-generation of hypothesized lexical strings in response to a given input, and the remainder of the paper investigates ways of restricting this overgeneration. Methods involve both further recourse to the acoustic signal (detection of the speaker's accent) and the invocation of more higher-level (morphological and syntactic) knowledge.","['Charles Hoequist', 'Francis Nolan']",April 1991,Computer Speech & Language,[],On an application of phonological knowledge in automatic speech recognition☆
197,"Words uttered in isolation are pronounced differently than when they are uttered in continuous speech, a major cause being coarticulation at word junctures. Between-word context-dependent phones have been proposed to provide a more precise phonetic representation of word junctures. This technique permits one to accurately model “soft” pronunciation changes (changes in which a phone undergoes a comparatively small alteration). However, “hard” pronunciation changes (changes in which a phone is completely deleted or replaced by a different phone) are much less frequent and hence cannot be modeled adequately due to the lack of training material. To overcome this problem we use a set of phonological rules to redefine word junctures, specifying how to replace or delete the boundary phones according to the neighboring phones. No new speech units are required, thus avoiding most of the training issues. Results, which are evaluated on the 991-word speaker-independent DARPA task, show that phonological rules are effective in providing corrective capability at low computational cost.","['Egidio P. Giachin', 'Aaron E. Rosenberg', 'Chin-Hui Lee']",April 1991,Computer Speech & Language,[],Word juncture modeling using phonological rules for HMM-based continuous speech recognition
198,"The application of a simple variable frame rate analysis to a continuous speech recognition system based on phone-level hidden Markov models, is described. Results are presented which show that, using standard three-state models, the addition of the variable frame rate analysis results in considerably improved performance, which is close to that obtained using simple duration sensitive models.","['K.M. Ponting', 'S.M. Peeling']",April 1991,Computer Speech & Language,[],The use of variable frame rate analysis in speech recognition
199,"In this paper, several special speech recognition approaches based on hidden Markov models (HMMs) are presented for the highly confusing Mandarin syllables by considering the characteristics of the vocabulary. This is because there are totally 408 syllables (disregarding the tones) in Mandarin speech, and it is believed that correct recognition of these syllables is the key to the development of a Mandarin dictation machine which recognizes Mandarin speech with very large vocabulary and unlimited texts. However, accurate recognition of these 408 syllables is very difficult because there exist 38 confusing sets among them, each of which has at most 19 very confusing syllables. Direct application of conventional standard approaches of HMMs to these syllables gives recognition rates in the order of only 70–80%, thus several special approaches are proposed in this paper to provide better performance. Some of these approaches concentrate on the training algorithms for the HMMs, including the two-pass training, the revised two-pass training, the three-pass training, and the revised three-pass training approaches. The basic idea is to protrude the very short initial parts (initial consonant parts) and de-emphasize the final parts (vowel or diphthong parts but including possible medial or nasal ending) of the syllables such that the confusing syllables can be better distinguished. Also, some other approaches concentrate on the recognition phase of the HMMs, including the state duration bounds and the two-stage search strategy, which can also improve the recognition rates and/or speeds. A special hardware is also implemented to complete all the recognition operations in real-time, on which all the approaches discussed in this paper can be applied.","['Lin-shan Lee', 'Chiu-yu Tseng', 'Fu-hua Liu', 'C.H. Chang', 'Hung-yan Gu', 'S.H. Hsieh', 'C.H. Chen']",April 1991,Computer Speech & Language,[],Special speech recognition approaches for the highly confusing Mandarin syllables based on hidden Markov models
203,"Speech signals show various types of non-stationary phenomena, one of these is the time varying behavior of formant frequencies. Spectrogram representations are widely used tools which are used to visualize speech signals. In this paper it is shown that the formant tracks of rapidly time varying speech are displayed correctly by spectrograms. Furthermore, if the model of the time variant formant is based on the notion of instantaneous frequency, the discrepancies in the interpretation of the spectrograms in a paper published by (H. F. Silverman & Y. T. Lee [1987]. Computer Speech and Language, 2, 63–86) disappear.",['Wolfgang Wokurek'],January 1991,Computer Speech & Language,[],Comments on “On the spectrographic representation of rapidly time varying speech”
204,An automatic speech recognizer for isolated word recognition of monosyllabic languages is proposed. Energy-time profiles of utterances are used as feature parameters and no dynamic time warping is employed. A probability matrix is computed which gives a similarity measure between an input token and the reference templates. The final matching decision is made upon the statistical probability criterion. Overall accuracy of 96% has been obtained for speaker independent recognition of a small vocabulary. The simplicity of the algorithm enables a low-cost real-time implementation of the recognizer.,"['C.K. Yu', 'P.C. Ching']",January 1991,Computer Speech & Language,[],A probability decision criterion for speech recognition
205,"In principle, n-gram probabilities can be estimated from a large sample of text by counting the number of occurrences of each n-gram of interest and dividing by the size of the training sample. This method, which is known as maximum likelihood estimator (MLE), is very simple. However, it is unsuitable because n-grams which do not occur in the training sample are assigned zero probability. This is qualitatively wrong for use as a prior model, because it would never allow the n-gram, while clearly some of the unseen n-grams will occur in other texts. For non-zero frequencies, the MLE is quantitatively wrong. Moreover, at all frequencies, the MLE does not separate bigrams with the same frequency.We study two alternative methods. The first method is an enhanced version of the method due to Good and Turing (I. J. Good [1953]. Biometrika, 40, 237–264). Under the modest assumption that the distribution of each bigram is binomial, Good provided a theoretical result that increases estimation accuracy. The second method is an enhanced version of the deleted estimation method (F. Jelinek & R. Mercer [1985]. IBM Technical Disclosure Bulletin, 28, 2591–2594). It assumes even less, merely that the training and test corpora are generated by the same process.We emphasize three points about these methods. First, by using a second predictor of the probability in addition to the observed frequency, it is possible to estimate different probabilities for bigrams with the same frequency. We refer to this use of a second predictor as “enhancement.” With enhancement, we find 1200 significantly different probabilities (with a range of five orders of magnitude) for the group of bigrams not observed in the training text; the MLE method would not be able to distinguish any one of these bigrams from any other. The probabilities found by the enhanced methods agree quite closely in qualitative comparisons with the standard calculated from the test corpus.Second, the enhanced Good-Turing method provides accurate predictions of the variances of the standard probabilities estimated from the test corpus. Third, we introduce a refined testing method that enables us to measure the prediction errors directly and accurately and thus to study small differences between methods. We find that while the errors of both methods are small due to the large amount of data that we use, the enhanced Good-Turing method is three to four times as efficient in its use of data as the enhanced deleted estimate method. Good-Turing method is preferable to the enhanced deleted estimate method. Both methods are much better than MLE.","['Kenneth W. Church', 'William A. Gale']",January 1991,Computer Speech & Language,[],A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams
206,"Humans can pronounce novel orthographically regular text strings such as pseudowords, like “peemers”, or words they have never seen before. How do they do this? Two hypotheses have been proposed to account for this ability. According to one view, pronunciation-by-rule, pseudowords are pronounced by a rule-based phonological process in which the pronunciation for a pseudoword is generated from its spelling by the use of a complex set of spelling-to-sound rules. According to the alternative view, pronunciation-by-analogy, pseudowords are pronounced by analogy to known words which are similar in spelling. Although the pronunciation-by-analogy approach is psychologically plausible, it is not clear that it is computationally feasible. Pronunciation-by-analogy depends on the degree to which orthographic consistency in the spelling patterns of words is related to phonotactic consistency in pronouncing those words. To investigate this theoretical issue, we developed a computer program called PRONOUNCE that automatically generates a set of rank-ordered pronunciations, in the form of a sequence of phonetic segments, using pronunciation-by-analogy with a lexicon of approximately 20 000 words based on Webster's Pocket Dictionary. PRONOUNCE examines every word in the lexicon and builds a pronunciation lattice structure using the phonetic representations of the words that match the input string. In this pronunciation lattice, each node represents a possible phoneme to be used at a particular position in the pronunciation, and each path through the lattice represents a possible pronunciation. At this time, PRONOUNCE performs reasonably well, generally producing pronunciations that agree with those given by native speakers of English. PRONOUNCE was tested on a set of 70 short pseudowords and was found to disagree with human subjects on only 9% of the pseudowords. These results suggest that pronunciation-by-analogy is indeed computationally feasible. Furthermore, the limited success of PRONOUNCE suggests a new approach to spelling-to-sound conversion for text-to-speech conversion systems.","['Michael J. Dedina', 'Howard C. Nusbaum']",January 1991,Computer Speech & Language,[],PRONOUNCE: a program for pronunciation by analogy
207,"The paper describes the architecture of VODIS, a voice operated database inquiry system, and presents some experiments which investigate the effects on performance of varying the level of a priori syntactic constraints. The VODIS system includes a novel mechanism for incorporating context-free grammatical constraints directly into the word recognition algorithm. This allows the degree of a priori constraint to be smoothly varied and provides for the controlled generation of multiple alternatives. The results show that when the spoken input deviates from the predefined task grammar, a combination of weak a priori syntax rules in conjunction with full a posteriori parsing on a lattice of alternative word matches provides the most robust recognition performance.","['S.J. Young', 'N.H. Russell', 'J.H.S. Thornton']",January 1991,Computer Speech & Language,[],The use of syntax and multiple alternatives in the VODIS voice operated database inquiry system☆
208,This paper reviews the “Wizard of Oz” technique for simulating future interactive technology and develops a partial taxonomy of such simulations. The issues of particular relevance to Wizard of Oz simulations of speech input/output computer systems are discussed and some experimental variables and confounding factors are reviewed. A general Wizard of Oz methodology is suggested.,"['Norman M. Fraser', 'G.Nigel Gilbert']",January 1991,Computer Speech & Language,[],Simulating speech systems☆
210,"A shift-reduce parser for probabilistic context-free grammars is described, based on the LR algorithm. Each of the standard types of LR parser generator has a probabilistic version and a Bayesian interpretation is advanced. A graph-structured stack permits action conflicts and allows the parser to be used with uncertain input, typical of speech recognition applications. The sentence uncertainty is measured using entropy and is found to be significantly lower for the grammar than for a derived first-order Markov model.",['J.H. Wright'],October 1990,Computer Speech & Language,[],LR parsing of probabilistic grammars with input uncertainty for speech recognition☆
211,"This paper proposes a model of a spectrum target prediction mechanism and a preprocessing method for automatic speech recognition by using the model to cope with coarticulation. The model is constructed to predict particular spectra for each phoneme, that is phoneme target, and to keep their spectra constant in each phoneme interval. The method is evaluated by four measures: spectrum sequence stability—Are predicted spectrum sequence in each phoneme interval fixed?; intra-category spectrum variation—Is a variation of predicted spectra in each phoneme category small?; inter-category spectrum variation—Is phoneme category pair far apart measuring by the Mahalanobis distance?; and lengths of transitional sounds—How long is the duration of wrong recognized results in a phoneme interval. Experimental results indicate that predicted spectra throughout the model are stabilized in each phoneme interval. Moreover, by using the method, intra-category variation decreases and inter-category variation increases. The results also indicate that the model recovers vowel characteristics neutralized by coarticulation at the spectral transition portion and decreases the duration of transitional sounds. Consequently, the spectrum target prediction model implemented as a speech recognition preprocessor reduces recognition error rates.","['Masato Akagi', ""Yoh'ichi Tohkura""]",October 1990,Computer Speech & Language,[],Spectrum target prediction model and its application to speech recognition
212,"In this paper, we report our development of context-dependent allophonic hidden Markov models (HMMs) implemented in a 75 000-word speaker-dependent Gaussian-HMM recognizer. The context explored is the immediate left and/or right adjacent phoneme. To achieve reliable estimation of the model parameters, phonemes are grouped into classes based on their expected co-articulatory effects on neighboring phonemes. Only five separate preceding and following contexts are identified explicitly for each phoneme. By grouping the contexts we ensure that they occur frequently enough in the training data to allow reliable estimation of the parameters of the HMM representing the context-dependent units. Further improvement in the estimation reliability is obtained by tying the covariance matrices in the HMM output distributions across all contexts. Speech recognition experiments show that when a large amount of data (e.g. over 2500 words) is used to train context-dependent HMMs, the word recognition error rate is reduced by 33%, compared with the context-independent HMMs. For smaller amounts of training data the error reduction becomes less significant.","['L. Deng', 'M. Lennig', 'F. Seitz', 'P. Mermelstein']",October 1990,Computer Speech & Language,[],Large vocabulary word recognition using context-dependent allophonic hidden Markov models☆
213,"In natural speech, segmental duration depends on several factors, including phonemic identity, phonetic context, phrase boundaries, lexical stress, and speaking rate. These factors interact: the magnitude of the effect of a factor—whether measured in milliseconds or as percentage change—often depends on other factors. This paper introduces two data analysis methods for constructing a duration model that best describes a given body of multi-factorially dependent durations. The methods are illustrated with a single-speaker data base consisting of vowel durations measured at two speaking rates in contexts varying in phrasal location, stress, and other factors; text materials are two-word pseudo-phrases. Both methods analyse the structure of two-way rearrangements of the N-way data matrix, in which columns correspond to combinations of levels of k factors and rows to combinations of levels of the remaining N−k factors. The first method concerns models that express duration as a sum of additive and multiplicative terms (additive-multiplicative models), such as various versions of a model by Klatt (Journal of the Acoustical Society of America, 54(4), 1973). It determines which additive-multiplicative model best describes the data, by making use of the fact that a given additive-multiplicative model predicts for any two-way rearrangement whether the between-columns covariance matrix is either constant, multiplicative, or neither. The second method determines the general functional form that best describes the data, by testing for which two-way rearrangement the durations in each column are in the same numerical order (a property known as joint independence).","['Jan P.H. van Santen', 'Joseph P. Olive']",October 1990,Computer Speech & Language,[],The analysis of contextual effects on segmental duration☆
215,"This article describes a method for synthesizing Arabic speech by using segments (synthesis units) which are parts of syllables. The method has several advantages. It is inherently capable of dealing with most phonetic variations of the sounds of modern standard Arabic; it permits speech to be synthesized using simple concatenation rules and synthesis techniques such as direct waveform concatenation of the synthesis units; and it allows the realization of prosodic features on segments because the segments are derived from Arabic syllables.Most intersyllabic coarticulations cannot be treated directly by this method. This has prompted a phonetic study of Arabic speech following the present method; we have investigated contextual variations of Arabic sounds and related these variations to the synthesis units. This enabled us to define a small number of allophones of the synthesis units which when included in the text-to-speech transcription process improved the quality of the synthesized speech.The segmentation of the Arabic syllables into synthesis units, on both phonetic and parametric levels, is similar to standard demisyllabic segmentation used by other speech synthesis researchers. However, it differs in one important respect which we have investigated.Synthesized speech is produced by two synthesis techniques: time-domain waveform concatenation and linear predictive (LPC) techniques. Time-domain waveform concatenation is used because it is simple and easy to carry out and is attractive since the juncture points between any contiguous synthesis units are stable. It involves direct concatenation of the digital data that represent the synthesis units. LPC synthesis is used because it eases the treatment of certain distortions which are introduced by the synthesis process. It is a parametric method that allows certain suprasegmental aspects of speech to be modified. This results in memory saving because it allows us to compare speech produced by the two synthesis techniques.The types of distortion introduced by concatenating the synthesis units are studied. It is shown how the stability of the juncture points between contiguous units and the phonological properties of Arabic syllables and speech make it possible to avoid these problems. This can be achieved by using simple synthesis techniques such as direct waveform concatenation and shows how problems can be avoided by using a parametric synthesis technique such as LPC.To demonstrate the suitability of the present method for the synthesis of Arabic speech, the method has been implemented as a non-real-time text-to-speech system on a personal computer (PC) and the intelligibility of the synthesized speech has been established by conducting perception tests.",['Yousif A. El-Imam'],July 1990,Computer Speech & Language,[],Speech synthesis using partial syllables
216,"This paper addresses the issue of learning hidden Markov model (HMM) parameters for speaker-independent continuous speech recognition. Bahl et al. (IEEE conference on acoustics, speech and signal processing, April 1988a) introduced the corrective training algorithm for speaker-dependent isolated-word recognition. Their algorithm attempted to improve the recognition accuracy on the training data. In this work, we extend this algorithm to speaker-independent continjous speech recognition. We use cross-validation to increase the effective training size. We also introduce a near-miss sentence hypothesization algorithm for continuous speech training. The combination of these two approaches resulted in over 20% error reductions both with and without grammar.","['Kai-Fu Lee', 'Sanjoy Mahajan']",July 1990,Computer Speech & Language,[],Corrective and reinforcement learning for speaker-independent continuous speech recognition☆
217,"In this paper, a method is proposed for segmenting syllables and syllable classes in continuous speech. The implementation involves a combination of phonological knowledge, speaker adaptable features, vector quantization and a hidden Markov modelling technique. Phoneme classes are represented as hidden Markov model states while syllable related phonological knowledge defines the state connections. The observation sequence is distances obtained from the decision planes of two single layer neural networks, which are respectively used to make voiced/unvoiced and (syllabic) nucleus/non-nucleus classifications. Tests were performed on a (bandlimited FM broadcast speech quality) database, containing speech from 10 male speakers. Our results indicate that 97·1% of all syllables were detected and 96·5% phoneme classes were labelled correctly.","['G.J. Prinsloo', 'M.W. Coetzer']",July 1990,Computer Speech & Language,[],Automatic syllabification and phoneme class labelling with a phonologically based hidden Markov model and adaptive acoustical features
218,"Active chart parsing offers a clean and flexible way of implementing dynamic programming to find the best path through a-cylic directed lattices. This paper describes how this comes about and what the general form of a chart parsing realization of dynamic programming takes. Advantage is taken of the resulting flexibility to produce a system which not only finds the best path, but enumerates paths in order. Finally, we exemplify the process for finding paths through a word lattice given bi-class probabilities, and give a few results from some experiments.",['Henry Thompson'],July 1990,Computer Speech & Language,[],Best-first enumeration of paths through a lattice—an active chart parsing solution
219,This paper compares the performances of three non-linear pattern classifiers in the recognition of static speech patterns. Two of these classifiers are neural networks (Multi-layered perceptron and the modified Kanerva model). The third is the method of Radial Basis Functions. A review of several classification techniques similar to the method of radial basis functions is presented. The class boundaries generated by the different methods are compared on simple two-dimensional examples. Experiments on classifying eight vowels from a subset of the DARPA TIMIT database are reported.,"['Mahesan Niranjan', 'Frank Fallside']",July 1990,Computer Speech & Language,[],Neural networks and radial basis functions in classifying static speech patterns
223,"Linear neuromorphic systems assume that neurally encoded stimuli are mapped through linear combinations of a set of underlying features (represented as the eigenvectors of the matrix of learned associations) onto perceptual categories. Learning in this type of system may be through an auto-associative process that induces prototypes that represent the central tendency of each perceptual category. It has been proposed that human listeners learn vowel categories through just this type of computational mechanism. To investigate this claim, we trained an auto-associative network on a set of American English vowels averaged over talkers. We examined performance of this network for learning and classifying vowels produced by the average of tokens from several male talkers. In addition, we compared the effects of different auditory coding representations on recognition performance for the male vowels. Since, for these models it is necessary for the stimulus set to be linearly independent, the perceptual representation of the vowels can affect learning. Furthermore, we investigated the extent to which the underlying feature space learned from a set of vowels is shared between talkers. For this type of network to be a plausible model of vowel perception, it must be capable of perceptual constancy across talkers. Finally, we compared the effects of different learning algorithms on the development of vowel categories in perceptual space, and investigated the ability of a paired-associate model to develop vowel categories. We will discuss the results of these studies and their implications for simple, network models of speech perception and talker normalization.","['Todd M. Morin', 'Howard C. Nusbaum']",April 1990,Computer Speech & Language,[],Perceptual learning of vowels in a neuromorphic system☆
224,"The field of large vocabulary, continuous-speech recognition has advanced to the point where there are several systems capable of attaining between 90 and 95% word accuracy for speaker-independent recognition, of a 1000-word vocabulary, spoken fluently for a task with a perplexity (average word branching factor) of about 60. There are several factors which account for the high performance achieved by these systems, including the use of hidden Markov model (HMM) methodology, the use of context-dependent sub-word units, the representation of between-word phonemic variations, and the use of corrective training techniques to emphasize differences between acoustically similar words in the vocabulary. In this paper we describe one of the large vocabulary speech-recognition systems which is being investigated at AT&T Bell Laboratories, and discuss the methods used to provide high word-recognition accuracy. In particular we focus on the techniques used to provide the acoustic models of the sub-word units (both context-independent and context-dependent units), and discuss the resulting system performance as a function of the type of acoustic modeling used.","['C.H. Lee', 'L.R. Rabiner', 'R. Pieraccini', 'J.G. Wilpon']",April 1990,Computer Speech & Language,[],Acoustic modeling for large vocabulary speech recognition
225,"Learning techniques based on the Hidden Markov Model (HMM) assumption have proved efficient in various facets of speech analysis. The application of HMMs to speech unit modelling suffers, however, from one major deficiency; the implicit state occupancy distribution which is a geometric process is inadequate to model speech segment duration. To avoid this drawback, we propose to replace the underlying Markov chain by a semi-Markov chain, a more general framework where the state occupancy is explicitly modelled by an appropriate probability density function, in our case a gamma distribution.Due to particular dependency properties inside the semi-Markov chain, the direct adaptation of the Forward-Backward algorithm to the Hidden Semi-Markov Model (HSMM) assumption leads to rather complicated solutions for the training task. With the aim of simplification, we propose to adapt the Derin's scheme, originally developed in the field of image segmentation. One important characteristic of this scheme is its a-posteriori probability formalism which implicitly introduces a normalization at each step of the basic recursions. This allows us to solve in a rigorous and efficient manner the well known underflow problem in the training task.The re-estimation formulas for the HSMM parameters are derived according to a maximum likelihood criterion. Derin's algorithm for HSMMs is presented and a practical implementation is detailed. Results are given on a 130 isolated-word recognition task.","['Y. Guédon', 'C. Cocozza-Thivent']",April 1990,Computer Speech & Language,[],Explicit state occupancy modelling by hidden semi-Markov models: application of Derin's scheme☆
226,"It is not too difficult to select a fairly small (on the order of 20 000 words) fixed recognition vocabulary that will cover over 99% of new input words when the task is limited to text in a specific knowledge domain and when one disregards names and acronyms. Achieving such a level of coverage is much more difficult when restrictions on knowledge domain and names are lifted, however. This report describes how we selected a 75 000-word English recognition vocabulary that covers over 98% of words in new newspaper text, including names and acronyms. Observations collected during the vocabulary selection process indicate the limiting factors for coverage of general knowledge domain text such as newspaper stories.","['Philip F. Seitz', 'Vishwa N. Gupta', 'Matthew Lennig', 'Patrick Kenny', 'Li Deng', ""Douglas O'Shaughnessy"", 'Paul Mermelstein']",April 1990,Computer Speech & Language,[],A dictionary for a very large vocabulary word recognition system☆
228,"It is argued that the prosodic feature stress is useful in constraining the number of hypotheses a speech recognition system produces. A probabilistic algorithm is described for the estimation of the lexical stress pattern of English words from the acoustic signal using hidden Markov models (HMMs) with continuous asymmetric Gaussian probability density functions. Adopting binary (stressed or unstressed) syllable models, two five-state HMMs of the left-to-right type were generated, one for each value of the binary opposition. Training observation vectors were extracted from a corpus of bisyllabic stress-minimal word pairs, where each word occurred in a continuously spoken sentence. The vectors consisted of nine acoustic measurements based on fundamental frequency, syllabic energy and coarse linear prediction spectra. Evaluation of both stressed and unstressed models using a new set of recordings of the same word pairs yielded an average syllable-stress recognition rate of 94%.","['G.J. Freij', 'F. Fallside', 'C. Hoequist', 'F. Nolan']",January 1990,Computer Speech & Language,[],Lexical stress estimation and phonological knowledge☆
229,"The “reduced auditory representation”, or RAR, provides a compact, but comprehensive, description of basilar membrane displacement in a form which is both closely related to overall neural firing statistics and suitable for direct presentation to existing speech recognition algorithms.This paper gives a detailed description of the RAR analysis and defines a preliminary version of a “position-tolerant” distance measure, which could be used with most high resolution representations of speech. The results of recognition experiments are then presented, illustrating the dramatic improvements which can be obtained when the discrimination measure is matched to the data representation.",['S.W. Beet'],January 1990,Computer Speech & Language,[],Automatic speech recognition using a reduced auditory representation and position-tolerant discrimination
230,"Using an entropy argument, it is shown that stochastic context-free grammars (SCFG's) can model sources with hidden branching processes more efficiently than stochastic regular grammars (or equivalently HMM's). However, the automatic estimation of SCFG's using the Inside-Outside algorithm is limited in practice by its O(n3) complexity. In this paper, a novel pre-training algorithm is described which can give significant computational savings. Also, the need for controlling the way that non-terminals are allocated to hidden processes is discussed and a solution is presented in the form of a grammar minimization procedure.","['K. Lari', 'S.J. Young']",January 1990,Computer Speech & Language,[],The estimation of stochastic context-free grammars using the Inside-Outside algorithm
231,"This paper describes recent and future work on the sphinx speech recognition system at Carnegie Mellon. Sphinx is a large-vocabulary speaker-independent continuous speech recognizer based on discrete hidden Markov models. Sphinx uses multiple codebooks of LPC-derived features, and phonetic units that model function words and phrases, as well as within and between word generalized triphones. Sphinx is first trained with the Forward-Backward algorithm, and then with the corrective training algorithm. On a 997-word task, sphinx attained accuracies of 96% with a grammar (perplexity 60), and 82% without grammar (perplexity 997). This paper describes the sphinx System and its performance in detail, as well as outlines our future work.","['Kai-Fu Lee', 'Hsiao-Wuen Hon', 'Mei-Yuh Hwang', 'Sanjoy Mahajan']",January 1990,Computer Speech & Language,[],Recent progress and future outlook of the SPHINX speech recognition system
232,"This paper presents the basic phonological rhythm rule used in the intonation component of the Edinburgh University Centre for Speech Technology Research (CSTR) text-to-speech system, and shows how stress-shift phenomena can be incorporated into such a rule. The first part of the paper discusses the need for rhythm rules in speech synthesis and outlines the operation of the CSTR rule. The second part discusses the implications of such a rule for lexical stress and the applicability of its output to synthesis by rule.",['A.I.C. Monaghan'],January 1990,Computer Speech & Language,[],Rhythm and stress-shift in speech synthesis
233,"When computing linear prediction (LP) parameters of speech, large numbers of data are uninformative in a certain set-theoretic sense, and the expense of updating the estimates at these times can be avoided. “Set-membership” (SM) identification is formulated as a weighted recursive covariance LP problem with a special criterion for dynamic weight determination. An algorithm is developed which can be implemented on a systolic processor if desired, but which retains a simple interpretation as a specially weighted convariance LP method. The algorithm is applied to identification of the LP parameters of real speech data, and a number of practical issues are discussed. The potential for an adaptive strategy and other open research questions generated by the experimental work are discussed in the conclusions.","['John R. Deller', 'T.C. Luk']",October 1989,Computer Speech & Language,[],Linear prediction analysis of speech based on set-membership theory☆
234,This paper describes work performed as part of the U.K. Alvey sponsored Voice Operated Database Inquiry System (VODIS) project in the area of intelligent dialogue control. The principal aims of the work were to develop a habitable interface for the untrained user; to investigate the degree to which dialogue control can be used to compensate for deficiencies in recognition performance; and to examine the requirements on dialogue control for generating natural speech output. A data-driven methodology is described based on the use of frames in which dialogue topics are organized hierarchically. The concept of a dynamically adjustable scope is introduced to permit adaptation to recognizer performance and the use of historical and hierarchical contexts are described to facilitate the construction of contextually relevant output messages.,"['S.J. Young', 'C.E. Proctor']",October 1989,Computer Speech & Language,[],The design and implementation of dialogue control in voice operated database inquiry systems
235,"An approach to the problem of inter-speaker variability in automatic speech recognition is described which exploits systematic vowel differences in a two-stage process of adaptation to individual speaker characteristics. In stage one, an accent identification procedure selects one of four gross regional English accents on the basis of vowel quality differences within four calibration sentences. In stage two, an adjustment procedure shifts the regional reference vowel space onto the speaker's vowel space as calculated from the accent identification data. Results for 58 speakers from the four regional accent areas are presented.","['W.J. Barry', 'C.E. Hoequist', 'F.J. Nolan']",October 1989,Computer Speech & Language,[],An approach to the problem of regional accent in automatic speech recognition☆
236,"This paper explores the number of word boundaries which can be detected from sequences of phonemes and broad classes in continuous speech transcriptions. In the first part of the paper, word boundaries are detected from sequences of three phonemes which occur across word boundaries but which are excluded word internally. When such sequences are matched against phonemic transcriptions of 145 utterances, it is shown that around 37% of all word boundaries can be correctly identified. When the same transcriptions are represented by broad classes rather than phonemes, a knowledge of sequences which span word boundaries but which do not occur word internally is almost completely ineffective for the purpose of word boundary detection. Instead, it is shown that a version of the model discussed in Cutler & Norris 1988 based on the distinction between “strong” and “weak” vowels enables over 40% of word boundaries to be correctly located at the broad class level although many word boundaries are also inserted at inappropriate points. The implications of these kinds of word boundary detection strategies for models of lexical access in a continuous speech recognizer are also discussed.","['Jonathan Harrington', 'Gordon Watson', 'Maggie Cooper']",October 1989,Computer Speech & Language,[],Word boundary detection in broad class and phoneme strings☆
243,"A 560-unit neural network with two layers of modifiable connections was trained by means of back-propagation to disambiguate the syntactic categories of words in samples of text taken from the Brown Corpus. After training, the network was able to successfully disambiguate words in previously unanalyzed text with 95% accuracy, a performance level comparable to the best current computational techniques for the disambiguation of syntactic function. The model incorporates plausible psychological constraints on its input and output representations, and exhibited human-like behavior during parts of the learning process. The network's success suggests that syntactic category disambiguation may be mainly a low-level, bottom-up process with little dependence on the recognition of higher-level syntactic structures. Although the network simulates only a restricted component of the human language processing mechanism, its intrinsic ability to use partially formed data should allow it to be easily integrated into a full-scale language comprehension system. The model's overall performance level, along with its psychological plausibility, indicates that neural networks may be a new and useful approach to building human language processing models.","['Julian Benello', 'Andrew W. Mackie', 'James A. Anderson']",July 1989,Computer Speech & Language,[],Syntactic category disambiguation with neural networks
244,"The use of “high level” knowledge sources in recognizing continuous speech is aimed at reducing the hypothesis space generated by acoustic-phonetic analysis. In this, a sentence parser can be a basic resource, provided that it can deal with the ambiguity of the input and with the fact that fragments may have been recognized even hypothetically. One of the most successful techniques for parsing natural language is chart parsing. Chart parsing is directional in the sense that it works from a starting point (usually the beginning of the sentence) and usually proceeds to the right. We describe the concept of a chart that works outward from islands (reliably recognized fragments), makes sense of as much of the sentence as possible, and then goes on to make predictions about missing fragments.","['Oliviero Stock', 'Rino Falcone', 'Patrizia Insinnamo']",July 1989,Computer Speech & Language,[],Bidirectional charts: a potential technique for parsing spoken natural language sentences
245,"A semi-continuous hidden Markov model, which can be considered as a special form of continuous mixture hidden Markov model with the continuous output probability density functions sharing in a mixture Gaussian density codebook, is proposed in this paper. The semicontinuous output probability density function is represented by a combination of the discrete output probabilities of the model and the continuous Gaussian density functions of a mixture Gaussian density codebook. The amount of training data required, as well as the computational complexity of the semi-continuous hidden Markov model, can be significantly reduced in comparison with the continuous mixture hidden Markov model. Parameters of the vector quantization codebook and hidden Markov model can be mutually optimized to achieve an optimal model/codebook combination, which leads to a unified modelling approach to vector quantization and hidden Markov modelling of speech signals. Experimental results are included which show that the recognition accuracy of the semi-continuous hidden Markov model is measurably higher than both the discrete and the continuous hidden Markov model.","['X.D. Huang', 'M.A. Jack']",July 1989,Computer Speech & Language,[],Semi-continuous hidden Markov models for speech signals
246,"This paper presents a method for the recognition of lexical tones in Mandarin speech based on vector quantization and hidden Markov models. A Kay Visi-Pitch 6087DS is used to extract the fundamental frequency (F0) contour. The features for the recognition of lexical tones are derived from an F0 contour within the voiced part of a syllable. Markov models are generated for the four monosyllabic tones and for 15 disyllabic tone pairs based on these derived features. A Viterbi algorithm is applied to find the probability scores of a test tone scored by the models during the recognition phase. In speaker-independent tone recognition experiments, the average recognition rate was 97·9% for isolated monosyllabic words, 92·9% for disyllabic words, and 91·0% for trisyllabic words. The decline comes from the tone variation and the coarticulation between syllables.","['Lih-Cherng Liu', 'Wu-Ji Yang', 'Hsiao-Chuan Wang', 'Yueh-Chin Chang']",July 1989,Computer Speech & Language,[],Tone recognition of polysyllabic words in Mandarin speech
247,"Recent studies have suggested that recognition systems should concentrate their efforts on the identification of stressed syllables, as they contain disproportionately more information than do unstressed syllables. The paper investigates whether this increased informativeness may be outweighed by the informational disadvantage associated with transcribing consecutive segments within the same syllable. Phonotactic correlations between such adjacent segments suggest that the most informative transcription of a polysyllabic word may be one where reliable phonemic information is scattered across different syllables. Lexical statistics are presented which support this view. In addition, the paper considers the reasons for the increased informativeness of stressed syllables, and shows that this is because lexical stress preserves vowel distinctions (and hence information) which would otherwise be lost in lexically unstressed syllables.","['Gerry Altman', 'David Carter']",July 1989,Computer Speech & Language,[],"Lexical stress and lexical discriminability: Stressed syllables are more informative, but why?"
248,"This paper proposes a new continuous-speech recognition method by phoneme-based word spotting and time-synchronous context-free parsing. The word pattern is composed of the concatenation of phoneme patterns. The knowledge of syntax is given in Backus normal form. Therefore, our method is task-independent in terms of reference patterns and task language.The system first spots word candidates in an input sentence, and then generates a word lattice. The word spotting is performed by a dynamic time-warping method. Secondly, it selects the best word sequences found in the word lattice from all possible sentences which are defined by a context-free grammar.We propose two time-synchronous context-free parsing algorithms. One is time-synchronous in terms of the ending times of spotted words. This is an extension of the augmented continuous DP algorithm proposed by the author. The other is time-synchronous in terms of the ending times of generated partial sentences. The latter algorithm predicts the words following a partial sentence and then concatenates a spotted word found in the set of predicted words and the partial sentence.We applied the above algorithms to the continuous speech of English sentences which were related to “electronic mail” and confirmed that it worked well.",['Sei-ichi Nakagawa'],July 1989,Computer Speech & Language,[],Speaker-independent continuous-speech recognition by phoneme-based word spotting and time-synchronous context-free parsing☆
249,"This paper describes a digital filter simulation of basilar membrane vibration, in which the cochlea is represented as a cascade of 128 filters. The parameters of each filter are determined from the mechanical characteristics of the basilar membrane at corresponding points. Using this model, it is possible to simulate, at each point, the waveform of the sound pressure in the cochlear fluid, as well as the deflection of the membrane itself. A significant advantage of this practical model is that the computation time is considerably shortened in comparison with other similar models and it thus has application as a front-end processor in speech recognition systems.This work can be used as a basis for developing a cochlear model which includes active elements in the cochlea in line with the most recent findings in this field.","['Eliathamby Ambikairajah', 'Norman D. Black', 'Robert Linggard']",April 1989,Computer Speech & Language,[],Digital filter simulation of the basilar membrane
250,"A back-propagation network is investigated as a new approach to the automatic discrimination of spoken minimally distinct word pairs. The choice of the parameters for the network is discussed and experimental results are reported. A comparison is made with hidden Markov modelling applied to the same data. The results, for this particular task, show that the discrimination accuracy obtained using a particular back-propagation network is superior to that attained using hidden Markov modelling.","['R.K. Moore', 'S.M. Peeling']",April 1989,Computer Speech & Language,[],Minimally distinct word-pair discrimination using a back-propagation network
251,"This paper presents an approach to text-independent speaker identification by short utterances. In the training stage, in order to utilize both phoneme-dependent and phoneme-independent speaker information and also to reduce the variation due to phonemes, discriminant analysis is applied to multiple subspaces in an observation space. In each resultant speaker discriminant space, each speaker is characterized by a Gaussian probability density function. The decision is based on a likelihood measure derived from the single subspace to which an unknown vector is automatically assigned by a minimum distance classification rule on a phoneme discriminant space. The observation vectors consist of the 20 cepstrum coefficients and pitch frequency obtained from every 40 ms voiced segment. The choice of subspaces as well as several likelihood measures for a single and multiple vectors are examined through speaker identification for a 10-speaker population. The use of eight extensively overlapped subspaces attains high accuracy for short utterances with a relatively small training data, providing 90% for 0·5 s to 100% for 1·4 s of voiced speech even for test data recorded six months after training.",['Hiroshi Matsumoto'],April 1989,Computer Speech & Language,[],Text-independent speaker identification from short utterances based on piecewise discriminant analysis☆
252,"Several ways for making the signal processing in an isolated word speech recognition system more robust against large variations in the background noise level are presented. Isolated word recognition systems are sensitive to accurate silence detection, and are easily overtrained on the specific noise circumstances of the training environment. Spectral subtraction provides good noise immunity in the cases where the noise level is lower or slightly higher in the testing environment than during training. Differences in residual noise energy after spectral subtraction between a clean training and noisy testing environment can still cause severe problems. The usability of spectral subtraction is largely increased if complemented with some extra noise immunity processing. This is achieved by the addition of artificial noise after spectral subtraction or by adaptively re-estimating the noise statistics during a training session. Both techniques are almost equally successful in dealing with the noise. Noise addition achieves the additional robustness that the system will never be allowed to learn about low amplitude events that might not be observable in all environments; this is achieved, however, at a cost that some information is consistently thrown away in the most favorable noise situations.",['Dirk Van Compernolle'],April 1989,Computer Speech & Language,[],Noise adaptation in a hidden Markov model speech recognition system
253,"Connected-word recognition schemes have been successfully applied to a number of speech applications of moderate size and their extension to large vocabulary problems is appealing. Unfortunately, this generalization is not straightforward. Recently, classical schemes for connected-word recognition have been extended to deal with large vocabulary tasks. This paper describes three dynamic time warping-based search procedures for connected-word recognition. These are the one-pass (OP), the level-building (LB), and the two-level algorithms. The latter two procedures are formulated in a frame-synchronous, OP-like data-flow process. This reformulation allows one to generalize algorithm-specific developments to all schemes. Moreover, a natural outcome has been to introduce a beam in the LB process which renders its complexity independent of the number of units in the input string. Finally, the OP and LB processes are shown to be identical when syntactic constraints are applied. This leads to a unified interpretation of both algorithms.","['C. Godin', 'P. Lockwood']",April 1989,Computer Speech & Language,[],DTW schemes for continuous speech recognition: a unified view☆
256,"A fundamental challenge in speech recognition is the discrimination of the acoustic vectors characterizing each speech time-slot into classes of sub-word units. The most popular technique relies on hidden Markov models (HMM) but alternative models could make use of linear or non-linear discriminant functions and also Multilayer Perceptrons (MLP).A common property of these techniques is their learning ability. However, the advantage of discriminant functions and multilayer perceptrons is their possibility to force simultaneously the acceptance of a vector into its own class and its rejection by the rival classes. Ideally, this classification relies on a logical decision with a score invariant for any vector in a given class. Generally, classes are not linearly separable. As linear discriminant functions are not flexible enough to attain both objectives, suitable non-linear discriminant functions could approximate logical decision and achieve non-linear separation. For that purpose, multilayer perceptrons are particularly powerful tools because of their possibility to approximate a very wide range of non-linear functions with a rather restricted set of parameters. The main properties of MLP are pointed out and illustrated by simple examples.The possibility of including discriminant principles in a Dynamic Time Warping process and in a Viterbi-like training is shown.The discriminant properties of HMM, linear discriminant functions and MLP are compared from the point of view of local labeling of the acoustic vectors and of the global recognition.Finally, a phoneme based real task application (50 phonemes, 1000 vocabulary words) is described. It makes use of a particular MLP, based on the NETtalk architecture, and shows how the non-linear discriminant functions and the consideration of the temporal context dependence of the acoustic vectors are useful for the phonetic speech labeling.","['H. Bourlard', 'C.J. Wellekens']",January 1989,Computer Speech & Language,[],Speech pattern discrimination and multilayer perceptrons
257,"Neural networks were trained to classify single 20 ms frames of vowels using either perceptually-based spectral representations or LPC spectra as input. Classification performance was compared with performance of several distance measures using nearest-neighbor and mean-distance decision criteria. The non-network distance measures included LPC-residual and cepstral distance measures used in conventional automatic speech recognition systems, as well as a formant-based measure and a new elastic distance measure that explicitly corrects for the effects of spectral tilt.Using an optimal error rate criterion, vowels were discriminated best using the elastic distance measure with the perceptually-based spectrum. Neural networks with LPC spectra as input performed comparably to the better conventional distance measures. While the performance of networks trained with perceptually-based spectral inputs was poorer than that of networks trained with LPC spectra, the features represented by the hidden nodes of this network were more consistent with factors related to human vowel perception.","['Candace A. Kamm', 'Lynn A. Streeter', 'Yana Kane-Esrig', 'David J. Burr']",January 1989,Computer Speech & Language,[],Comparing performance of spectral distance measures and neural network methods for vowel recognition
258,"Recently, hidden Markov models (HMM) have been applied successfully to both isolated and connected word recognition. However, when the same formulation is adopted for recognition of more confusable vocabularies, like English alphabets, the recognition performance is often less satisfactory. One main reason is that robustness issues, such as model validity, recognition parameter selection, model parameter initialization, model parameter estimation, training sample size, and durational information incorporation, can no longer be ignored. In this paper, a stochastic segment model (SSM) is proposed, which is a simplified HMM, for speech recognition. Three specific robustness issues are then discussed, namely the choice of observation densities, the initialization of model parameters, and the incorporation of duration information. In a step-by-step attempt to address those issues, it was found that the same SSM formulation can still be adopted if acoustic and phonetic knowledge about the vocabulary is taken into account in the model parameter estimation and recognition phases. Testing on the 39-word English alpha-digit vocabulary indicates that the recognition performance, based on conventional HMM techniques, can be signficantly improved if model parameters are adequately initialized and durational information is properly incorporated.",['Chin-Hui Lee'],January 1989,Computer Speech & Language,[],On the use of some robust modeling techniques for speech recognition
259,"The segmental intelligibility of two speech coding methods was evaluated using the modified rhyme test (MRT). In order to investigate how language knowledge and experience affects the perception of coded speech, we used both native and non-native speakers of English as listeners. We used 8 kb/s pitch predictive multi-pulse coding (MPC) and 50 kb/s μ-law PCM as coding methods, along with 120 kb/s unprocessed speech as a control condition. The results indicated that for native listeners, the intelligibility of unprocessed speech was best, followed by PCM and then MPC speech. However, for non-native listeners, the intelligibility of the two types of coded speech was much worse than the unprocessed speech when compared with the performance of the native English listeners. The difference in error rates between MPC and PCM was not significant for non-native listeners. In contrast, for native listeners, the error rate for PCM was significantly less than for MPC speech. Non-native listeners also had a tendency to confuse stops and fricatives more than native listeners, especially in the coded speech. These results suggest that language knowledge and experience play a much more important role in the perception of coded speech than in the perception of unprocessed speech. Furthermore, non-native listeners appear to be much more affected by some of the unique characteristics of coded speech than native listeners. The results have implications for the design and improvement of low bit-rate speech coding methods and for the development of new techniques for perceptual evaluation.","['Kazanori Ozawa', 'John S. Logan']",January 1989,Computer Speech & Language,[],Perceptual evaluation of two speech coding methods by native and non-native speakers of English☆
260,"A parallel processing network derived from Kanerva's associative memory theory Kanerva 1984 is shown to be able to train rapidly on connected speech data and recognize further speech data with a label error rate of 0·68%. This modified Kanerva model can be trained substantially faster than other networks with comparable pattern discrimination properties.Kanerva presented his theory of a self-propagating search in 1984, and showed theoretically that large-scale versions of his model would have powerful pattern matching properties. This paper describes how the design for the modified Kanerva model is derived from Kanerva's original theory. Several designs are tested to discover which form may be implemented fastest while still maintaining versatile recognition performance. A method is developed to deal with the time varying nature of the speech signal by recognizing static patterns together with a fixed quantity of contextual information.In order to recognize speech features in different contexts it is necessary for a network to be able to model disjoint pattern classes. This type of modelling cannot be performed by a single layer of links. Network research was once held back by the inability of single-layer networks to solve this sort of problem, and the lack of a training algorithm for multi-layer networks.Rumelhart, Hinton & Williams 1985 provided one solution by demonstrating the “back propagation” training algorithm for multi-layer networks. A second alternative is used in the modified Kanerva model. A non-linear fixed transformation maps the pattern space into a space of higher dimensionality in which the speech features are linearly separable. A single-layer network may then be used to perform the recognition. The advantage of this solution over the other using multi-layer networks lies in the greater power and speed of the single-layer network training algorithm.","['R.W. Prager', 'F. Fallside']",January 1989,Computer Speech & Language,[],The modified Kanerva model for automatic speech recognition
261,"Linguistic re-write rules are very popular in phonology, as they are well suited to describe phonological processes. These rules can also well be used for string manipulation such as grapheme-to-phoneme (spelling-to-sound) conversion, which is needed in text-to-speech systems.In this paper a tool is presented with which one can develop and test a set of linguistic rules which, together, define a scheme to convert an input string to an output string. One possible application is the design of a grapheme-to-phoneme conversion system.The system is approached from the point of view of linguists, since they are the main users of such a system. The linguist has to specify conversion data, which together with the development system form a conversion system. First the basic system is discussed, viz. the format in which the linguist must present the information, followed by a discussion of how the system supports the development of linguistic rules.A special characteristic of the system is that input-to-output relations are being preserved. Given a rulebase which defines a grapheme-to-phoneme conversion, the system can be used as an analysis tool for statistics on grapheme-to-phoneme relations.The paper is concluded with the discussion of some additional characteristics of the system, which are compared to those of some other systems, and a survey of the applications in which it is used.",['Hugo C. van Leeuwen'],January 1989,Computer Speech & Language,[],A development tool for linguistic rules
262,"Studies of human speech processing have provided evidece for a segmentation strategy in the perception of continuous speech, whereby a word boundary is postulated, and a lexical access procedure initiated, at each metrically strong syllable. The likely success of this strategy was here estimated against the characteristics of the English vocabulary. Two computerized dictionaries were found to list approximately three times as many words beginning with strong syllables (i.e. syllables containing a full vowel) as beginning with weak syllables (i.e. syllables containing a reduced vowel). Consideration of frequency of lexical word occurrence reveals that words beginning with strong syllables occur on average more often than words beginning with weak syllables. Together, these findings motivate an estimate for everyday speech recognition that approximately 85% of lexical words (i.e. excluding function words) will begin with strong syllables. This estimate was tested against a corpus of 190 000 words of spontaneous British English conversion. In this corpus, 90% of lexical words were found to begin with strong syllables. This suggests that a strategy of postulating word boundaries at the onset of strong syllables would have a high success rate in that few actual lexical word onsets would be missed.","['Anne Cutler', 'David M. Carter']",September–December 1987,Computer Speech & Language,[],The predominance of strong initial syllables in the English vocabulary☆
263,"A vector quantization based talker recognition system is described and evaluated. The system is based on constructing highly efficient short-term spectral representations of individual talkers using vector quantization codebook construction techniques. Although the approach is intrinsically text-independent, the system can be easily extended to text-dependent operation for improved performance and security by encoding specified training word utterances to form word prototypes. The system was evaluated using a 100-talker database of 20 000 digits spoken in isolation. In a talker verification mode, average equal-error rate performance of 2·2% for text-independent operation and 0·3% for text-dependent operation was obtained for 7-digit long test utterances. Because the evaluation database was restricted to the vocabulary of spoken digits, the text-independent operation of the system has not been formally tested beyond the confines of that vocabulary.","['A.E. Rosenberg', 'F.K. Soong']",September–December 1987,Computer Speech & Language,[],Evaluation of a vector quantization talker recognition system in text independent and text dependent modes
264,"This paper describes a parser for processing spontaneous spoken utterances which deals with both dislocation and “noise” phenomena. The parser is built upon a timetable corpus collected in an information center of the SNCF (French railway company). With spontaneous speech we consider that the only relevant problems are those that have been found in a corpus. Our corpus has been recorded in a human/machine situation and in a human/human situation in order to observe how the linguistic behavior provoked by the machine, changes automatic processing.As this work deals with cognitive levels, ALORS processes a keyboard input. Using a skimming strategy, it takes into account some aspects of speech recognition, and we evaluate its compatibility with word spotting speech recognition.",['D. Luzzati'],September–December 1987,Computer Speech & Language,[],ALORS: a skimming parser for spontaneous speech processing
266,"Hidden Markov Models are used in an experiment to investigate how state occupancy corresponds to prosodic parameters and spectral balance. In order to define separate sub-classes in the data using a maximum likelihood approach, modelling was performed using a single model where individual states correspond to different categories without assuming the structure of the data, rather than manually segmenting the data and modelling each predefined category separately.The results indicate a significant content of segmental information in the prosodic parameters, but the results based on the time-alignment of the model states with the feature vectors are in a form which is not directly usable in a recognition environment. The classification of various phonetic categories is particularly consistent for vowels and nasals and is generally better for voiced than unvoiced speech. The classification is also robust to influences of segmental effects on the data, with consistent alignments with segments regardless of the type of neighbouring phenemes.","['Andrej Ljolje', 'Frank Fallside']",September–December 1987,Computer Speech & Language,[],Modelling of speech using primarily prosodic parameters
267,"The effect of the coefficients used in the conventional back propagation algorithm on training connectionist models is discussed, using a vowel recognition task in speech processing as an example. Some weaknesses of the use of fixed coefficients are described and an adaptive algorithm using variable coefficients is presented. This is found to be efficient and robust in comparison with the fixed parameter case, to give fast near optimal training and to avoid trial and error choice of fixed coefficients. It has also been successfully used in a vision processing application.","['L.-W. Chan', 'F. Fallside']",September–December 1987,Computer Speech & Language,[],An adaptive training algorithm for back propagation networks
268,"The acoustic modeling problem in automatic speech recognition is examined from an information-theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is broken down into two steps: a signal-processing step which converts a speech waveform into a sequence of information-bearing acoustic feature vectors, and a step which models such a sequence. We are primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space. We explore the trade-off between packing information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous-parameter sequences is addressed by investigating a method of parameter estimation which is designed to cope with inaccurate modeling assumptions.","['Lalit R. Bahl', 'Peter F. Brown', 'Peter V. de Souza', 'Robert L. Mercer']",September–December 1987,Computer Speech & Language,[],Speech recognition with continuous-parameter hidden Markov models
269,"A technique for the assignment of word stress to isolated British English words is described. The input form of the word contains both morphological and phonological information. The input phonological form is more abstract than the normal phonemic transcription. All vowels are shown in unreduced form, allowing the stress rules to condition the subsequent reduction. The morphological analysis is based on a simple set of morpheme types.The input string is parsed into stress foot units. The stress foot units form the basis for the assignment of primary and secondary word stress. Unstressed vowels are reduced and certain other vowel modifications are performed.The results of testing the technique are compared with the performance of an affix-based method. The technique accounts for slightly more data than this taxonomic approach; this is because the latter uses a morphological shorthand to encode what are essentially phonological facts. The performance of the technique is also compared with that of two further methods and is found to be more satisfactory on both the theoretical and the practical levels.",['Briony Williams'],September–December 1987,Computer Speech & Language,[],Word stress assignment in a text-to-speech synthesis system for British English
270,"This study assesses the effect of using different phonological units on parsing a given string of phonemes into words in a continuous speech recognizer. It is shown that when an input utterance is encoded using a representation intermediate between the broad-classes, in Huttenlocher & Zue 1984, and the 44 phonemes of Received Pronunciation, the input can be parsed into more than 10 million different word-strings; and that even when all 44 phonemes are implemented, some input utterances can still be parsed into in excess of 10 000 different word-strings. The results suggest that there is insufficient information in a mid-class representation for post-lexical processing to identify the target word-string.","['Jonathan Harrington', 'Anne Johnstone']",September–December 1987,Computer Speech & Language,[],The effects of equivalence classes on parsing phonemes into words in continuous speech recognition
271,"The subject of artificial neural networks has attracted great interest recently. One network which has aroused particular interest is the error back propagation network, because of its ability to learn a mapping between two sets of patterns. Sejnowski & Rosenberg used a back propagation network to learn a mapping between (American) English text and its phonetic transcription Sejnowski & Rosenberg 1987. They called their network NETtalk, and demonstrated it as part of a text-to-speech system. This report describes work done at the Research Initiative in Pattern Recognition on a local re-implementation of NETtalk. It also describes some extensions to NETtalk and makes some suggestions for further possible improvements.","['Neil McCulloch', 'Mark Bedworth', 'John Bridle']",September–December 1987,Computer Speech & Language,[],NETspeak — A re-implementation of NETtalk
272,"A sentence verification task (SVT) was used to study the effects of sentence predictability on comprehension of natural speech and synthetic speech that was controlled for intelligibility. Sentences generated using synthetic speech were matched on intelligibility with natural speech using results obtained from a separate sentence transcription task. In the main experiment, the sentence verification task included both true and false sentences that varied in predictability. Results showed differences in verification speed between natural and synthetic sentences, despite the fact that these materials were equated for intelligibility. This finding suggests that the differences in perception and comprehension between natural and synthetic speech go beyond segmental intelligibility as measured by transcription accuracy. The observed differences in response times appear to be related to the cognitive processes involved in understanding and verifying the truth value of short sentences. Reliable effects of predictability on error rates and response latencies were also observed. High-predictability sentences displayed lower error rates and faster response times than low-predictability sentences. However, predictability did not have differential effects on the processing of synthetic speech as expected. The results demonstrate the need to develop new measures of sentence comprehension that can be used to study speech communication at processing levels above and beyond those indexed through transcription tasks, or forced-choice intelligibility tests such as the Modified Rhyme Test (MRT) or the Diagnostic Rhyme Test (DRT).","['David B. Pisoni', 'Laura M. Manous', 'Michael J. Dedina']",September–December 1987,Computer Speech & Language,[],Comprehension of natural and synthetic speech: effects of predictability on the verification of sentences controlled for intelligibility☆
273,"Accurate detection of the boundaries of a speech utterance during a recording interval has been shown to be crucial for reliable and robust automatic speech recognition. The endpoint detection problem is fairly straightforward for high-level speech signals spoken in low-level stationary noise environments (e.g. signal-to-noise ratios greater than 30 dB). However, these ideal conditions do not always exist. One example, where reliable word detection is difficult, is speech spoken in a mobile environment. Because of road, tire, fan noises, etc. detection of speech often becomes problematic.Currently, most endpoint detection algorithms use only signal energy and duration information to perform the endpoint detection task. These algorithms perform quite well with reasonable signal-to-noise ratios. However, under the harshest of conditions (e.g. in a car travelling at 60 mph with the fan on high) these algorithms begin to fail.In this paper, an endpoint detection algorithm is presented which is based on hidden Markov model (HMM) technology. The algorithm explicitly determines a set of speech endpoints based on the output of a Viterbi decoding algorithm. This algorithm was tested using a template-based speech recognition system and also using an HMM based system.Based on a speaker dependent speech database from four talkers, recorded in a mobile environment under five different driving conditions (including traveling at 60 mph with the fan on), we tested several endpoint detection schemes. The results showed that, under some conditions, the HMM-based approach to endpoint detection performed significantly better than the energy-based system. The overall accuracy of the system using the HMM endpoint detector, when trained with clean inputs and when tested on the 11 word digits vocabulary (zero through nine and oh) with speech recorded in various mobile environments, was 99.7%. The equivalent accuracy of the energy based endpoint detector was 95.2% in a template based recognizer.","['J.G. Wilpon', 'L.R. Rabiner']",September–December 1987,Computer Speech & Language,[],Application of hidden Markov models to automatic speech endpoint detection
274,"The performance of isolated word speech recognition system has steadily improved over time as we learn more about how to represent the significant events in speech, and how to capture these events via appropriate analysis procedures and training algorithms. In particular, algorithms based on both template matching (via dynamic time warping (DTW) procedures) and hidden Markov models (HMMs) have been developed which yield high accuracy on several standard vocabularies, including the 10 digits (zero to nine) and the set of 26 letters of the English alphabet (A-Z). Results are given showing currently attainable performance of a laboratory system for both template-based (DTW) and HMM-based recognizers, operating in both speaker trained and speaker independent modes, on the digits and the alphabet vocabularies using telephone recordings. We show that the average error rates of these systems, on standard vocabularies, are significantly lower than those reported several years back on the exact same databases, thereby reflecting the progress which has been made in all aspects of the speech recognition process.","['L.R. Rabiner', 'J.G. Wilpon']",September–December 1987,Computer Speech & Language,[],Some performance benchmarks for isolated work speech recognition systems
280,"A spectrogram is the normal display for visual interpretation of the speech signal and, in one form or another, spectrographic data are very often used as the feature space for speech recognition. It is generally accepted that normal sampling rates and window sizes may be inappropriate for good spectrography given time-varying speech. We conjecture that rapid time variation is much more insidious than that; a spectrogram can indicate formant tracks which totally belie the vocal-tract generation process. It is shown that increased bandwidth due to rapid time variation can mask an expected, instantaneous spectral representation; current spectral analyses are very likely to provide inconsistent information for accurate classification of rapidly time-varying events such as stops.","['Harvey F. Silverman', 'Yi-Teh Lee']",June 1987,Computer Speech & Language,[],On the spectrographic representation of rapidly time-varying speech
281,"Although performance data are often freely cited by vendors of speech recognition devices, the conditions under which these data were collected are seldom specified in sufficient detail to permit comparisons among different systems. To directly compare the performance of six commercially available speech recognition devices, we developed a computer-controlled testing system and a set of standard tests. We carried out these tests to assess the performance of speech recognition devices sold by Texas Instruments, Votan, Dragon, IBM, Interstate, and NEC. The results demonstrate several reliable performance differences among these systems. In general, however, performance differences among these devices are quite small and are reduced by appropriate training. Our results also indicate that the effects of training on performance of a speech recognition device are much greater for difficult vocabularies than they are for discriminable vocabularies. Finally, an examination of the results for recognition of the speech produced by one talker in the testing database suggests that user-specific difficulties in recognition performance may, in some cases, result from interactions among the application vocabulary, the user's speech, and the algorithm of a recognition device.","['Howard C. Nusbaum', 'David B. Pisoni']",June 1987,Computer Speech & Language,[],Automatic measurement of speech recognition performance: a comparison of six speaker-dependent recognition devices☆
282,"Research into the automatic recognition of anaphoric features in English is illustrated by an examination of the pronoun “it”. Various non-referential usages of “it” are described, and simple surface rules are proposed for identifying them. A computer program applies these rules to a text and classifies all cases of “it” according to mode of usage. Runs were performed on the Technical Section of the Lancaster-Oslo/Bergen Corpus of printed English; this was divided into a development section, which was used for refining the rules, and a test section. The final success rate for distinguishing referential from non-referential “it” was 95·8% for the development section and 92·2% for the test section. The likely performance with further unseen text corpora, and the general utility of the approach, are discussed.","['C.D. Paice', 'G.D. Husk']",June 1987,Computer Speech & Language,[],Towards the automatic recognition of anaphoric features in English text: the impersonal pronoun “it”
284,"Recent studies of English vocabulary have suggested that much of the linguistic content of the speech signal resides in stressed syllables and in broad phonetic classes corresponding to manner of articulation, both of which are comparatively easy to recognize. The implication is drawn that a promising strategy for speech recognition is to concentrate initially on these aspects of the signal, using phonotactic, lexical and (if available) higher level constraints to reduce the need for more detailed analysis. This paper argues that the evaluation criteria used to date in such studies are inappropriate, and, using a more appropriate information-theoretic approach, shows, by repeating a representative experiment, that many of the resulting claims are misleading and that there is in fact no reason to expect a recognition strategy of the type suggested to be particularly fruitful.",['David M. Carter'],March 1987,Computer Speech & Language,[],An information-theoretic analysis of phonetic dictionary access☆
285,"A speaker-independent word-recognition system has been developed using multiple classification functions for separating 100 spoken words. The speech signal is first analysed and then non-uniformly time-sampled by referring to word-structure tables to construct a word pattern vector of 120 dimensions. Equivalently piece-wise quadratic classification functions are calculated based on a linear-programming algorithm using a large number of spoken-word design samples. A hardware system for real-time recognition has been built as a high-speed microprocessor complex. Using the classification functions calculated from design samples of 100 speakers, a recognition rate of 99% has been obtained for 50 unknown speakers.","['Seibi Chiba', 'Masao Watari', 'Takao Watanabe']",March 1987,Computer Speech & Language,[],A speaker-independent word-recognition system using multiple classification functions☆
286,"The use of Hidden Markov Models with continuous asymmetric probability density functions for modelling prosodic patterns in isolated utterances is described. Finite Gaussian mixtures were used in each state of the models which describe fundamental frequency, fundamental frequency time derivative, energy and smoothed energy. Six models were generated using recordings of 16 monosyllabic words spoken by five speakers per model representing four basic intonational features: falls, rises, fall rises and rise falls. Falls and rises were each described by two models, one for high and the other for low pitch in the speaker's natural pitch range. The recognition results based on these models clearly show the ability of Hidden Markov Models to model some aspects of the underlying prosodic structure.","['Andrej Ljolje', 'Frank Fallside']",March 1987,Computer Speech & Language,[],Recognition of isolated prosodic patterns using Hidden Markov Models
287,"A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in “weight space”. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.","['David C. Plaut', 'Geoffrey E. Hinton']",March 1987,Computer Speech & Language,[],Learning sets of filters using back-propagation☆
288,"This paper describes various speaker normalization and adaptation techniques of a knowledge data base or reference templates to new speakers in automatic speech recognition (ASR). It focuses on a technique for learning spectral transformations, based on a statistical-analysis tool (canonical correlation analysis), to adapt a standard dictionary to arbitrary speakers. The proposed method should permit to improve speaker independence in large vocabulary ASR. Application to an isolated word recognizer improved a 70% correct score to 87%.A dynamic aspect of the speaker adaptation procedure is introduced and evaluated in a particular strategy.","['K. Choukri', 'G. Chollet']",December 1986,Computer Speech & Language,[],Adaptation of automatic speech recognizers to new speakers using canonical correlation analysis techniques*
289,"We describe here a computational model based upon the temporal characteristics of the information in the auditory nerve-fiber firing patterns. The model produces a frequency domain representation of the input signal in terms of the ensemble histogram of the inverse of the interspike intervals, measured from firing patterns generated by a simulated nerve-fiber array. The nerve-fiber mechanism is modeled by a multi-level-crossing detector at the output of each cochlear filter. We use 85 cochlear filters, equally spaced on a log-frequency scale from 200 Hz to 3200 Hz, and the level crossings are measured at positive threshold levels which are uniformly distributed in log scale. The resulting Ensemble Interval Histogram (EIH) pseudo spectrum shares two main properties: (1) fine spectral details are well preserved in the low-frequency region but become fuzzy at the high-frequency end; (2) the EIH spectrum is more robust in noise, compared with the traditional Fourier spectrum. This representation of the speech has been used as a front-end to a Dynamic Time Warping (DTW), speaker-dependent, isolated word recognizer. The database consisted of a 39-word alpha-digits vocabulary spoken by two male and two female speakers, in different levels of additive white noise. In the noise-free case, the performance of the EIH-based front-end is comparable to a conventional Fourier Transform (FFT)-based front-end. In the presence of noise, however, the EIH-based front-end is more robust. Compared with the FFT-based front-end, with increasing noise the recognition scores drop more slowly, the resulting gap increases as the SNR values decreases. Quantitatively, with the EIH-based front-end the recognizer achieves a given recognition score with global-SNR values which are between 5 dB and 15 dB lower.",['Oded Ghitza'],December 1986,Computer Speech & Language,[],Auditory nerve representation as a front-end for speech recognition in a noisy environment
290,"A finite element model of the vocal tract was made in which the time-dependent, compressible, Navier Stokes equations were solved. No assumptions or simplifications in the physics were made, but a simple fixed wall tube was used as the vocal tract wall. The model was used to study the various effects that may be observed which are due to the Navier Stokes equations' non-linear and stress-related terms. These terms contribute boundary layer effects and energy transfers between frequency bands. Overall, the finite element model proved capable of resolving many effects that are not explained by linear theory. Two set of experiments were performed on the model to examine both time-varying and steady-state responses. Using the model a complete synthetic dipthong was produced to demonstrate the use of the system. The boundary layers could be distinguished, and their effect on the overall response demonstrated.",['T.J. Thomas'],December 1986,Computer Speech & Language,[],A finite element model of fluid flow in the vocal tract*
291,"A technique has been developed for aligning the phonemes in a phonemic transcription of a word with the graphemes in its orthographic representation. For example, creationism /kri:eızam/ can be alighed ascreationism/kri:ei∫nizam/Tables of phonemic-to-orthographic correspondences are given together with the frequency of occurrence of each correspondence in the texts constituting the Lancaster-Oslo/Bergen Corpus (LOB) (1978). The initial motivation for this algorithm was as an aid to the production of a computerized dictionary of English pronunciation. It has been used subsequently to extract from the dictionary all possible pronunciations of English prefixes, and is being used as part of the inferential process in the building of rules for the automated phonemic transcription of English. Uses are also seen in automatic speech recognition as part of the process of decoding the phonetic information extracted from the analysis of the acoustic signal. The algorithm has been checked using a phonemically tagged version of the LOB Corpus. The accent of English used in this work is Received Pronunciation (RP), see Gimson (1980).","['S.G.C. Lawrence', 'G. Kaye']",December 1986,Computer Speech & Language,[],Alignment of phonemes with their corresponding orthography
292,"Although a great deal of effort has gone into studying large-vocabulary speech-recognition problems, there remains a number of interesting, and potentially exceedingly important, problems which do not require the complexity of these large systems. One such problem is connected-digit recognition, which has applications to telecommunications, order entry, credit-card entry, forms automation, and data-base management, among others. Connected-digit recognition is also an interesting problem for another reason, namely that it is one in which whole-word training patterns are applicable as the basic speech-recognition unit. Thus one can bring to bear all the fundamental speech recognition technology associated with whole-word recognition to solve this problem. As such, several connected digit recognizers have been proposed in the past few years. The performance of these systems has steadily improved to the point where high digit-recognition accuracy is achievable in a speaker-trained mode.In this paper we present a unified system for automatically recognizing fluently spoken digit strings based on whole-word reference units. The system that we will describe can use either hidden Markov model (HMM) technology or template-based technology. In fact the overall system contains features from both approaches.A key factor in the success of the various connected digit recognizers is the ability to derive, via a training procedure, a good set of representations of the behavior of the individual digits in actual connected digit strings. For most applications, isolated digit training does not provide a good enough characterization of the variability of the digits in strings. The “best” training procedure is to derive the digit reference patterns (either templates or statistical models) from connected digit strings. Such a connected word training procedure, based on a segmental k-means loop, has been proposed and was tested on seven experienced users of speech recognizers. For these seven talkers, average string accuracies of greater than 98% for unknown length strings, and greater than 99% for known length strings were obtained on an independent test set of 525 variable length strings (1–7 digits) recorded over local dialed-up telephone lines.To evaluate the performance of the overall connected digit recognizer under more difficult conditions, a set of 50 people (25 men, 25 women), from the non-technical local population, was each asked to record 1200 random digit strings over local dialed-up telephone lines. Both a speaker-trained and a multi-speaker training set was created, and a full performance evaluation was made. Results show that the average string accuracy for unknown- and known-length strings, in the speaker-trained mode, was 98% and 99% respectively; in the multi-speaker mode the average string accuracies were 94% and 96.6% respectively. A complete analysis of these results is given in this paper.","['L.R. Rabiner', 'J.G. Wilpon', 'B.H. Juang']",December 1986,Computer Speech & Language,[],A model-based connected-digit recognition system using either hidden Markov models or templates
295,"Boltzmann machines offer a new and exciting approach to automatic speech recognition, and provide a rigorous mathematical formalism for parallel computing arrays. In this paper we briefly summarize Boltzmann machine theory, and present results showing their ability to recognize both static and time-varying speech patterns. A machine with 2000 units was able to distinguish between the 11 steady-state vowels in English with an accuracy of 85%. The stability of the learning algorithm and methods of preprocessing and coding speech data before feeding it to the machine are also discussed. A new type of unit called a carry input unit, which involves a type of state-feedback, was developed for the processing of time-varying patterns and this was tested on a few short sentences. Use is made of the implications of recent work into associative memory, and the modelling of neural arrays to suggest a good configuration of Boltzmann machines for this sort of pattern recognition.","['R.W. Prager', 'T.D. Harrison', 'F. Fallside']",March 1986,Computer Speech & Language,[],Boltzmann machines for speech recognition
296,"During the past decade, the applicability of hidden Markov models (HMM) to various facets of speech analysis has been demonstrated in several different experiments. These investigations all rest on the assumption that speech is a quasi-stationary process whose stationary intervals can be identified with the occupancy of a single state of an appropriate HMM. In the traditional form of the HMM, the probability of duration of a state decreases exponentially with time. This behavior does not provide an adequate representation of the temporal structure of speech.The solution proposed here is to replace the probability distributions of duration with continuous probability density functions to form a continuously variable duration hidden Markov model (CVDHMM). The gamma distribution is ideally suited to specification of the durational density since it is one-sided and only has two parameters which, together, define both mean and variance. The main result is a derivation and proof of convergence of re-estimation formulae for all the parameters of the CVDHMM. It is interesting to note that if the state durations are gamma-distributed, one of the formulae is non-algebraic but, fortuitously, has properties such that it is easily and rapidly solved numerically to any desired degree of accuracy. Other results are presented including the performance of the formulae on simulated data.",['S.E. Levinson'],March 1986,Computer Speech & Language,[],Continuously variable duration hidden Markov models for automatic speech recognition
297,"For large vocabulary recognition, partial phonetic information can be used to reduce the number of likely match candidates before some detailed analysis is performed. This paper reports experimental results for lexical access via broad acoustic-phonetic feature representation. A manageable subset of the lexicon is retrieved using a speaker-independent word representation indicative of manner of articulation, stress, and fricative location. The synchronized electroglottographic (EGG) signal is used as a second channel of data to ensure reliable first pass utterance representation. The glottal sensing characteristics of the EGG aid endpoint detection and the voiced/unvoiced/mixed/silent classifications.",['J.N. Larar'],March 1986,Computer Speech & Language,[],Lexical access using broad acoustic-phonetic classifications
298,"Inconsistencies in speaking pose a major obstacle to the effective use of automatic speech recognition (ASR) systems. In the present study, we sought to improve speaker consistency in ASR by means of instructions and feedback. The instructions were specific suggestions for effective interaction with ASR devices. Feedback was given by a visual barometer-like display, representing the similarity between an utterance and previous utterances. In two experiments, naive speakers were tested on isolated words using speaker-dependent templates. In the first experiment, subjects who were given instructions demonstrated greater speaker consistency than those who did not receive instructions. In contrast, feedback had very little effect on performance. Instructions then were revised, based on an extensive analysis of the types of errors made by speakers, and were tested in a second experiment. Results of this experiment showed a further improvement in recognition performance; the new-instructions group had recognition accuracies that were 6% higher than the no-instructions group.","['Linda A. Roberts', 'Jay G. Wilpon', 'Dennis E. Egan', 'Jean Bakk']",March 1986,Computer Speech & Language,[],Improving speaker consistency in an automatic speech recognition framework
