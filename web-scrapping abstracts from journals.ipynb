{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropping neccesasary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mechanize\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen, urljoin, Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 1(html page code not fully displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'accept': '*/*',\n",
    "         'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.86 Chrome/73.0.3683.86 Safari/537.36'}\n",
    "base_url = 'https://www.sciencedirect.com/journal/computer-speech-and-language/vol/59/suppl/C'\n",
    "def lul_parser(base_url,headers):\n",
    "    session = requests.Session()\n",
    "    request = session.get(base_url,headers = headers)\n",
    "    if request.status_code == 200:\n",
    "        soup = bs(request.content, 'html.parser')\n",
    "        a = soup.find('a', attrs = \"anchor article-content-title u-margin-xs-top u-margin-s-bottom\")\n",
    "        #href = soup.find_all('a', attrs = 'anchor js-issue-item-link text-m')['href']\n",
    "        print(a)\n",
    "        #print(soup)\n",
    "    else:\n",
    "        print('shit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt n 2 : just copied html code from necessary source(url) and get links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open('main_page_html.txt', \"r\")\n",
    "main_page_html = outF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_soup = bs(main_page_html)\n",
    "a_list = main_soup.find_all('a')\n",
    "ssilki = []\n",
    "for i in a_list:\n",
    "    g = str(i).replace('<a class=\"anchor js-issue-item-link text-m\" href=\"','')\n",
    "    ssilki.append('https://www.sciencedirect.com'+g.split('\"><')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89,\n",
       " 'https://www.sciencedirect.com/journal/computer-speech-and-language/vol/59/suppl/C')"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ssilki),ssilki[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lul_parser(base_url,headers):\n",
    "    session = requests.Session()\n",
    "    request = session.get(base_url,headers = headers)\n",
    "    if request.status_code == 200:\n",
    "        soup = bs(request.content, 'html.parser')\n",
    "        divs = soup.find_all('div', attrs = 'issue-item u-margin-s-bottom')\n",
    "        \n",
    "        #href = soup.find_all('a', attrs = 'anchor js-issue-item-link text-m')['href']\n",
    "        print(divs)\n",
    "    else:\n",
    "        print('shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main page have 89 links of volumes from 2001-2019\n"
     ]
    }
   ],
   "source": [
    "print(('Main page have {} links of volumes from 2001-2019').format(len(ssilki)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target = 'date keywords title abstract text author journal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt number 3,and its working only fo volume page, main page suck\n",
    "### main page of  journal have links(89) for volumes(every release),  we need to get this links and in every links we have issues(5-20). From this issues's links we can download our target objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a function that returns links to articles from journal issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def links_from_volumes(links_from_main_page):\n",
    "\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_robots(False)\n",
    "    br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
    "    \n",
    "    l1nks = []\n",
    "    \n",
    "    for i in links_from_main_page:\n",
    "        html = br.open(i,timeout=100).read()\n",
    "        soup = bs(html,'html5lib')\n",
    "        links = soup.find_all('a', {\"class\":'anchor article-content-title u-margin-xs-top u-margin-s-bottom'})\n",
    "\n",
    "        for j in links:\n",
    "            l1nks.append('https://www.sciencedirect.com'+str(j).replace('<a class=\"anchor article-content-title u-margin-xs-top u-margin-s-bottom\" href=\"','').split('\" id=')[0])\n",
    "            \n",
    "    return l1nks        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "четвертая ссылка из массива со ссылками: https://www.sciencedirect.com/journal/computer-speech-and-language/vol/56/suppl/C\n"
     ]
    }
   ],
   "source": [
    "print('четвертая ссылка из массива со ссылками: {}'.format(ssilki[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_links = links_from_volumes(ssilki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we have All  links(880), now we gonna download target objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('final_links.txt','a+')\n",
    "for i in final_links:\n",
    "    \n",
    "    g.write('\\r\\n'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = mechanize.Browser()\n",
    "br.set_handle_robots(False)\n",
    "br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_parser(links):\n",
    "    #upperAlphabet = ['Q','W','E','R','T','Y','U','O','P','A','S','D','F','G','H','J','K','L','Z','X','C','V','B','N','M']\n",
    "    target = []\n",
    "    errors = []\n",
    "\n",
    "    for k in links:\n",
    "        html = br.open(k,timeout=200).read()\n",
    "        soup = bs(html,'html5lib')\n",
    "        \n",
    "        #*******************DATE**************************\n",
    "        \n",
    "        date = (soup.find('div',{'class':'text-xs'},{\"title\":'Go to table of contents for this volume/issue'}).text).split(', ')\n",
    "        for i in date:\n",
    "            if ('January' in i) or ('February' in i) or ('March' in i) or ('April' in i) or ('May' in i) or ('June' in i) or ('July' in i) or ('August' in i) or ('September' in i) or ('October' in i) or ('November' in i) or ('December' in i):\n",
    "                date = i\n",
    "                \n",
    "        #*******************KEYWORDS**********************\n",
    "        \n",
    "        keywordz =[]\n",
    "        keywords = soup.find('div',{'class':'Keywords u-font-serif'})\n",
    "        if type(keywords) == type(None):\n",
    "            errors.append('KeywordNone'+k)\n",
    "        else:\n",
    "            for i in keywords.find_all('div',{'class':'keyword'}):\n",
    "                keywordz.append(i.text)\n",
    "        #*******************TITLE*************************\n",
    "        \n",
    "        title = soup.find('h1',{'class':'Head u-font-serif u-h2 u-margin-s-ver'}).text\n",
    "        \n",
    "        #*******************ABSTRACT**********************\n",
    "        \n",
    "        abstract = soup.find('div',{'class':'abstract author'})\n",
    "        if type(abstract)==type(None):\n",
    "            errors.append('AbstractNone'+k)\n",
    "        else:\n",
    "            abstract = (abstract.text).replace('Abstract','')\n",
    "                             \n",
    "        #*******************JOURNAL***********************\n",
    "        #in this case,everywhere we have same journal\n",
    "        #*******************AUTHOR************************\n",
    "        authors = []                     \n",
    "        author = soup.find('div',{'class':'author-group'})\n",
    "        given_name = author.find_all('span',{'class':'text given-name'})\n",
    "        surname = author.find_all('span',{'class':'text surname'})\n",
    "        for i in range(len(given_name)):\n",
    "            authors.append(given_name[i].text+' '+surname[i].text)\n",
    "        \n",
    "                             \n",
    "        target.append({ 'Date':date,\n",
    "                        'Title':title,\n",
    "                        'Keywords':keywordz,\n",
    "                        'Abstract': abstract,\n",
    "                        'Jornal':'Computer Speech & Language',\n",
    "                        'Author': authors})\n",
    "    return target        #,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_parser(final_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('Computer Speech & Language.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('Computer Speech & Language.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## парсим журналы с 1986-2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open('old_page_html.txt', \"r\")\n",
    "old_page_html = outF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_soup = bs(old_page_html)\n",
    "old_list = old_soup.find_all('a')\n",
    "old_ssilki = []\n",
    "for i in old_list:\n",
    "    g = str(i).replace('<a class=\"anchor js-issue-item-link text-m\" href=\"','')\n",
    "    old_ssilki.append('https://www.sciencedirect.com'+g.split('\"><')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_ssilki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get links of issues from 53 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "older_links = links_from_volumes(old_ssilki)\n",
    "final_older_data = final_parser(older_links)\n",
    "older_final_data = pandas.DataFrame(final_older_data)\n",
    "older_final_data = older_final_data.dropna()\n",
    "older_final_data.to_csv('final_data_older.csv')\n",
    "QWERTY = pandas.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beksultan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "result = pandas.concat([QWERTY, older_final_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
